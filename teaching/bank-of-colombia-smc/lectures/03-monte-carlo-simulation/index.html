<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ed Herbst/teaching/bank-of-colombia-smc/lectures/03-monte-carlo-simulation/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/terminal-0.7.1.min.css">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/animate-3.7.2.min.css">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/console.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="http://localhost:1313/css/custom.css">
<link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
<script src="https://tikzjax.com/v1/tikzjax.js"></script>


    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <meta property="og:title" content="Monte Carlo Simulation" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/teaching/bank-of-colombia-smc/lectures/03-monte-carlo-simulation/" /><meta property="article:published_time" content="2020-12-11T09:46:48-05:00" />



<meta name="twitter:title" content="Monte Carlo Simulation"/>
<meta name="twitter:description" content="Importance Sampling
The main event


Inference: Need to characterize posterior \(p(\theta|Y)\).



Unfortunately, for many interesting models it is not possible to
evaluate the moments and quantiles of the posterior \(p(\theta|Y)\)
analytically.
 


Rules of game: we can only numerically evaluate prior \(p(\theta)\)
and likelihood \(p(Y|\theta)\).
 


To evaluate posterior moments of function \(h(\theta)\), we need numerical
techniques.


Estimating Posterior Moments


We will often abbreviate posterior distributions
\(p(\theta|Y)\) by \(\pi(\theta)\) and posterior expectations of \(h(\theta)\) by
\[
\mathbb{E}_\pi[h] = \mathbb{E}_\pi[h(\theta)] = \int h(\theta) \pi(\theta) d\theta = \int h(\theta) p(\theta|Y) d\theta.
\]"/>

</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              
              <a href='http://localhost:1313/teaching'>teaching</a>/<a href='http://localhost:1313/teaching/bank-of-colombia-smc'>bank-of-colombia-smc</a>/<a href='http://localhost:1313/teaching/bank-of-colombia-smc/lectures'>lectures</a>/<a href='http://localhost:1313/teaching/bank-of-colombia-smc/lectures/03-monte-carlo-simulation'>03-monte-carlo-simulation</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="http://localhost:1313/" typeof="ListItem">&lt;/&gt;</a></li>
                
                <li><a href="http://localhost:1313/research/" typeof="ListItem">research</a></li>
                
                <li><a href="http://localhost:1313/teaching/" typeof="ListItem">teaching</a></li>
                
                <li><a href="http://localhost:1313/etc/" typeof="ListItem">et cetera</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container animated zoomIn fast">
        
<h1>Monte Carlo Simulation</h1>
<h2 id="importance-sampling">Importance Sampling</h2>
<h3 id="the-main-event">The main event</h3>
<ul>
<li>
<p>Inference: Need to characterize posterior \(p(\theta|Y)\).
<br></p>
</li>
<li>
<p>Unfortunately, for many interesting models it is not possible to
evaluate the moments and quantiles of the posterior \(p(\theta|Y)\)
analytically.</p>
 <br>
</li>
<li>
<p><strong>Rules of game</strong>: we can only numerically evaluate prior \(p(\theta)\)
and likelihood \(p(Y|\theta)\).</p>
 <br>
</li>
<li>
<p>To evaluate posterior moments of function \(h(\theta)\), we need numerical
techniques.</p>
</li>
</ul>
<h3 id="estimating-posterior-moments">Estimating Posterior Moments</h3>
<ul>
<li>
<p>We will often abbreviate posterior distributions
\(p(\theta|Y)\) by \(\pi(\theta)\) and posterior expectations of \(h(\theta)\) by
\[
\mathbb{E}_\pi[h] = \mathbb{E}_\pi[h(\theta)] = \int h(\theta) \pi(\theta) d\theta = \int h(\theta) p(\theta|Y) d\theta.
\]</p>
</li>
<li>
<p>We will focus on algorithms that generate draws
\(\{\theta^i\}_{i=1}^N\) from posterior distributions of parameters
in time series models.</p>
</li>
<li>
<p>These draws can then be transformed into objects of interest,
\(h(\theta^i)\), and under suitable conditions a Monte Carlo
average of the form \[ \bar{h}_N = \frac{1}{N} \sum_{i=1}^N
h(\theta^i) \approx \mathbb{E}_\pi[h].  \]</p>
</li>
<li>
<p>Strong law of large numbers (SLLN), central limit theorem (CLT)&hellip;</p>
</li>
</ul>
<h3 id="direct-sampling">Direct Sampling</h3>
<ul>
<li>
<p>In the simple linear regression model with Gaussian posterior it is
possible to sample directly.
<br></p>
</li>
<li>
<p>For \(i=1\) to \(N\), draw \(\theta^i\) from \(\mathcal N \big( \tilde{\theta}_T, \tilde{V}_T \big)\).
<br></p>
</li>
<li>
<p>Provided that \(\mathbb{V}_\pi[h(\theta)] &lt; \infty\) we can deduce
from Kolmogorov&rsquo;s SLLN and the Lindeberg-Levy CLT that</p>
<p>\begin{eqnarray}
\bar{h}_N  &amp;\stackrel{a.s.}{\longrightarrow} &amp; \mathbb{E}_\pi[h]  \nonumber \\\
\sqrt{N} \left( \bar{h}_N - \mathbb{E}_\pi[h] \right) &amp; \Longrightarrow &amp; N \big( 0, \mathbb{V}_\pi[h(\theta)] \big).
\end{eqnarray}</p>
</li>
</ul>
<h3 id="decision-making">Decision Making</h3>
<ul>
<li>
<p>The posterior expected loss associated with a decision
\(\delta(\cdot)\) is given by
\[
\rho\big( \delta(\cdot) | Y \big) = \int_{\Theta} L \big(\theta,\delta(Y) \big) p(\theta|Y) d\theta.
\]</p>
</li>
<li>
<p>A Bayes decision is a decision that minimizes
the posterior expected loss:
\[
\delta^*(Y) = \mbox{argmin}_{d} ; \rho\big( \delta(\cdot) | Y \big).
\]</p>
</li>
<li>
<p>Since in most applications it is not feasible to derive
the posterior expected risk analytically, we replace
\(\rho\big( \delta(\cdot) | Y \big)\) by a Monte Carlo approximation
of the form
\[
\bar{\rho}_N \big( \delta(\cdot) | Y \big) = \frac{1}{N} \sum_{i=1}^N L \big(\theta^i,\delta(\cdot) \big).
\]</p>
</li>
<li>
<p>A numerical approximation to the Bayes decision \(\delta^*(\cdot)\) is then
given by
\[
\delta_N^*(Y) = \mbox{argmin}_{d} ; \bar{\rho}_N \big( \delta(\cdot) | Y \big).
\]</p>
</li>
</ul>
<h3 id="importance-sampling">Importance Sampling</h3>
<p>\begin{equation}
\pi(\theta) =  \frac{f(\theta)}{Z} = \frac{p(Y|\theta)p(\theta)}{p(Y)}
\end{equation}</p>
<p>\(f(\cdot)\) is the function we can evaluate numerically.
<br>
References: <sup id="38076dbc619583085f2e4be6c0229c07"><a href="#Hammersley_1964" title="Hammersley \&amp; Handscomb, Monte Carlo Methods, v(), (1964).">Hammersley_1964</a></sup>, <sup id="aa638b39999ab5704a9c4308b2fb7e69"><a href="#KloekVanDijk" title="Kloek \&amp; van Dijk, Bayesian Estimates of Equation System Parameters: An Application of Integration by Monte Carlo, {Econometrica}, v(1), 1--19 (1978).">KloekVanDijk</a></sup>, and <sup id="f26c5482124067ac084d605918d80221"><a href="#Geweke_1989" title="Geweke, Bayesian Inference in Econometric Models Using Monte  Carlo Integration, {Econometrica}, v(6), 1317 (1989).">Geweke_1989</a></sup>.
<br>
Let \textcolor{blue}{$g$} be an arbitrary, easy-to-sample pdf over \(\theta\) (think normal distribution).
<br>
Importance sampling (IS) is based on the following identity:</p>
<p>\begin{equation}
\label{eq_isidentity}
\mathbb{E}_{\pi}[h(\theta)]
= \int h(\theta) \pi(\theta) d\theta
= \frac{1}{Z} \int_{\Theta}h(\theta)\frac{f(\theta)}{g(\theta)}g(\theta)d\theta.
\end{equation}</p>
<p>Since \(\mathbb{E}_\pi[1]=1\),
\[
Z = \int_{\Theta}\frac{f(\theta)}{g(\theta)}g(\theta)d\theta.
\]</p>
<h3 id="importance-sampling">Importance Sampling</h3>
<p><strong>(Unnormalized) Importance weight</strong>:
\[
w(\theta) = \frac{f(\theta)}{g(\theta)}
\]
<strong>Normalized Importance Weight</strong>:</p>
<p>\begin{eqnarray}
v(\theta) = \frac{ w(\theta)}{\int w(\theta) g(\theta) d\theta} = \frac{ w(\theta) }{ \int Z \pi(\theta) d\theta} = \frac{w(\theta)}{Z}.
\label{eq_defvtheta}
\end{eqnarray}</p>
 <br>
Can show:
<p>\begin{equation}
\mathbb{E}_\pi[h(\theta)] = \int v(\theta) h(\theta) g(\theta) d\theta.
\end{equation}</p>
<h3 id="the-details">The Details</h3>
<ul>
<li>
<p>For \(i=1\) to \(N\), draw \(\theta^i \stackrel{iid}{\sim} g(\theta)\)  and
compute the unnormalized importance weights</p>
<p>\begin{equation}
w^i = w(\theta^i) = \frac{f(\theta^i)}{g(\theta^i)}.
\end{equation}</p>
</li>
<li>
<p>Compute the normalized importance weights</p>
<p>\begin{equation}
W^i = \frac{w^i}{\frac{1}{N} \sum_{i=1}^N w^i}.
\end{equation}</p>
<p>An approximation of \(\mathbb{E}_\pi[h(\theta)]\) is given by</p>
<p>\begin{equation}
\bar{h}_N = \frac{1}{N} \sum_{i=1}^N W^i h(\theta^i).
\label{eq_hbar}
\end{equation}</p>
<p>Note \(W^i\) is (slightly) different from \(v\) in previous slide.</p>
</li>
</ul>
<h3 id="the-details">The Details</h3>
<ul>
<li>Refer to the collection of pairs \(\{(\theta^{i}, W^i)\}_{i=1}^N\) as a
<strong>particle approximation</strong> of \(\pi(\theta)\).
<br></li>
<li>The accuracy of the approximation is driven by the ``closeness&rsquo;&rsquo; of
\(g(\cdot)\) to \(f(\cdot)\) and is reflected in the distribution of the
weights.
<br></li>
<li>If the distribution of weights is very uneven, the Monte Carlo approximation \(\bar{h}\) is
inaccurate.
<br></li>
<li>Uniform weights arise if \(g(\cdot)\propto f(\cdot)\), which means
that we are sampling directly from \(\pi(\theta)\).</li>
</ul>
<h3 id="effectiveness-of-is-depends-on-similarity-of--f--and--g">Effectiveness of IS depends on similarity of \(f\) and \(g\)</h3>
<p>\(f = \mathcal N(0,1),\quad g_1 = t(0,1,5),\quad g_2 = \mathcal N(2,1)\)
\includegraphics[width=4in]{static/is.pdf}</p>
<p>Only a few draws from \(N(2,1)\) have meaningful weight. <br />
\(\implies\) estimate is based on small sample. <br />
\(\implies\) estimate will be noisy.</p>
<h3 id="convergence">Convergence</h3>
<ul>
<li><strong>SLLN:</strong> If \(\mathbb{E}_g [|h f/g|] &lt; \infty\) and \(\mathbb{E}_g
[|f/g|] &lt; \infty\), see \cite{Geweke_1989}, the Monte Carlo estimate
\(\bar{h}_N\) defined in (\ref{eq_hbar}) converges almost surely (a.s.) to
\(E_{\pi}[h(\theta)]\) as \(N \longrightarrow \infty\).
<br></li>
<li><strong>CLT:</strong> A bit more complicated.</li>
</ul>
<h3 id="central-limit-theorem">Central Limit Theorem</h3>
<p>Define the population analogue of the normalized importance weights as \(v(\theta) = w(\theta)/Z\)
and write</p>
<p>\begin{equation}
\bar{h}_N = \frac{ \frac{1}{N} \sum_{i=1}^N (w^i/Z) h(\theta^i)}{ \frac{1}{N} \sum_{i=1}^N (w^i/Z) }
= \frac{ \frac{1}{N} \sum_{i=1}^N v(\theta^i) h(\theta^i)}{ \frac{1}{N} \sum_{i=1}^N v(\theta^i) }.
\end{equation}</p>
<p>Now consider a first-order Taylor series expansion in terms of
deviations of the numerator from \(\mathbb{E}_\pi[h]\) and deviations of
the denominator around 1:</p>
<p>\begin{eqnarray}
\lefteqn{\sqrt{N}(\bar{h}_N - \mathbb{E}_\pi[h]) } \\\
&amp;=&amp; \sqrt{N} \left( \frac{1}{N} \sum_{i=1}^N v(\theta^i) h(\theta^i) - \mathbb{E}_\pi[h] \right) \nonumber \\\
&amp;&amp;  - \mathbb{E}_\pi[h] \sqrt{N} \left( \frac{1}{N} \sum_{i=1}^N  v(\theta^i) - 1 \right) + o_p(1) \nonumber \\\
&amp;=&amp;  (I) - \mathbb{E}_\pi[h] \cdot (II) + o_p(1). \nonumber
\end{eqnarray}</p>
<h3 id="central-limit-theorem">Central Limit Theorem</h3>
<p>Under some regularity conditions, we can apply a multivariate
extension of the Lindeberg-Levy CLT to the terms \((I)\) and \((II)\).
<br>
The variances and covariance of \((I)\) and \((II)\) are given by</p>
<p>\begin{eqnarray*}
\mathbb{V}_g[hv] &amp;=&amp; \mathbb{E}_\pi [ (\pi/g) h^2 ] - \mathbb{E}^2_\pi[h],\\\
\mathbb{V}_g[v]  &amp;=&amp; \mathbb{E}_\pi [ (\pi/g) ]  - 1, \\\
COV_g(hv,v) &amp;=&amp; \big( \mathbb{E}_\pi [ (\pi/g) h ] - \mathbb{E}_\pi[h] \big).
\end{eqnarray*}</p>
<p>In turn we can deduce that</p>
<p>\begin{equation}
\sqrt{N}(\bar{h}_N - \mathbb{E}_\pi[h])
\Longrightarrow N \big( 0, \Omega(h) \big), \label{eq_isomegah}
\end{equation}</p>
<p>where
\[
\Omega(h) = \mathbb{V}_g[(\pi/g)(h-\mathbb{E}_\pi[h])].
\]</p>
<h3 id="accuracy">Accuracy</h3>
<ul>
<li>Assess the accuracy by computing a Monte Carlo approximation \(\bar{h}_N\)
multiple times and examine its variability across repeated runs of the
posterior sampler.
<br></li>
<li>If \(\bar{h}_N\) satisfies a CLT and the number of draws \(N\) is
sufficiently large, then the variance across repeated runs of the algorithm
(provided this variance is finite for the given \(N\)) will approximately
coincide with the asymptotic variance implied by the CLT.
<br></li>
<li>Define <strong>inefficiency factor</strong> relative to IID sampling,
\[
\mbox{InEff}_\infty = \frac{\Omega(h)}{\mathbb{V}_\pi[h]}.
\]
If \(\mbox{Ineff}_{\infty} &gt; 1\) we are worse than iid sampling.</li>
</ul>
<h3 id="numerical-illustration">Numerical Illustration</h3>
<ul>
<li>Let&rsquo;s take a harder \(\pi(\theta)\), the set-identified posterior from Moon-Schorfheide (2013).
<br></li>
<li>Consider <strong>diffuse</strong> and <strong>concentrated</strong> importance sample densities \(g\).</li>
</ul>
<p>\begin{center}
\includegraphics[width=4.3in]{static/IS_proposal}
\end{center}</p>
<h3 id="experiment">Experiment</h3>
<ul>
<li>
<p>Using various \(N\), generate \(IS\) approximations for \(h(\theta) =
\theta\) and \(h(\theta) = \theta^2\).</p>
 <br>
</li>
<li>
<p>Calculate estimate of \(\mbox{InEff}_{\infty}\) using \(N_{run} = 1000\) Monte Carlo
simulations, as well as the exact value [by sampling from \(\pi(\theta)\).]  Estimates come from:</p>
<p>\begin{equation}
\mbox{InEff}_N = \frac{\mathbb{V}[\bar{h}_N]}{\mathbb{V}_\pi[h]/N}.
\end{equation}</p>
 <br>
</li>
<li>
<p>Also calculate poor man&rsquo;s version of Inefficiency Factor, because everyone uses it.</p>
<p>\begin{equation}
\mbox{InEff}_\infty \approx 1+ \mathbb{V}_g[\pi/g].
\label{eq_IS_InEff_approx}
\end{equation}</p>
</li>
</ul>
<h3 id="concentrated-is-density">Concentrated IS Density</h3>
<ul>
<li>solid line = estimates of \(\mbox{InEff}_{\infty}[h]\), dashed = truth</li>
<li>triangles = \(h(\theta) = \theta\), circles = \(h(\theta) = \theta^2\)</li>
<li>grey line = poor man&rsquo;s inefficiency</li>
</ul>
<p>\begin{center}
\includegraphics[width=4.3in]{static/IS_ineff0125}
\end{center}</p>
<h3 id="diffuse-is-density">Diffuse IS Density</h3>
<ul>
<li>Solid line = estimates of \(\mbox{InEff}_{\infty}[h]\), dashed = truth</li>
<li>triangles = \(h(\theta) = \theta\), circles = \(h(\theta) = \theta^2\)</li>
<li>grey line = poor man&rsquo;s inefficiency</li>
</ul>
<p>\begin{center}
\includegraphics[width=4.3in]{static/IS_ineff05}
\end{center}</p>
<h3 id="take-aways">Take aways</h3>
<ul>
<li>It is important that the importance density \(g\) is well-tailored toward the target distribution \index{target distribution}
\(\pi\)!
<br></li>
<li>Everything is \(h\) specific!
<br></li>
<li>with approximately elliptical posterior, a good importance density
can be obtained by centering a fat-tailed \(t\) distribution at the
mode of \(\pi\) and using a scaled version of the inverse
\index{Hessian matrix} Hessian of \(\ln \pi\) at the mode to align
the contours of the importance density with the contours of the
posterior \(\pi\).
<br></li>
<li>Very bad for  highly irregular and non-elliptical posteriors&hellip;</li>
</ul>
<h3 id="references">References</h3>
<h1 id="bibliography">Bibliography</h1>
<p><a id="Hammersley_1964"></a>[Hammersley_1964] Hammersley &amp; Handscomb, Monte Carlo Methods, <i></i>,  (1964). <a href="http://dx.doi.org/10.1007/978-94-009-5819-7">link</a>. <a href="http://dx.doi.org/10.1007/978-94-009-5819-7">doi</a>. <a href="#38076dbc619583085f2e4be6c0229c07">↩</a></p>
<p><a id="KloekVanDijk"></a>[KloekVanDijk] Kloek &amp; van Dijk, Bayesian Estimates of Equation System Parameters: An Application of Integration by Monte Carlo, <i>Econometrica</i>, <b>46(1)</b>, 1-19 (1978). <a href="http://www.jstor.org/stable/1913641">link</a>. <a href="#aa638b39999ab5704a9c4308b2fb7e69">↩</a></p>
<p><a id="Geweke_1989"></a>[Geweke_1989] Geweke, Bayesian Inference in Econometric Models Using Monte  Carlo Integration, <i>Econometrica</i>, <b>57(6)</b>, 1317 (1989). <a href="http://dx.doi.org/10.2307/1913710">link</a>. <a href="http://dx.doi.org/10.2307/1913710">doi</a>. <a href="#f26c5482124067ac084d605918d80221">↩</a></p>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] },
  tex2jax: {
      inlineMath: [['$','$'],['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script type="text/javascript"
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
