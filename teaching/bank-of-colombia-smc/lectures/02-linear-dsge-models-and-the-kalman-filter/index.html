<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ed Herbst/teaching/bank-of-colombia-smc/lectures/02-linear-dsge-models-and-the-kalman-filter/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="https://edherbst.net/hugo-theme-console/css/terminal-0.7.1.min.css">
    <link rel="stylesheet" href="https://edherbst.net/hugo-theme-console/css/animate-3.7.2.min.css">
    <link rel="stylesheet" href="https://edherbst.net/hugo-theme-console/css/console.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://edherbst.net/css/custom.css">
<link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
<script src="https://tikzjax.com/v1/tikzjax.js"></script>


    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <meta property="og:title" content="Introduction Bayes 6: (Linear) State Space Models" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://edherbst.net/teaching/bank-of-colombia-smc/lectures/02-linear-dsge-models-and-the-kalman-filter/" /><meta property="article:published_time" content="2020-10-27T21:06:35-04:00" />



<meta name="twitter:title" content="Introduction Bayes 6: (Linear) State Space Models"/>
<meta name="twitter:description" content="Intro
Background

Textbook treatments: woodford_2003, Gali2008

Key empirical papers: ireland2004,  christiano2005, Smets2007, An2007b,

Frequentist estimation: Harvey1991, Hamilton,

Bayesian estimation: HerbstSchorfheide2015

A DSGE Model
Small-Scale DSGE Model

Intermediate and final goods producers

Households

Monetary and fiscal policy

Exogenous processes

Equilibrium Relationships

Final Goods Producers

Perfectly competitive firms combine
a continuum of intermediate goods:
\[
Y_t = \left( \int_0^1 Y_t(j)^{1-\nu} dj \right)^{\frac{1}{1-\nu}}.
\]
Firms take input prices \(P_t(j)\) and output prices \(P_t\) as given; maximize profits
\[
\Pi_t =  P_t \left( \int_0^1 Y_t(j)^{1-\nu} dj \right)^{\frac{1}{1-\nu}} - \int_{0}^1 P_t(j)Y_t(j)dj.
\]
Demand for intermediate good \(j\):
\[
Y_t(j) = \left( \frac{P_t(j)}{P_t} \right)^{-1/\nu} Y_t.
\]
Zero-profit condition implies
\[
P_t = \left( \int_0^1 P_t(j)^{\frac{\nu-1}{\nu}} dj \right)^{\frac{\nu}{\nu-1}}.
\]

Intermediate Goods Producers

Intermediate good \(j\) is produced by a monopolist according to:
\[
Y_t(j) = A_t N_t(j).
\]
Nominal price stickiness via quadratic price adjustment costs
\[
AC_t(j) = \frac{\phi}{2} \left( \frac{ P_t(j) }{ P_{t-1}(j)} - \pi \right)^2 Y_t(j).
\]
Firm \(j\)
chooses its labor input \(N_t(j)\) and the price \(P_t(j)\) to maximize
the present value of future profits:
\[ \mathbb{E}_t \bigg[
\sum_{s=0}^\infty \beta^{s} Q_{t&#43;s|t} \bigg(
\frac{P_{t&#43;s}(j)}{P_{t&#43;s}} Y_{t&#43;s}(j) - W_{t&#43;s} N_{t&#43;s}(j) - AC_{t&#43;s}(j) \bigg) \bigg].
\]

Households


Household derives disutility from hours worked \(H_t\) and maximizes"/>

</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              
              <a href='https://edherbst.net/teaching'>teaching</a>/<a href='https://edherbst.net/teaching/bank-of-colombia-smc'>bank-of-colombia-smc</a>/<a href='https://edherbst.net/teaching/bank-of-colombia-smc/lectures'>lectures</a>/<a href='https://edherbst.net/teaching/bank-of-colombia-smc/lectures/02-linear-dsge-models-and-the-kalman-filter'>02-linear-dsge-models-and-the-kalman-filter</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="https://edherbst.net/" typeof="ListItem">&lt;/&gt;</a></li>
                
                <li><a href="https://edherbst.net/research/" typeof="ListItem">research</a></li>
                
                <li><a href="https://edherbst.net/teaching/" typeof="ListItem">teaching</a></li>
                
                <li><a href="https://edherbst.net/etc/" typeof="ListItem">et cetera</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container animated zoomIn fast">
        
<h1>Introduction Bayes 6: (Linear) State Space Models</h1>
<h2 id="intro">Intro</h2>
<h3 id="background">Background</h3>
<ul>
<li><em>Textbook treatments:</em> <sup id="a387326a2038fd7309392dc655b58018"><a href="#woodford_2003" title="Woodford, Interest and Prices, Princeton University Press (2003).">woodford_2003</a></sup>, <sup id="e501ccdeda2f0731e31cee0bb583687c"><a href="#Gali2008" title="Jordi Gal\'i, Monetary Policy, Inflation, and the Business Cycle: An Introduction to the New Keynesian Framework, Princeton University Press (2008).">Gali2008</a></sup>
<br></li>
<li><em>Key empirical papers</em>: <sup id="29dbe0ba987d7fc6fe70b0eb8b9164a8"><a href="#ireland2004" title="Ireland, A Method for Taking Models to the Data, {Journal of Economic Dynamics and Control}, v(6), 1205-1226 (2004).">ireland2004</a></sup>,  <sup id="47048f0095d5c18f125a005804f82697"><a href="#christiano2005" title="Christiano, Eichenbaum, Evans \&amp; , Nominal Rigidities and the Dynamic Effects of a Shock to Monetary  Policy, {Journal of Political Economy}, v(1), 1-45 (2005).">christiano2005</a></sup>, <sup id="0d8b307a60fe1d0ae3cf2a838eca7a27"><a href="#Smets2007" title="Smets \&amp; Wouters, Shocks and Frictions in US Business Cycles: A Bayesian DSGE Approach, {American Economic Review}, v(), 586-608 (2007).">Smets2007</a></sup>, <sup id="188111b01398a8806c0a2b9d9be3fd1c"><a href="#An2007b" title="An \&amp; Frank Schorfheide, Bayesian Analysis of DSGE Models, {Econometric Reviews}, v(2-4), 113-172 (2007).">An2007b</a></sup>,
<br></li>
<li><em>Frequentist estimation:</em> <sup id="6ca9488bf724ffa1b1742c738fcd1634"><a href="#Harvey1991" title="Andrew Harvey, Forecasting, Structural Time Series Models and the Kalman Filter, University of Cambridge Press (1991).">Harvey1991</a></sup>, <sup id="adec714ae69bef54c5ee79cfcb41955d"><a href="#Hamilton" title="James Hamilton, Time Series Analysis, Princeton University Press (1994).">Hamilton</a></sup>,
<br></li>
<li><em>Bayesian estimation:</em> <sup id="f275eaf93510eb80c8e1a928b194e45f"><a href="#HerbstSchorfheide2015" title="Edward Herbst \&amp; Frank Schorfheide, Bayesian Estimation of DSGE Models, Princeton University Press (2015).">HerbstSchorfheide2015</a></sup></li>
</ul>
<h2 id="a-dsge-model">A DSGE Model</h2>
<h3 id="small-scale-dsge-model">Small-Scale DSGE Model</h3>
<ul>
<li>Intermediate and final goods producers
<br></li>
<li>Households
<br></li>
<li>Monetary and fiscal policy
<br></li>
<li>Exogenous processes
<br></li>
<li>Equilibrium Relationships</li>
</ul>
<h3 id="final-goods-producers">Final Goods Producers</h3>
<ul>
<li>Perfectly competitive firms combine
a continuum of intermediate goods:
\[
Y_t = \left( \int_0^1 Y_t(j)^{1-\nu} dj \right)^{\frac{1}{1-\nu}}.
\]</li>
<li>Firms take input prices \(P_t(j)\) and output prices \(P_t\) as given; maximize profits
\[
\Pi_t =  P_t \left( \int_0^1 Y_t(j)^{1-\nu} dj \right)^{\frac{1}{1-\nu}} - \int_{0}^1 P_t(j)Y_t(j)dj.
\]</li>
<li>Demand for intermediate good \(j\):
\[
Y_t(j) = \left( \frac{P_t(j)}{P_t} \right)^{-1/\nu} Y_t.
\]</li>
<li>Zero-profit condition implies
\[
P_t = \left( \int_0^1 P_t(j)^{\frac{\nu-1}{\nu}} dj \right)^{\frac{\nu}{\nu-1}}.
\]</li>
</ul>
<h3 id="intermediate-goods-producers">Intermediate Goods Producers</h3>
<ul>
<li>Intermediate good \(j\) is produced by a monopolist according to:
\[
Y_t(j) = A_t N_t(j).
\]</li>
<li>Nominal price stickiness via quadratic price adjustment costs
\[
AC_t(j) = \frac{\phi}{2} \left( \frac{ P_t(j) }{ P_{t-1}(j)} - \pi \right)^2 Y_t(j).
\]</li>
<li>Firm \(j\)
chooses its labor input \(N_t(j)\) and the price \(P_t(j)\) to maximize
the present value of future profits:
\[ \mathbb{E}_t \bigg[
\sum_{s=0}^\infty \beta^{s} Q_{t+s|t} \bigg(
\frac{P_{t+s}(j)}{P_{t+s}} Y_{t+s}(j) - W_{t+s} N_{t+s}(j) - AC_{t+s}(j) \bigg) \bigg].
\]</li>
</ul>
<h3 id="households">Households</h3>
<ul>
<li>
<p>Household derives disutility from hours worked \(H_t\) and maximizes</p>
<p>\begin{eqnarray*}
\lefteqn{ \mathbb{E}_t \bigg[ \sum_{s=0}^\infty \beta^s \bigg( \frac{ (C_{t+s}/A_{t+s})^{1-\tau} -1 }{1-\tau} } \\\
&amp;&amp;+ \chi_M \ln \left( \frac{M_{t+s}}{P_{t+s}} \right) - \chi_H H_{t+s} \bigg) \bigg].
\end{eqnarray*}</p>
</li>
<li>
<p>Budget constraint:</p>
<p>\begin{eqnarray*}
\lefteqn{P_t C_{t} + B_{t} + M_t + T_t} \\\
&amp;=&amp; P_t W_t H_{t} + R_{t-1}B_{t-1} + M_{t-1} + P_t D_{t} + P_t SC_t.
\end{eqnarray*}</p>
</li>
</ul>
<h3 id="monetary-and-fiscal-policy">Monetary and Fiscal Policy</h3>
<ul>
<li>
<p>Central bank adjusts money supply to attain desired interest rate.</p>
</li>
<li>
<p>Monetary policy rule:</p>
<p>\begin{eqnarray*}
R_t &amp;=&amp; R_t^{*, 1-\rho_R} R_{t-1}^{\rho_R} e^{\epsilon_{R,t}} \\\
R_t^* &amp;=&amp; r \pi^* \left( \frac{\pi_t}{\pi^*} \right)^{\psi_1} \left( \frac{Y_t}{Y_t^*} \right)^{\psi_2}
\end{eqnarray*}</p>
</li>
<li>
<p>Fiscal authority consumes fraction of aggregate output: \(G_t = \zeta_t Y_t\).</p>
</li>
<li>
<p>Government budget constraint:
\[
P_t G_t + R_{t-1} B_{t-1} + M_{t-1} = T_t + B_t + M_t.
\]</p>
</li>
</ul>
<h3 id="exogenous-processes">Exogenous Processes</h3>
<ul>
<li>Technology:
\[
\ln A_t = \ln \gamma + \ln A_{t-1} + \ln z_t, \quad
\ln z_t = \rho_z \ln z_{t-1} + \epsilon_{z,t}.
\]</li>
<li>Government spending / aggregate demand: define \(g_t = 1/(1-\zeta_t)\); assume
\[
\ln g_t = (1-\rho_g) \ln g + \rho_g \ln g_{t-1} + \epsilon_{g,t}.
\]</li>
<li>Monetary policy shock \(\epsilon_{R,t}\) is assumed to be serially uncorrelated.</li>
</ul>
<h3 id="equilibrium-conditions">Equilibrium Conditions</h3>
<ul>
<li>
<p>Consider the symmetric equilibrium in which all intermediate goods producing
firms make identical choices; omit \(j\) subscript.</p>
</li>
<li>
<p>Market clearing:
\[
Y_t = C_t + G_t + AC_t \quad \mbox{and} \quad H_t = N_t.
\]</p>
</li>
<li>
<p>Complete markets:
\[
Q_{t+s|t} = (C_{t+s}/C_t)^{-\tau}(A_t/A_{t+s})^{1-\tau}.
\]</p>
</li>
<li>
<p>Consumption Euler equation and New Keynesian Phillips curve:</p>
<p>\begin{eqnarray*}
1 &amp;=&amp; \beta \mathbb{E}_t \left[ \left( \frac{ C_{t+1} /A_{t+1} }{C_t/A_t} \right)^{-\tau} \frac{A_t}{A_{t+1}} \frac{R_t}{\pi_{t+1}} \right] \label{eq_dsge1HHopt} \\\
1 &amp;=&amp;
\phi (\pi_t - \pi) \left[ \left( 1 - \frac{1}{2\nu} \right) \pi_t + \frac{\pi}{2 \nu} \right] \label{eq_dsge1Firmopt}\\\
&amp;&amp; - \phi \beta \mathbb{E}_t \left[ \left( \frac{ C_{t+1} /A_{t+1} }{C_t/A_t} \right)^{-\tau} \frac{ Y_{t+1} /A_{t+1} }{Y_t/A_t}
(\pi_{t+1} - \pi) \pi_{t+1} \right] \nonumber \\\
&amp;&amp; + \frac{1}{\nu} \left[ 1 - \left( \frac{C_t}{A_t} \right)^\tau \right]. \nonumber
\end{eqnarray*}</p>
</li>
</ul>
<h3 id="equilibrium-conditions-continued">Equilibrium Conditions &ndash; Continued</h3>
<ul>
<li>In the absence of nominal rigidities \((\phi = 0)\)
aggregate output is given by
\[
Y_t^* = (1-\nu)^{1/\tau} A_t g_t,
\]
which is the target level of output that appears in the monetary policy rule.</li>
</ul>
<h3 id="steady-state">Steady State</h3>
<ul>
<li>
<p>Set \(\epsilon_{R,t}\), \(\epsilon_{g,t}\), and \(\epsilon_{z,t}\)
to zero at all times.</p>
</li>
<li>
<p>Because technology \(\ln A_t\) evolves according
to a random walk with drift \(\ln \gamma\), consumption and output need
to be detrended for a steady state to exist.</p>
</li>
<li>
<p>Let
\[
c_t = C_t/A_t, \quad y_t = Y_t/A_t, \quad
y^*_t = Y^*_t/A_t.
\]</p>
</li>
<li>
<p>Steady state is given by:</p>
<p>\begin{eqnarray*}
\pi &amp;=&amp; \pi^*, \quad r = \frac{\gamma}{\beta}, \quad R = r
\pi^*, \\\
c &amp;=&amp; (1-\nu)^{1/\tau}, \quad  y = g c =
y^*.
\end{eqnarray*}</p>
</li>
</ul>
<h2 id="solving-dsge-models">Solving DSGE Models</h2>
<h3 id="solving-dsge-models">Solving DSGE Models</h3>
<ul>
<li>Derive nonlinear equilibrium conditions:
<ul>
<li>System of nonlinear expectational difference equations;</li>
<li>transversality conditions.</li>
</ul>
</li>
<li>Find solution(s) of system of expectational difference methods:
<ul>
<li>Global (nonlinear) approximation</li>
<li>Local approximation near steady state</li>
</ul>
</li>
<li>\textcolor{blue}{We will focus on log-linear approximations around the steady state.}</li>
<li>More detail in: <sup id="fd709478b3c27866339020943ed1cd0a"><a href="#Fern_ndez_Villaverde_2016" title="Fernandez-Villaverde, Rubio-Ramirez, \&amp; Schorfheide, Solution and Estimation Methods for DSGE Models, {Handbook of Macroeconomics}, v(), 527 724 (2016).">Fern_ndez_Villaverde_2016</a></sup>: ``Solution and Estimation Methods for DSGE Models.''</li>
</ul>
<h3 id="what-is-a-local-approximation">What is a Local Approximation?</h3>
<ul>
<li>In a nutshell&hellip; consider the backward-looking model
\[
y_t = f(y_{t-1},\sigma \epsilon_t).
\]</li>
<li>Guess that the solution is of the form
\[
y_t = y_t^{(0)} + \sigma y_t^{(1)} + o(\sigma).
\]</li>
<li>Steady state:
\[
y_t^{(0)} = y^{(0)} = f(y^{(0)},0)
\]</li>
<li>Suppose \(y^{(0)}=0\). Expand \(f(\cdot)\) around \(\sigma=0\):
\[
f(y_{t-1},\sigma \epsilon_t)
= f_y y_{t-1} + f_\epsilon \sigma \epsilon_t + o(|y_{t-1}|) + o(\sigma)
\]</li>
<li>Now plug-in conjectured solution:
\[
\sigma y_t^{(1)}
=  f_y \sigma y_{t-1}^{(1)} + f_\epsilon \sigma \epsilon_t + o(\sigma)
\]</li>
<li>Deduce that \(y_t^{(1)} = f_y y_{t-1}^{(1)} + f_\epsilon \epsilon_t\)</li>
</ul>
<h3 id="what-is-a-log-linear-approximation">What is a Log-Linear Approximation?</h3>
<ul>
<li>
<p>Consider a Cobb-Douglas production function: \(Y_t = A_t K_t^\alpha N_t^{1-\alpha}\).</p>
</li>
<li>
<p>\textcolor{red}{Linearization} around \(Y_*\), \(A_*\), \(K_*\), \(N_*\):</p>
<p>\begin{eqnarray*}
Y_t-Y_* &amp;\approx&amp; K_*^\alpha N_*^{1-\alpha}(A_t - A_*)
+ \alpha A_* K_*^{\alpha-1} N_*^{1-\alpha} (K_t-K_*) \\\
&amp;&amp;	  + (1-\alpha) A_* K_*^\alpha N_*^{-\alpha} (N_t-N_*)
\end{eqnarray*}</p>
</li>
<li>
<p>\textcolor{blue}{Log-linearization:} Let \(f(x) = f(e^v)\) and linearize
with respect to \(v\):
\[
f(e^v) \approx f(e^{v_*}) + e^{v_*} f&rsquo;(e^{v_*}) (v-v_*).
\]
Thus:
\[
f(x) \approx f(x_*) + x_* f&rsquo;(x_*){\color{blue} (\ln x/x_*)} = f(x_*) + f&rsquo;(x_*) {\color{blue} \tilde{x}}
\]</p>
</li>
<li>
<p>Cobb-Douglas production function (here relationship is exact):
\[
\tilde{Y}_t = \tilde{A}_t + \alpha \tilde{K}_t + (1-\alpha) \tilde{N_t}
\]</p>
</li>
</ul>
<h3 id="loglinearization-of-new-keynesian-model">Loglinearization of New Keynesian Model</h3>
<ul>
<li>Consumption Euler equation:
\[
\hat{y}_{t} =  \mathbb{E}_t[\hat y_{t+1}] - \frac{1}{\tau} \bigg( \hat R_t -  \mathbb{E}_t[\hat\pi_{t+1}] - \mathbb{E}_t[\hat{z}_{t+1}] \bigg) + \hat{g}_t - \mathbb{E}_t[\hat{g}_{t+1}]
\]</li>
<li>New Keynesian Phillips curve:
\[
\hat \pi_t = \beta \mathbb{E}_t[\hat \pi_{t+1}] + \kappa (\hat y_t- \hat g_t),
\]
where
\[
\kappa = \tau \frac{1 -\nu}{ \nu \pi^2 \phi }
\]</li>
<li>Monetary policy rule:
\[
\hat R_{t} = \rho_R \hat R_{t-1} + (1-\rho_R) \psi_1 \hat \pi_{t} + (1-\rho_R) \psi_2 \left( \hat y_{t} - \hat g_t \right)+ \epsilon_{R,t}
\]</li>
</ul>
<h3 id="canonical-linear-rational-expectations-system">Canonical Linear Rational Expectations System</h3>
<ul>
<li>Define
\[
x_t  = [ \hat y_t, \hat \pi_t, \hat R_t, \epsilon_{R,t}, \hat{g}_t, \hat z_t  ]&rsquo;.
\]</li>
<li>Augment \(x_t\) by
\(\mathbb{E}_t[\hat{y}_{t+1}]\) and \(\mathbb{E}_t[\hat{\pi}_{t+1}]\).</li>
<li>Define
\[
s_t = \big[ x_t&rsquo;, \mathbb{E}_t[\hat{y}_{t+1}], \mathbb{E}_t[\hat{\pi}_{t+1}] \big]&rsquo;.
\]</li>
<li>Define rational expectations forecast errors forecast errors for inflation and output. Let
\[
\eta_{y,t} = y_t - \mathbb{E}_{t-1}[\hat{y}_t], \quad \eta_{\pi,t} = \pi_t - \mathbb{E}_{t-1}[\hat{\pi}_t].
\]</li>
<li>Write system in canonical form <sup id="44a5cfd4aac55ac2b563d69becd9f5ad"><a href="#Sims2002" title="Sims, Solving Linear Rational Expectations Models, {Computational Economics}, v(), 1-20 (2002).">Sims2002</a></sup>:
\[
\Gamma_0 s_t = \Gamma_1 s_{t-1} + \Psi \epsilon_t + \Pi \eta_t.
\]</li>
</ul>
<h3 id="how-can-one-solve-linear-rational-expectations-systems-a-simple-example">How Can One Solve Linear Rational Expectations Systems? A Simple Example</h3>
<ul>
<li>
<p>Consider</p>
<p>\begin{eqnarray}
y_t = \frac{1}{\theta} \EE_t[y_{t+1}] + \epsilon_t,
\label{eq_yex}
\end{eqnarray}</p>
<p>where \(\epsilon_t \sim iid(0,1)\) and \(\theta \in \Theta = [0,2]\).
<br></p>
</li>
<li>
<p>Introduce conditional expectation \(\xi_t = \mathbb E_{t}[y_{t+1}]\) and forecast error \(\eta_t = y_t - \xi_{t-1}\).
<br></p>
</li>
<li>
<p>Thus,</p>
<p>\begin{eqnarray}
\xi_t = \theta \xi_{t-1} - \theta \epsilon_t + \theta \eta_t. \label{eq_lreex}
\end{eqnarray}</p>
</li>
</ul>
<h3 id="a-simple-example">A Simple Example</h3>
<ul>
<li>
<p>Determinacy: \(\theta &gt; 1\). Then only stable solution:</p>
<p>\begin{eqnarray}
\xi_t = 0, \quad \eta_t = \epsilon_t, \quad  y_t = \epsilon_t
\end{eqnarray}</p>
</li>
<li>
<p>Indeterminacy: \(\theta \le 1\) the stability requirement imposes no restrictions on forecast error:</p>
<p>\begin{eqnarray}
\eta_t = \widetilde{M} \epsilon_t + \zeta_t.
\end{eqnarray}</p>
</li>
<li>
<p>For simplicity assume now  \(\zeta_t = 0\). Then</p>
<p>\begin{eqnarray}
y_t - \theta y_{t-1} = \widetilde{M} \epsilon_t - \theta \epsilon_{t-1}.
\label{eq_arma11}
\end{eqnarray}</p>
</li>
<li>
<p>General solution methods for LREs: Blanchard and Kahn (1980), King and Watson (1998), Uhlig (1999),
Anderson (2000), Klein (2000), Christiano (2002), Sims (2002).</p>
</li>
</ul>
<h3 id="solving-a-more-general-system">Solving a More General System</h3>
<ul>
<li>
<p>Canonical form:</p>
<p>\begin{equation}
\Gamma_{0}(\theta)s_{t}=\Gamma_{1}(\theta) s_{t-1}+\Psi
(\theta)\epsilon_t+\Pi (\theta)\eta_{t},
\end{equation}</p>
</li>
<li>
<p>The system can be rewritten as</p>
<p>\begin{equation}
s_{t}=\Gamma _{1}^{\ast }(\theta) s_{t-1}+\Psi^{\ast}(\theta)\epsilon_{t}
+\Pi^{\ast }(\theta)\eta_{t}.
\end{equation}</p>
</li>
<li>
<p>Replace \(\Gamma _{1}^{\ast }\) by  \(J\Lambda J^{-1}\) and define  \(w_{t}=J^{-1}s_{t}\).</p>
</li>
<li>
<p>To deal with repeated eigenvalues and non-singular \(\Gamma_0\) we use Generalized Complex Schur Decomposition (QZ) in practice.</p>
</li>
<li>
<p>Let the \(i\)&rsquo;th element of \(w_{t}\) be \(w_{i,t}\) and denote the \(i\)&rsquo;th
row of \(J^{-1}\Pi ^{\ast }\) and \(J^{-1}\Psi ^{\ast }\) by \([J^{-1}\Pi
^{\ast }]_{i.}\) and \([J^{-1}\Psi ^{\ast }]_{i.}\), respectively.</p>
</li>
</ul>
<h3 id="solving-a-more-general-system">Solving a More General System</h3>
<ul>
<li>
<p>Rewrite model:</p>
<p>\begin{equation}
w_{i,t}=\lambda_{i}w_{i,t-1}+[J^{-1}\Psi ^{\ast }]_{i.} \epsilon_{t}+[J^{-1}\Pi ^{\ast }]_{i.}\eta _{t}.  \label{eq_wit1}
\end{equation}</p>
</li>
<li>
<p>Define the set of stable AR(1) processes as</p>
<p>\begin{equation}
I_{s}(\theta)=\bigg\{i\in \{1,\ldots n\}\bigg|\left\vert \lambda_{i}(\theta)\right\vert	 \le 1\bigg\}
\end{equation}</p>
</li>
<li>
<p>Let \(I_{x}(\theta)\) be its complement. Let \(\Psi _{x}^{J}\) and \(\Pi_{x}^{J}\) be the matrices composed of the row vectors \([J^{-1}\Psi^{\ast }]_{i.}\) and \([J^{-1}\Pi ^{\ast }]_{i.}\) that correspond to unstable eigenvalues, i.e., \(i\in I_{x}(\theta)\).</p>
</li>
<li>
<p>Stability condition:</p>
<p>\begin{equation}
\Psi_{x}^{J}\epsilon_{t}+\Pi_{x}^{J}\eta_{t}=0  \label{eq_stabcond}
\end{equation}</p>
<p>for all \(t\).</p>
</li>
</ul>
<h3 id="solving-a-more-general-system">Solving a More General System</h3>
<ul>
<li>
<p>Solving for \(\eta_t\). Define</p>
<p>\begin{eqnarray}
\Pi_x^J &amp;=&amp; \left[
\begin{array}{cc}
U_{.1} &amp; U_{.2}
\end{array}
\right] \left[
\begin{array}{cc}
D_{11} &amp; 0 \\\
0 &amp; 0
\end{array}
\right] \left[
\begin{array}{c}
V_{.1}^{\prime } \\\
V_{.2}^{\prime }
\end{array}
\right] \label{eq_svd} \\\
&amp;=&amp;\underbrace{U}_{m\times m}\underbrace{D}_{m\times k}\underbrace{V^{\prime }}_{k\times k} \nonumber \\\
&amp;=&amp;\underbrace{U_{.1}}_{m\times r}\underbrace{D_{11}}_{r\times r}\underbrace{V_{.1}^{\prime }}_{r\times k}. \nonumber
\end{eqnarray}</p>
</li>
<li>
<p>If there exists a solution to Eq.~(\ref{eq_stabcond}) that expresses the forecast errors as function of the fundamental shocks \(\epsilon_t\) and sunspot shocks \(\zeta_t\), it is of the form</p>
<p>\begin{eqnarray}
\eta_t &amp;=&amp; \eta_1 \epsilon_t + \eta_2 \zeta_t  \label{eq_etasol} \\\
&amp;=&amp; ( - V_{.1}D_{11}^{-1} U_{.1}^{\prime}\Psi_x^J + V_{.2} \widetilde{M}) \epsilon_t +
V_{.2} M_\zeta \zeta_t,	 \notag
\end{eqnarray}</p>
<p>where \(\widetilde{M}\) is
an \((k-r) \times l\) matrix, \(M_\zeta\) is a \((k-r) \times p\) matrix, and the dimension
of \(V_{.2}\) is \(k\times (k-r)\). The solution is unique if \(k = r\) and \(V_{.2}\)
is zero.</p>
</li>
</ul>
<h3 id="proposition">Proposition</h3>
<p>If there exists a solution to Eq. (\ref{eq_stabcond}) that expresses the forecast errors as function of the
fundamental shocks \(\epsilon_t\) and sunspot shocks \(\zeta_t\), it is of the form</p>
<p>\begin{eqnarray}
\eta_t &amp;=&amp; \eta_1 \epsilon_t + \eta_2 \zeta_t  \label{eq_etasol} \\\
&amp;=&amp; ( - V_{.1}D_{11}^{-1} U_{.1}^{\prime}\Psi_x^J + V_{.2} \widetilde{M}) \epsilon_t +
V_{.2} M_\zeta \zeta_t,	 \notag
\end{eqnarray}</p>
<p>where \(\widetilde{M}\) is
an \((k-r) \times l\) matrix, \(M_\zeta\) is a \((k-r) \times p\) matrix, and the dimension
of \(V_{.2}\) is \(k\times (k-r)\). The solution is unique if \(k = r\) and \(V_{.2}\)
is zero.</p>
<h3 id="at-the-end-of-the-day-dot-dot-dot">At the End of the Day&hellip;</h3>
<ul>
<li>We obtain a transition equation for the vector \(s_t\):
\[
s_{t} = T(\theta) s_{t-1} + R(\theta) \epsilon_{t}.
\]</li>
<li>The coefficient matrices \(T(\theta)\) and \(R(\theta)\) are
functions of the parameters of the DSGE model.</li>
</ul>
<h3 id="measurement-equation">Measurement Equation</h3>
<ul>
<li>
<p>Relate model variables \(s_t\) to observables \(y_t\).</p>
</li>
<li>
<p>In NK model:</p>
<p>\begin{eqnarray*}
YGR_t  &amp;=&amp; \gamma^{(Q)} + 100(\hat y_t - \hat y_{t-1} + \hat z_t) \label{eq_dsge1measure}\\\
INFL_t &amp;=&amp; \pi^{(A)} + 400 \hat \pi_t  \nonumber \\\
INT_t  &amp;=&amp; \pi^{(A)} + r^{(A)} + 4 \gamma^{(Q)} + 400 \hat R_t . \nonumber
\end{eqnarray*}</p>
<p>where
\[
\gamma = 1+ \frac{\gamma^{(Q)}}{100}, \quad \beta = \frac{1}{1+ r^{(A)}/400}, \quad
\pi = 1 + \frac{\pi^{(A)}}{400} .
\]</p>
</li>
<li>
<p>More generically:
\[
y_t = D(\theta) + Z(\theta) s_t \underbrace{+u_t}_{\displaystyle \mbox{optional}}.
\]
The state and measurement equations define a <em>State Space Model</em>.</p>
</li>
</ul>
<h2 id="state-space-models-and-the-kalman-filter">State Space Models and The Kalman Filter</h2>
<p>State space models form a very general class of models that encompass
many of the specifications that we encountered earlier.  VARMA models,
linearized DSGE models, and more can be written in state space form.
State space models are particularly popular at the FRB.  For example,
the models in the \(r^*\) suite can all be written in state space form.</p>
<p>A state space model can be described by two different equations: a
measurement equation that relates an <em>unobservable</em> state vector \(s_t\)
to the <em>observables</em> \(y_t\), and a transition equation that describes
the evolution of the state vector \(s_t\).  For now, we&rsquo;ll restrict
attention to the case in which both of these equations are linear.</p>
<p><strong>Measurement.</strong> The measurement equation is of the form</p>
<p>\begin{eqnarray}
\label{eq:obs}
y_t = D_{t|t-1} + Z_{t|t-1} s_t  + \eta_t , \quad t=1,\ldots,T
\end{eqnarray}</p>
<p>where \(y_t\) is an \(n_y \times 1\) vector of observables, \(s_t\) is an \(n_s
\times 1\) vector of state variables, \(Z_{t|t-1}\) is an \(n_y \times n_s\)
vector, \(D_{t|t-1}\) is a \(n_y\times 1\) vector, and \(\eta_t\) are
innovations (or often ``measurement errors&rsquo;&rsquo;) with mean zero and
\(\mathbb{E}_{t-1}[ \eta_t \eta_t&rsquo;] = H_{t|t-1}\).</p>
<ul>
<li>The matrices \(Z_{t|t-1}\), \(D_{t|t-1}\), and \(H_{t|t-1}\) are in many applications constant (&ldquo;time-invariant.&rdquo;)</li>
<li>However, it is sufficient that they are predetermined at \(t-1\). They could be functions of \(y_{t-1}, y_{t-2}, \ldots\).</li>
<li>To simplify the notation, we will denote them by \(Z_t\), \(D_t\), and \(H_t\), respectively.</li>
</ul>
<p><strong>Transition.</strong> The transition equation is of the form</p>
<p>\begin{eqnarray}
\label{eq:transition}
s_t = C_{t|t-1} + T_{t|t-1} s_{t-1}  + R_{t|t-1} \epsilon_t
\end{eqnarray}</p>
<p>where \(R_t\) is \(n_s \times n_\epsilon\), and \(\epsilon\) is a \(n_\epsilon \times 1\) vector of innovations
with mean zero and variance \(\mathbb{E}_{t|t-1}[ \epsilon_t \epsilon_t&rsquo;] = Q_{t|t-1}\).</p>
<ul>
<li>The assumption that \(s_t\) evolves according to an VAR(1) process
is not very restrictive, since it could be the companion form to a
higher order VAR process.</li>
<li>It is furthermore assumed that (i) expectation and variance of the
initial state vector are given by \(\mathbb E[s_0] = A_0\) and
\(\mathbb V[s_0] = P_0\);</li>
<li>\(\eta_t\) and \(\epsilon_t\) are uncorrelated with each other in all
time periods , and uncorrelated with the initial state. This
assumption is not really necessary, but it simplies things
considerable.</li>
</ul>
<p>The collection of matrices in (<a href="#eq:obs">eq:obs</a>) and (<a href="#eq:transition">eq:transition</a>)
define the state space system.  For that reason, they are often
referred to as the &ldquo;system matrices.&rdquo;</p>
<p><em>Example.</em> Consider the ARMA(1,1) model of the form</p>
<p>\begin{eqnarray}
y_t = \phi y_{t-1} + \epsilon_t + \theta \epsilon_{t-1} \quad \epsilon_t \sim iid{\cal N}(0,\sigma^2)
\end{eqnarray}</p>
<p>The model can be rewritten in state space form</p>
<p>\begin{eqnarray}
y_t &amp; = &amp; [ 1 ; \theta] \left[ \begin{array}{c} \epsilon_t \ \epsilon_{t-1} \end{array} \right] + \phi y_{t-1}\\\
\left[ \begin{array}{c} \epsilon_t \ \epsilon_{t-1} \end{array} \right]
&amp; = &amp;
\left[ \begin{array}{cc} 0 &amp; 0 \ 1 &amp; 0 \end{array} \right]
\left[ \begin{array}{c} \epsilon_{t-1} \ \epsilon_{t-2} \end{array} \right]
+
\left[ \begin{array}{c} \eta_t \ 0 \end{array} \right]
\end{eqnarray}</p>
<p>where \(\epsilon_t \sim iid{\cal N}(0,\sigma^2)\). Thus, the state
vector is composed of \(s_t = [\epsilon_t, \epsilon_{t-1}]&rsquo;\) and \(D_{t}
= \rho y_{t-1}\).  This construction is not unique. We could also write
the model as:</p>
<p>\begin{eqnarray}
y_t &amp; = &amp; [ 1 ; 0] \left[ \begin{array}{c} y_t \ \epsilon_{t} \end{array} \right] \\\
\left[ \begin{array}{c} y_t \ \epsilon_{t} \end{array} \right]
&amp; = &amp;
\left[ \begin{array}{cc} \phi &amp; \theta \ 0 &amp; 0 \end{array} \right]
\left[ \begin{array}{c} y_{t-1} \ \epsilon_{t-1} \end{array} \right]
+
\left[ \begin{array}{c} 1\ 1 \end{array} \right]\epsilon_t.
\end{eqnarray}</p>
<p>Notice in this formulation the state vector \(s_t = [y_t,\epsilon_t]\)
is partially observed.  So it&rsquo;s not true, strictly speaking, that the
entire \(s_t\) vector must be unobserved. \(\Box\)</p>
<p>If the system matrices \(D_t, Z_t, H_t, C_t, T_t, R_t, Q_t\) are
non-stochastic and predetermined, then the system is linear and \(y_t\)
can be expressed as a function of present and past \(\eta_t\)&rsquo;s and
\(\epsilon_t\)&rsquo;s.  We&rsquo;ve done some work on linear systems previously
(VARs), so the natural next step is to expand our toolkit to do the
kinds of things we liked to do with VARs:</p>
<ul>
<li>
<p>Calculate predictions \(y_t|Y^{t-1}\), where \(Y^{t-1} = [ y_{t-1}, \ldots, y_1]\),</p>
</li>
<li>
<p>Obtain a likelihood function</p>
<p>\begin{eqnarray}
\label{eq:likelihood}
p(Y^T| \{Z_t, d_t, H_t, T_t, c_t, R_t, Q_t \}),
\end{eqnarray}</p>
</li>
<li>
<p>and, <em>something we didn&rsquo;t do with VARs</em>, back out a sequence
\[
\left\{ p(s_t |Y^t, \{Z_\tau, d_\tau, H_\tau, T_\tau,
c_\tau, R_\tau, Q_\tau \} ) \right\}.
\]</p>
</li>
</ul>
<p>Now, if the state vector was observed, it would be easy to combine
equation (<a href="#eq:obs">eq:obs</a>) and (<a href="#eq:transition">eq:transition</a>) to obtain a VAR jointly
in \([y_t, s_t]\).  Thus, it would be straightforward to obtain the
(perhaps conditional) likelihood:
\[
p(Y^T,S^T | \{Z_t, d_t, H_t, T_t, c_t, R_t, Q_t \}).
\]
But life is hard, and we don&rsquo;t get to observe \(S^T\).  We need to
compute the likelihood for the data we have, i.e., the likelihood in
(<a href="#eq:likelihood">eq:likelihood</a>).  We have to marginalize out \(S^T\).  It turns out
that their is an algorithm that does this, and fulfills the three
desiderata above.  The algorithm is called the <em>Kalman Filter</em> and was
originally adopted from the engineering literature.</p>
<h3 id="the-kalman-filter">The Kalman Filter</h3>
<p>For this presentation of the Kalman filter, we&rsquo;re going to assume
that the system matrices are time invariant, that is, they do not
depend on \(t\).  So we drop these subscripts from our notation.
Furthermore, we&rsquo;re going to collect them in the vector \(\theta =
[C,T,R,Q,D,Z,H]\), where the \(vec\) operator is being implicitly
applied to each matrix.</p>
<p>We&rsquo;re also going to assume that the innovations \(\eta_t\) and
\(\epsilon_t\) are normally distributed.  We need to this to obtain
an exact likelihood, although the Kalman filter can
be used to obtain an optimal&mdash;in terms of MSE&mdash;predictor \(y_{t+h}\)
given \(Y^T\) for \(h \ge 1\) using linear projections, regardless of
the parametric distributions for \(\eta_t\) and \(\epsilon_t\).  The
chapter on state space models in <sup id="adec714ae69bef54c5ee79cfcb41955d"><a href="#Hamilton" title="James Hamilton, Time Series Analysis, Princeton University Press (1994).">Hamilton</a></sup> derives this. In
this case the likelihood calculation delivers a quasi-likelihood.</p>
<p>With our normality assumption, the derivation of the Kalman filter
has a natural Bayesian interpretation.  Before we proceed, we&rsquo;re
going to state some results about multivariate normal
distributions, which will help later on.</p>
<p><em>Lemma.</em> Let \((x&rsquo;,y&rsquo;)&rsquo;\) be jointly normal with
\[
\mu = \left[ \begin{array}{c} \mu_x \ \mu_y \end{array} \right]
\quad \mbox{and} \quad
\Sigma = \left[ \begin{array}{cc} \Sigma_{xx} &amp; \Sigma_{xy} \\\
\Sigma_{yx} &amp; \Sigma_{yy} \end{array} \right]
\]
Then the \(pdf(x|y)\) is multivariate normal with</p>
<p>\begin{eqnarray}
\mu_{x|y} &amp;=&amp; \mu_x + \Sigma_{xy} \Sigma_{yy}^{-1}(y - \mu_y) \\\
\Sigma_{xx|y} &amp;=&amp; \Sigma_{xx} - \Sigma_{xy} \Sigma_{yy}^{-1} \Sigma_{yx}
\end{eqnarray}</p>
<p>Note that the converse is not necessarily true. \(\Box\)</p>
<p>In both theory and practice, the Kalman filter proceeds
recursively, using the natural prior-posterior sequencing, after an
initialization.</p>
<p><em>Initialization.</em> We&rsquo;re going to start at period \(t=0\), that is,
the period before we first observe \(y\).  We assume that \(s_0\) is
normally distributed:</p>
<p>\begin{align}
s_0 | \theta \sim \mathcal N\left(A_0, P_0\right).
\end{align}</p>
<p>Importantly, we conceptualize this distribution as prior
distribution.  We&rsquo;ll discuss possible ways to select \(A_0\) and
\(P_0\) in a bit.</p>
<p><em>Prediction.</em> We can combine our prior distribution for \(s_0\) with
the state transition equation (<a href="#eq:transition">eq:transition</a>).  Since \(s_0\) is
normally distributed and \(\epsilon_1\) is also normally distributed
(and independent of \(s_0\)), \(s_1\) is also normally distributed,
\[
s_1 | \theta \sim \mathcal N\left(A_{1|0}, P_{1|0}\right)
\]
where
\[
A_{1|0} = C+T A_{0} \mbox{ and } P_{1|0} = T P_0 T&rsquo; + RQR&rsquo;.
\]
Note that this is the unconditional distribution of \(s_1\), a prior
distribution for \(s_1\) before seeing \(y_1\).  We write the mean
\(A_{1|0}\) and \(P_{1|0}\) as conditional on time \(t=0\).</p>
<p>Next consider the prediction of \(y_1\).  The conditional
distribution of \(y_1\) is of the form</p>
<p>\begin{eqnarray}
y_1|s_1,\theta  \sim {\cal N}(D+Z s_1, H)
\end{eqnarray}</p>
<p>Since \(s_1 \sim {\cal N}( A_{1|0}, P_{1|0})\), we can deduce that
the marginal distribution of \(y_1\) is of the form</p>
<p>\begin{eqnarray}
y_1|\theta  \sim {\cal N} (\hat{y}_{1|0}, F_{1|0})
\end{eqnarray}</p>
<p>where</p>
<p>\begin{eqnarray*}
\hat{y}_{1|0} = D + Z A_{1|0} \mbox{ and }  F_{1|0} =  Z P_{1|0} Z&rsquo; + H.
\end{eqnarray*}</p>
<p>Here we&rsquo;ve been explicit in going \(s_0 \rightarrow s_1 \rightarrow
y_1\).</p>
<p><em>Updating.</em> Another way to see this is to rewrite the
observation equation (<a href="#eq:obs">eq:obs</a>) in terms of \(s_{t-1}\) and
\(\epsilon_t\).  If \(s_{0}\) is normally distributed as above it&rsquo;s
easy to see that \(s_1\) and \(y_1\) are jointly normally distributed with
the marginal and conditional distributions mentioned above.  We have:</p>
<p>\begin{eqnarray}
s_1 &amp;=&amp; C + T s_0 + R\epsilon_t \\\
y_1 &amp;=&amp; D + Z T s_0 + Z\epsilon_t + \eta_t.
\end{eqnarray}</p>
<p>Direct calculation yields:</p>
<p>\begin{eqnarray}
\begin{bmatrix}s_1 \ y_1 \end{bmatrix} \bigg| \theta \sim \mathcal N \left( \begin{bmatrix} A_{1|0} \ \hat y_{1|0} \end{bmatrix}, \begin{bmatrix} P_{1|0} &amp; P_{1|0} Z&rsquo; \ Z P_{1|0} &amp; F_{1|0} \end{bmatrix}\right).
\end{eqnarray}</p>
<p>Consider the third goal of toolbox: delivering
\(p(s_1|y_1,\theta)\). Well, we can get that easily using the formula
for the conditional normal distribution:</p>
<p>\begin{align}
\label{eq:update}
s_1 | y_1 \sim N\left( A_{1|0} + P_{1|0} Z&rsquo; F_{1|0}^{-1} \left(y_1 - \hat y_{1|0}\right), P_{1|0} - P_{1|0} Z&rsquo; F_{1|0}^{-1} Z P_{1|0}\right).
\end{align}</p>
<p>Note that we could have instead obtained this using:</p>
<p>\begin{align}
\label{eq:bayes}
p(s_1 | y_1,\theta) \propto p(y_1|s_1,\theta) p(s_1|\theta),
\end{align}</p>
<p>i.e., our good friend Bayes rule!  Note the conjugacy
(normal-normal) likelihood-prior relationship yields a normally
distributed posterior.  Finally, let&rsquo;s call give our updated state mean and variance:</p>
<p>\begin{align}
\label{eq:ms}
A_1 = A_{1|0} + P_{1|0} Z&rsquo; F_{1|0}^{-1} \left(y_1 - \hat y_{1|0}\right) \mbox{ and } P_1 = P_{1|0} - P_{1|0} Z&rsquo; F_{1|0}^{-1} Z P_{1|0}.
\end{align}</p>
<p><em>Generalization.</em> Now, with the distribution form
\(s_1|y_1,\theta\), we&rsquo;re back where we started!  So all we have to
do is construct \(s_2|y_1,\theta\) and \(y_2 | s_2, y_1, \theta\) in an
identical fashion as above, and so on for \(t = 2,\ldots,T\).  We
can summarize the recursions:</p>
<ol>
<li>
<p><strong>Initialization.</strong>  Set \(s_0 \sim N(A_0,P_0).\)</p>
</li>
<li>
<p><strong>Recursions.</strong> For \(t=1,\ldots,T\):</p>
<p>\begin{align}
\label{eq:state-prediction}
\mbox{state prediction}&amp;: A_{t|t-1} = C+T A_{t-1} \mbox{ and } P_{t|t-1} = T P_{t-1} T&rsquo; + RQR&rsquo;.\\\
\label{eq:obs-prediction}
\mbox{observation prediction}&amp;: \hat y_{t|t-1} = D + Z A_{t|t-1} \mbox{ and } F_{t|t-1} = Z P_{t|t-1} Z&rsquo; + H. \\\
\label{eq:state-update}
\mbox{state update}&amp;: A_t = A_{t|t-1} + P_{t|t-1} Z&rsquo; F_{t|t-1}^{-1} \left(y_t - \hat y_{t|t-1}\right) \mbox { and } \nonumber \\\
&amp;~~ P_t = P_{t|t-1} - P_{t|t-1} Z&rsquo; F_{t|t-1}^{-1} Z P_{t|t-1}.
\end{align}</p>
</li>
</ol>
<p><em>Likelihood function.</em> We can define the one-step ahead forecast error</p>
<p>\begin{eqnarray}
\nu_t = y_t - \hat{y}_{t|t-1} =  Z (s_t - A_{t|t-1}) + \eta_t.
\end{eqnarray}</p>
<p>The likelihood function is given by</p>
<p>\begin{eqnarray}
p(Y^T | \theta )
&amp; = &amp; \prod_{t=1}^T p(y_t|Y^{t-1}, \theta) \nonumber \\\
&amp; = &amp; ( 2 \pi)^{-n_yT/2} \left( \prod_{t=1}^T |F_{t|t-1}| \right)^{-1/2} \times \exp \left\{ - \frac{1}{2} \sum_{t=1}^T \nu_t F_{t|t-1}^{-1} \nu_t&rsquo; \right\}
\end{eqnarray}</p>
<p>This representation of the likelihood function is often called prediction
error form, because it is based on the recursive prediction one-step ahead
prediction errors \(\nu_t\). \(\Box\)</p>
<h4 id="discussion">Discussion</h4>
<p><em>Initialization.</em> First, on the initialization step, if the
system-matrices are time-invariant and the process for \(s_t\) is
stationary (i.e., all the eigenvalues of \(T\) are less than one in
magnitude), it might make sense to initialize the Kalman filter
from the invariant distribution, i.e., we have \(A_0\) and \(P_0\) such that
\[
A_0 = (I_{n_s} - T)^{-1} C \mbox{ and } P_0 = T P_0 T&rsquo; + RQR&rsquo;.
\]
If the system is not too big, you can solve for \(P_0\) directly
using the \(vec\) operator:
\[
vec(P_0) = \left(I_{n_s^2} - (T&rsquo; \otimes T)\right)^{-1} RQR&rsquo;.
\]
Otherwise, there are algorithms available for computing \(P_0\)
reliably and quickly.</p>
<p>If the system is not stationary, it&rsquo;s common practice to set the
variance of \(P_0\) be extremely large, like \(1000\times I_{n_s}\).</p>
<p><em>Kalman Gain.</em> In (<a href="#eq:state-update">eq:state-update</a>), the matrix that maps the
prediction errors, \(\nu_t\), into the state revision is important
enough to warrant it&rsquo;s own name: the Kalman Gain.  The Kalman Gain,
\[
K_t = P_{t|t-1} Z F_{t|t-1}^{-1},
\]
is an \(n_s\times n_y\) matrix that maps the &ldquo;surprises&rdquo; (forecast
errors) in the observed data to changes in our beliefs about the
underlying unobserved states.  Essentially, the gain tells us how
we learn about the states from the data.</p>
<p><em>Time-varying system matrices and missing data.</em> The Kalman filter
recursions in (<a href="#eq:state-prediction">eq:state-prediction</a>), (<a href="#eq:obs-prediction">eq:obs-prediction</a>),
and (<a href="#eq:state-update">eq:state-update</a>) are valid if the system matrices are
time-varying (but pre-determined.)  In practice, it is simply a
matter of adding the relevant subscripts onto the system matrices.
An important case of time-varying system matrices is when they are
constant except for the fact that some of the observations are
missing; i.e, for some \(t\), at least one element of \(y_t\) is
missing.  In this case, we simply modify the observation equation
(<a href="#eq:obs">eq:obs</a>)&mdash;and hence, (<a href="#eq:obs-prediction">eq:obs-prediction</a>) and
(<a href="#eq:state-update">eq:state-update</a>)&mdash;in order to account for the fact that we
observe fewer series at some periods.  Suppose in period \(t\) we
observe \(n_{y_t}\), which is less than or equal to \(n_y\).  Define the
\(n_{y_t} \times n_y\) select matrix \(M_t\), to be the matrix whose
columns are comprised of \(\{e_i : i\mbox{th series is
observed}\}\), where \(e_i\) is the \(n_y\times 1\) vector with a
one in the \(i\)th position and zeros elsewhere.  Then,</p>
<p>\begin{align}
\label{eq:missing}
D_t = M_t D,~~ Z_t = M_t Z, \mbox{ and } H_t = M_t H M_t&rsquo;.
\end{align}</p>
<p>The ability to handle missing data is an extremely powerful feature
of the Kalman filter, as it allows us to both handle estimating
models with missing data, and make inference about the missing data
itself.  More on this later.  Most programmed Kalman filter
routines can handle missing data without an modification of the
system matrices on the part of the user.  Simply code your missing
data as <code>nan</code>.  Finally, note that the likelihood calculation in
(<a href="#eq:likelihood">eq:likelihood</a>) needs to be modified (i.e., \(n_y\) needs to be
replaced by \(n_{y_t}\).)  Again, preprogrammed routines should
handle this without user intervention.</p>
<p><em>&ldquo;Steady-state&rdquo; Kalman filter.</em> Suppose the system matrices are constant. If we combine
(<a href="#eq:state-prediction">eq:state-prediction</a>), (<a href="#eq:obs-prediction">eq:obs-prediction</a>) (<a href="#eq:state-update">eq:state-update</a>) for the state
variance, we obtain</p>
<p>\begin{eqnarray}
P_{t+1|t} = T P_{t|t-1}T&rsquo; + RQR - T P_{t|t-1} Z&rsquo; (Z P_{t|t-1} Z&rsquo; + H)^{-1} Z P_{t|t-1}T'
\end{eqnarray}</p>
<p>with \(P_{0|-1} = P_0.\) This equation is known as the <em>matrix
Riccati recursion</em>, a discrete time analogue to the popular set of
ODEs.  Under some regularity conditions, as \(t\) gets sufficiently
large, \(P_{t+1|t} \rightarrow \bar P\), i.e., there is an
invariant solution to the Riccati equation.  Some people refer to
this as the &ldquo;steady-state&rdquo; prediction variance (and
correspondingly, the &ldquo;steady-state&rdquo; Kalman gain.)  It can be useful
in computation as well: after a sufficiently amount of time, one
does not need to continue to update \(P_{t|t-1}\), which is the
typically the costliest part of evaluating the Kalman filter.  Note
this also makes clear that the variances in the Kalman filter to
not depend on the observed data.</p>
<p><em>Innovations representation of the Kalman Filter.</em> TBD</p>
<p><em>Caution.</em> Some authors adopt a slightly different timing
convention with the Kalman Filter; specifically,
<sup id="24ddf79004b9c9c124e26e4c2a10a17e"><a href="#DurbinKoopman2001" title="Durbin \&amp; Koopman, Time Series Analysis by State Space Methods, Oxford University Press (2001).">DurbinKoopman2001</a></sup>.  The initialization of the filter changes
slightly.  It&rsquo;s all very tedious.</p>
<h4 id="kalman-smoothing">Kalman Smoothing</h4>
<p>Note that the Kalman filter is a <em>filter</em>: it delivers the
sequence of smoothed distribution \(\{s_t | Y^{t}\}_{t=1}^T\),
which since they are normal, are simply described by the sequence
\(\{A_t,P_t\}_{t=1}^T\).  Sometimes, we interested in the
<em>smoothed</em> distributions, \(\{s_t | Y^T\}_{t=1}^T\), that is
distributions of the unobserved states conditional on all of the
data.  These distributions are also normally distributed, and can
be found another recursive algorithm known as the Kalman smoother.</p>
<p><em>Details TBD.</em></p>
<h4 id="drawing-from-the-smoothed-distribution">Drawing from the Smoothed Distribution</h4>
<p><em>TBD</em>.</p>
<h3 id="an-example-gdp-plus">An Example: GDP+</h3>
<p>Here I&rsquo;m going to through a simple state space model described in
<sup id="4a8176e7e49ddd5d4c003a31f7a8fe04"><a href="#Aruoba_2016" title="Aruoba, Diebold, , Nalewaik, Schorfheide, Song \&amp; Dongho, Improving GDP measurement: A measurement-error  perspective, {Journal of Econometrics}, v(2), 384 397 (2016).">Aruoba_2016</a></sup>.  Since GDP data is inherently noisy, the authors
use both income-side \((GDP_{It})\) and expenditure-side
\((GDP_{Et})\) data on GDP growth to infer the true (unobserved) growth rate,
\(GDP_t\).  The authors posit that the true growth rate follows an AR(1):</p>
<p>\begin{align}
\label{eq:gdp}
GDP_t = \mu (1-\rho) + \rho GDP_{t-1} + \epsilon_t, \quad \epsilon_t \sim IID N(0, \sigma^2).
\end{align}</p>
<p>An that both income- and expenditure-side estimates are mismeasured versions of this:</p>
<p>\begin{align}
\label{eq:gdp-measure}
\begin{array}{c}
GDP_{Et} \ GDP_{It}
\end{array}
\bigg| GDP_t
\sim IID N \left(
\left[
\begin{array}{c}
GDP_t \ GDP_t
\end{array}
\right],
\left[
\begin{array}{cc}
\sigma_{E}^2 &amp; 0 \ 0 &amp; \sigma_{I}^2
\end{array}
\right]\right)
\end{align}</p>
<p>We can cast this into state space form with \(n_y = 2\) and \(n_s = n_\epsilon = 1\).  We have</p>
<p>\begin{align}
C = \mu(1-\rho),~~ T = \rho,~~ R = 1, \mbox{ and } Q = \sigma^2, \nonumber \\\
D = \begin{bmatrix}0 \ 0 \end{bmatrix}, ~~ Z = \begin{bmatrix}1 \ 1 \end{bmatrix}, \mbox{ and } H =     \left[
\begin{array}{cc}
\sigma_{E}^2 &amp; 0 \ 0 &amp; \sigma_{I}^2
\end{array}
\right].
\end{align}</p>
<p>Here&rsquo;s a look at the data:</p>
<p><img src="https://edherbst.net/ox-hugo/8f8c7ed886ee0fc55dde9d8e51bd3f65cc2ca7fc.png" alt="">
We can use the Kalman filter to maximize the likelihood function,
since we haven&rsquo;t quite worked out how to elicit the posterior of
this model just yet.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Initial likelihood:  -4293.835439078496
</span></span><span style="display:flex;"><span>      fun: 358.8810028467478
</span></span><span style="display:flex;"><span> hess_inv: &lt;5x5 LbfgsInvHessProduct with dtype=float64&gt;
</span></span><span style="display:flex;"><span>      jac: array([-1.08002496e-04, -1.70530257e-05, -3.12638804e-04, -2.95585778e-04,
</span></span><span style="display:flex;"><span>       -2.16004992e-04])
</span></span><span style="display:flex;"><span>  message: b&#39;CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH&#39;
</span></span><span style="display:flex;"><span>     nfev: 108
</span></span><span style="display:flex;"><span>      nit: 14
</span></span><span style="display:flex;"><span>   status: 0
</span></span><span style="display:flex;"><span>  success: True
</span></span><span style="display:flex;"><span>        x: array([0.50974449, 0.39613152, 0.28275435, 0.40326539, 0.64044755])
</span></span><span style="display:flex;"><span>Maximized Likelihood:  -358.8810028467478
</span></span><span style="display:flex;"><span>{&#39;rho&#39;: 0.5097444850915837, &#39;mu&#39;: 0.39613152196112617, &#39;sige&#39;: 0.2827543510275178, &#39;sigi&#39;: 0.4032653925782693, &#39;sig&#39;: 0.6404475458159359}
</span></span></code></pre></div><p>We these point estimates, we can use the kalman filter to extract
\(\{ A_t\}_{t=1}^T\), the filtered means of the &ldquo;true&rdquo; GDP series.
We&rsquo;ll plot them along with the observables, and the simple average
of expenditure-side and income-side GDP estimates.</p>
<figure><img src="https://edherbst.net/ox-hugo/cb6ce1f5d1fa3c6a180eb701c9ed82c3b28b84cb.png">
</figure>

<p>To make it a bit easier to see, let&rsquo;s look at the last five or so years.</p>
<p><img src="https://edherbst.net/ox-hugo/29b3789b20a0fa50d6408b5db6a61177c58f491c.png" alt="">
You can see the average is different form the filtered estimate.
Why is that?  The Kalman Gain matrix places different weights on
the income- and expenditure-side GDP data.</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>0.4217678091997104</td>
          <td>0.42176780919971</td>
      </tr>
  </tbody>
</table>
<h2 id="bibliography">Bibliography</h2>
<h3 id="references">References</h3>
<h1 id="bibliography-1">Bibliography</h1>
<p><a id="woodford_2003"></a>[woodford_2003] Woodford, Interest and Prices, Princeton University Press (2003). <a href="#a387326a2038fd7309392dc655b58018">↩</a></p>
<p><a id="Gali2008"></a>[Gali2008] Jordi Gal'i, Monetary Policy, Inflation, and the Business Cycle: An Introduction to the New Keynesian Framework, Princeton University Press (2008). <a href="#e501ccdeda2f0731e31cee0bb583687c">↩</a></p>
<p><a id="ireland2004"></a>[ireland2004] Ireland, A Method for Taking Models to the Data, <i>Journal of Economic Dynamics and Control</i>, <b>28(6)</b>, 1205-1226 (2004). <a href="#29dbe0ba987d7fc6fe70b0eb8b9164a8">↩</a></p>
<p><a id="christiano2005"></a>[christiano2005] Christiano, Eichenbaum, Evans &amp; , Nominal Rigidities and the Dynamic Effects of a Shock to Monetary  Policy, <i>Journal of Political Economy</i>, <b>113(1)</b>, 1-45 (2005). <a href="#47048f0095d5c18f125a005804f82697">↩</a></p>
<p><a id="Smets2007"></a>[Smets2007] Smets &amp; Wouters, Shocks and Frictions in US Business Cycles: A Bayesian DSGE Approach, <i>American Economic Review</i>, <b>97</b>, 586-608 (2007). <a href="#0d8b307a60fe1d0ae3cf2a838eca7a27">↩</a></p>
<p><a id="An2007b"></a>[An2007b] An &amp; Frank Schorfheide, Bayesian Analysis of DSGE Models, <i>Econometric Reviews</i>, <b>26(2-4)</b>, 113-172 (2007). <a href="http://dx.doi.org/10.1080/07474930701220071">doi</a>. <a href="#188111b01398a8806c0a2b9d9be3fd1c">↩</a></p>
<p><a id="Harvey1991"></a>[Harvey1991] Andrew Harvey, Forecasting, Structural Time Series Models and the Kalman Filter, University of Cambridge Press (1991). <a href="#6ca9488bf724ffa1b1742c738fcd1634">↩</a></p>
<p><a id="Hamilton"></a>[Hamilton] James Hamilton, Time Series Analysis, Princeton University Press (1994). <a href="#adec714ae69bef54c5ee79cfcb41955d">↩</a></p>
<p><a id="HerbstSchorfheide2015"></a>[HerbstSchorfheide2015] Edward Herbst &amp; Frank Schorfheide, Bayesian Estimation of DSGE Models, Princeton University Press (2015). <a href="#f275eaf93510eb80c8e1a928b194e45f">↩</a></p>
<p><a id="Fern_ndez_Villaverde_2016"></a>[Fern_ndez_Villaverde_2016] Fernandez-Villaverde, Rubio-Ramirez, &amp; Schorfheide, Solution and Estimation Methods for DSGE Models, <i>Handbook of Macroeconomics</i>,  527 724 (2016). <a href="http://dx.doi.org/10.1016/bs.hesmac.2016.03.006">link</a>. <a href="http://dx.doi.org/10.1016/bs.hesmac.2016.03.006">doi</a>. <a href="#fd709478b3c27866339020943ed1cd0a">↩</a></p>
<p><a id="Sims2002"></a>[Sims2002] Sims, Solving Linear Rational Expectations Models, <i>Computational Economics</i>, <b>20</b>, 1-20 (2002). <a href="#44a5cfd4aac55ac2b563d69becd9f5ad">↩</a></p>
<p><a id="DurbinKoopman2001"></a>[DurbinKoopman2001] Durbin &amp; Koopman, Time Series Analysis by State Space Methods, Oxford University Press (2001). <a href="#24ddf79004b9c9c124e26e4c2a10a17e">↩</a></p>
<p><a id="Aruoba_2016"></a>[Aruoba_2016] Aruoba, Diebold, , Nalewaik, Schorfheide, Song &amp; Dongho, Improving GDP measurement: A measurement-error  perspective, <i>Journal of Econometrics</i>, <b>191(2)</b>, 384 397 (2016). <a href="http://dx.doi.org/10.1016/j.jeconom.2015.12.009">link</a>. <a href="http://dx.doi.org/10.1016/j.jeconom.2015.12.009">doi</a>. <a href="#4a8176e7e49ddd5d4c003a31f7a8fe04">↩</a></p>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] },
  tex2jax: {
      inlineMath: [['$','$'],['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script type="text/javascript"
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
