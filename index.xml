<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ED HERBST on Ed Herbst</title>
    <link>https://edherbst.net/</link>
    <description>Recent content in ED HERBST on Ed Herbst</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Jan 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://edherbst.net/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Forecasting With DSGE Models</title>
      <link>https://edherbst.net/research/dsge-forecasting-handbook/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/dsge-forecasting-handbook/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bias in Local Projections</title>
      <link>https://edherbst.net/research/bias-in-local-projections/</link>
      <pubDate>Sat, 24 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/bias-in-local-projections/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Short-term Planning, Monetary Policy, and Macroeconomic Persistence</title>
      <link>https://edherbst.net/research/woodford/</link>
      <pubDate>Mon, 15 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/woodford/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Particle Filter</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/07-nonlinear-dsge-models-and-particle-filters/</link>
      <pubDate>Fri, 11 Dec 2020 09:47:16 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/07-nonlinear-dsge-models-and-particle-filters/</guid>
      <description>Nonlinear DSGE Models From Linear to Nonlinear DSGE Models While DSGE models are inherently nonlinear, the nonlinearities are often small and decision rules are approximately linear. One can add certain features that generate more pronounced nonlinearities: stochastic volatility; markov switching coefficients; asymmetric adjustment costs; occasionally binding constraints. From Linear to Nonlinear DSGE Models Linear DSGE model leads to
\begin{eqnarray*} y_t &amp;amp;=&amp;amp; \Psi_0(\theta) + \Psi_1(\theta)t + \Psi_2(\theta) s_t + u_t, \quad u_t \sim N(0,\Sigma_u) ,\\\ s_t &amp;amp;=&amp;amp; \Phi_1(\theta)s_{t-1} + \Phi_\epsilon(\theta) \epsilon_t, \quad \epsilon_t \sim N(0,\Sigma_\epsilon).</description>
    </item>
    
    <item>
      <title>Particle MCMC and SMC^2</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/08-pmcmc-and-smc-squared/</link>
      <pubDate>Fri, 11 Dec 2020 09:47:02 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/08-pmcmc-and-smc-squared/</guid>
      <description>Introduction Embedding PF Likelihoods into Posterior Samplers Likelihood functions for nonlinear DSGE models can be approximated by the PF. We will now embed the likelihood approximation into a posterior sampler: PFMH Algorithm (a special case of PMCMC). Embedding PF Likelihoods into Posterior Samplers Distinguish between: \(\{ p(Y|\theta), p(\theta|Y), p(Y) \}\), which are related according to: \[ p(\theta|Y) = \frac{p(Y|\theta) p(\theta)}{p(Y)} , \quad p(Y) = \int p(Y|\theta) p(\theta) d\theta \] \(\{ \hat{p}(Y|\theta), \hat{p}(\theta|Y), \hat{p}(Y) \}\), which are related according to: \[ \hat{p}(\theta|Y) = \frac{\hat{p}(Y|\theta) p(\theta)}{\hat{p}(Y)} , \quad \hat{p}(Y) = \int \hat{p}(Y|\theta) p(\theta) d\theta.</description>
    </item>
    
    <item>
      <title>Monte Carlo Simulation</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/03-monte-carlo-simulation/</link>
      <pubDate>Fri, 11 Dec 2020 09:46:48 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/03-monte-carlo-simulation/</guid>
      <description>Importance Sampling The main event Inference: Need to characterize posterior \(p(\theta|Y)\). Unfortunately, for many interesting models it is not possible to evaluate the moments and quantiles of the posterior \(p(\theta|Y)\) analytically.
Rules of game: we can only numerically evaluate prior \(p(\theta)\) and likelihood \(p(Y|\theta)\).
To evaluate posterior moments of function \(h(\theta)\), we need numerical techniques.
Estimating Posterior Moments We will often abbreviate posterior distributions \(p(\theta|Y)\) by \(\pi(\theta)\) and posterior expectations of \(h(\theta)\) by \[ \mathbb{E}_\pi[h] = \mathbb{E}_\pi[h(\theta)] = \int h(\theta) \pi(\theta) d\theta = \int h(\theta) p(\theta|Y) d\theta.</description>
    </item>
    
    <item>
      <title>Advanced MCMC: Hamiltonian Monte Carlo</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/10-hamiltonian-monte-carlo/</link>
      <pubDate>Fri, 11 Dec 2020 09:46:32 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/10-hamiltonian-monte-carlo/</guid>
      <description>Introduction Hamiltonian Monte Carlo We previously spoke about how much the efficacy of MCMC algorithms depended on the shape of the posterior. We&amp;rsquo;re going to talk about posterior simulators that make that idea explicit. In particular, the Hamiltonian Monte Carlo sampler described in \cite{Neal_2011}.
Details Hamiltonian Monte Carlo adapts methods from the study of molecular dynamics: simulate the motion of molecules based on Newton&amp;rsquo;s laws. The systems which describe the evolution of molecules over time exhibit so-called Hamiltonian dynamics The state of the system at any point in time is summarized by a pair \((\theta, p)\).</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/00-introduction/</link>
      <pubDate>Fri, 11 Dec 2020 09:46:18 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/00-introduction/</guid>
      <description>Introduction Hello! My name is Ed Herbst. I&amp;rsquo;m currently an economist at the Federal Reserve Board.
I&amp;rsquo;m interested in (Bayesian) macroeconometrics, and I&amp;rsquo;m excited to spend the next two weeks talking about it with you!
The next two weeks The syllabus has a rough plan of where we&amp;rsquo;re going.
But, in my experience, there is usually some re-optimization.
If there&amp;rsquo;s something you&amp;rsquo;d like to talk about, or spend more (or less) time on, just let me know.</description>
    </item>
    
    <item>
      <title>Estimating Three DSGE Models</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/06-estimating-a-linear-dsge-model/</link>
      <pubDate>Fri, 11 Dec 2020 09:46:05 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/06-estimating-a-linear-dsge-model/</guid>
      <description>Three DSGE Models Application 1: A New Keynesian Model with Correlated Shocks The assumption that exogenous shocks evolve according to independent AR(1) is to some extent arbitrary.
Trying to generalize this assumption seems natural.
However, the more elaborate the exogenous propagation mechanism, the more difficult it becomes to disentangle endogenous from exogenous propagation.
This generates identification problems.
Application 1: A New Keynesian Model with Correlated Shocks Technology growth shock \(\hat{z}_t\), government spending shock \index{government!</description>
    </item>
    
    <item>
      <title>Sequential Monte Carlo</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/05-sequential-monte-carlo/</link>
      <pubDate>Fri, 11 Dec 2020 09:45:51 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/05-sequential-monte-carlo/</guid>
      <description>Introduction MCMC: What works and what doesn&amp;rsquo;t, Simple Model State-space representation:
\begin{align} y_t = [\begin{array}{cc} 1 &amp;amp; 1 \end{array} ] s_t, \quad s_t = \left[ \begin{array}{cc} {\color{blue} \phi_1} &amp;amp; 0 \ {\color{blue} \phi_3} &amp;amp; {\color{blue} \phi_2} \end{array} \right] s_{t-1} + \left[ \begin{array}{c} 1 \ 0 \end{array} \right] \epsilon_t. \label{eq_exss} \end{align}
The state-space model can be re-written as ARMA(2,1) process \[ (1- {\color{blue} \phi_1} L)(1-{\color{blue} \phi_2} L) y_t = (1-({\color{blue} \phi_2} - {\color{blue} \phi_3} )L) \epsilon_t.</description>
    </item>
    
    <item>
      <title>Markov Chain Monte Carlo</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/04-metropolis-hastings/</link>
      <pubDate>Fri, 11 Dec 2020 09:45:35 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/04-metropolis-hastings/</guid>
      <description>Metropolis-Hastings Algorithm The Metropolis-Hastings Algorithm Metropolis-Hastings (MH) algorithm belongs to the class of Markov chain Monte Carlo (MCMC) algorithms. Algorithm constructs a Markov chain such that the stationary distribution associated with this Markov chain is unique and equals the posterior distribution of interest. First version constructed by Metropolis1953. Later generalized by Hastings1970. Tierney1994 proved important convergence results for MCMC algorithms. Introduction: Chib1995a. Textbook Robert2004 or Geweke2005. Markov Chain Monte Carlo Importance sampler generates a sequence of independent draws from the posterior distribution \(\pi(\theta)\), the MH algorithm generates a sequence of serially correlated draws.</description>
    </item>
    
    <item>
      <title>Some Notes on the Kalman Filter</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/notes/kalman-filter/</link>
      <pubDate>Fri, 11 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/notes/kalman-filter/</guid>
      <description>State space models form a very general class of models that encompass many of the specifications that we encountered earlier. VARMA models, linearized DSGE models, and more can be written in state space form. State space models are particularly popular at the FRB. For example, the models in the \(r^*\) suite can all be written in state space form.
A state space model can be described by two different equations: a measurement equation that relates an unobservable state vector \(s_t\) to the observables \(y_t\), and a transition equation that describes the evolution of the state vector \(s_t\).</description>
    </item>
    
    <item>
      <title>Introduction Bayes 6: (Linear) State Space Models</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/02-linear-dsge-models-and-the-kalman-filter/</link>
      <pubDate>Tue, 27 Oct 2020 21:06:35 -0400</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/02-linear-dsge-models-and-the-kalman-filter/</guid>
      <description>Intro Background Textbook treatments: woodford_2003, Gali2008 Key empirical papers: ireland2004, christiano2005, Smets2007, An2007b, Frequentist estimation: Harvey1991, Hamilton, Bayesian estimation: HerbstSchorfheide2015 A DSGE Model Small-Scale DSGE Model Intermediate and final goods producers Households Monetary and fiscal policy Exogenous processes Equilibrium Relationships Final Goods Producers Perfectly competitive firms combine a continuum of intermediate goods: \[ Y_t = \left( \int_0^1 Y_t(j)^{1-\nu} dj \right)^{\frac{1}{1-\nu}}. \] Firms take input prices \(P_t(j)\) and output prices \(P_t\) as given; maximize profits \[ \Pi_t = P_t \left( \int_0^1 Y_t(j)^{1-\nu} dj \right)^{\frac{1}{1-\nu}} - \int_{0}^1 P_t(j)Y_t(j)dj.</description>
    </item>
    
    <item>
      <title>A Crash Course In Bayesian Inference</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/01-a-crash-course-in-bayesian-inference/</link>
      <pubDate>Tue, 27 Oct 2020 19:22:48 -0400</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/01-a-crash-course-in-bayesian-inference/</guid>
      <description>Introduction These notes are available as slides and a Jupyter notebook.
Modes of Inference Previously, we focussed on frequentist inference (repeated sampling prodecures) measures of accuracy and performance that we used to assess the statistical procedures were pre-experimental However, many statisticians and econometricians believed that post-experimental reasoning should be used to assess inference procedures wherein only the actual observation \(Y^T\) is relevant and not the other observations in the sample space that could have been observed Example Suppose \(Y_1\) and \(Y_2\) are independently and identically distributed and \[ P_\theta \{ Y_i = \theta-1 \} = \frac{1}{2}, \quad P_\theta \{ Y_i = \theta+1 \} = \frac{1}{2} \] Consider the following confidence set</description>
    </item>
    
    <item>
      <title>Online Estimation of DSGE Models</title>
      <link>https://edherbst.net/research/online-estimation-of-dsge-models/</link>
      <pubDate>Wed, 09 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/online-estimation-of-dsge-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Factor Structure of Disagreement</title>
      <link>https://edherbst.net/research/factor-structure-of-disagreement/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/factor-structure-of-disagreement/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Forward Guidance with Bayesian Learning and Estimation</title>
      <link>https://edherbst.net/research/learning/</link>
      <pubDate>Sun, 10 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tempered Particle Filtering</title>
      <link>https://edherbst.net/research/tpf/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/tpf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MONETARY POLICY, REAL ACTIVITY, AND CREDIT SPREADS: EVIDENCE FROM BAYESIAN PROXY SVARS </title>
      <link>https://edherbst.net/research/bpsvar/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/bpsvar/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ESTIMATING (MARKOV-SWITCHING) VARS WITHOUT GIBBS SAMPLING</title>
      <link>https://edherbst.net/research/smc-var/</link>
      <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/smc-var/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hey Hey</title>
      <link>https://edherbst.net/etc/test/</link>
      <pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/etc/test/</guid>
      <description>Here&amp;rsquo;s a heading %matplotlib inline import matplotlib.pyplot as plt plt.plot([1,2,3,4]); $x_t = 1$</description>
    </item>
    
    <item>
      <title>Stock and Watson (2007) in pymc3 and Stan</title>
      <link>https://edherbst.net/etc/sw/</link>
      <pubDate>Sun, 10 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/etc/sw/</guid>
      <description>This file is available as Jupyter Notebook.
The STOCK_2007 model inflation as an unobserved components models:
\begin{eqnarray} \pi_t &amp;amp;=&amp;amp; \tau_t + \eta_t \\\ \tau_t &amp;amp;=&amp;amp; \tau_{t-1} + \epsilon_t \end{eqnarray}
where \(\eta_t\) and \(\epsilon_t\) are independently and identically distributed with mean 0 and variances \(\sigma_\eta^2\) and \(\sigma_\epsilon^2\).
A Replication First we start by importing some standard libraries.
%matplotlib inline import matplotlib.pyplot as plt import numpy as np import pandas as p Next we get the data used and plot it.</description>
    </item>
    
    <item>
      <title>THE EMPIRICAL IMPLICATIONS OF THE INTEREST-RATE LOWER BOUND</title>
      <link>https://edherbst.net/research/zlb/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/zlb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BAYESIAN ESTIMATION OF DSGE MODELS</title>
      <link>https://edherbst.net/research/bayes-book/</link>
      <pubDate>Fri, 25 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/bayes-book/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EFFECTIVE MONETARY POLICY STRATEGIES IN NEW KEYNESIAN MODELS</title>
      <link>https://edherbst.net/research/sticky-information/</link>
      <pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/sticky-information/</guid>
      <description>See also discussions by Mark Gertler and Lars Svensson.
Note the Macro Annual is not peer-reviewed.</description>
    </item>
    
    <item>
      <title>USING THE &#34;CHANDRASEKHAR RECURSIONS&#34; FOR LIKELIHOOD EVALUATION OF DSGE MODELS</title>
      <link>https://edherbst.net/research/chandrasekhar-recursions/</link>
      <pubDate>Mon, 19 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/chandrasekhar-recursions/</guid>
      <description>In likelihood-based estimation of linearized Dynamic Stochastic General Equilibrium (DSGE) models, the evaluation of the Kalman Filter dominates the running time of the entire algorithm. In this paper, we revisit a set of simple recursions known as the “Chandrasekhar Recursions” developed by Morf (Fast Algorithms for Multivariate Systems, Ph.D. thesis, Stanford University, 1974) and Morf et al. (IEEE Trans Autom Control 19:315–323, 1974) for evaluating the likelihood of a Linear Gaussian State Space System.</description>
    </item>
    
    <item>
      <title>SEQUENTIAL MONTE CARLO SAMPLING FOR DSGE MODELS</title>
      <link>https://edherbst.net/research/smc-dsge/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/smc-dsge/</guid>
      <description></description>
    </item>
    
    <item>
      <title>EVALUATING DSGE MODEL FORECASTS OF COMOVEMENTS</title>
      <link>https://edherbst.net/research/dsge-forecasting/</link>
      <pubDate>Thu, 19 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/dsge-forecasting/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://edherbst.net/research/software/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/research/software/</guid>
      <description>Title: Software Coauthor: Version: Date: 2016-01-06 url: software/ save_as: software/index.html
Python Packages Template: bayes_book
dsge &amp;ndash; a small python package for solving DSGE models. More information here
smc &amp;ndash; a small python package using Sequential Monte Carlo methods to estimate Bayesian models.
Installation. For linux/mac users, at the command line type:
pip install http://edherbst.net/bookmaterials/dsge-0.0.2.tar.gz
For windows users, use
pip install http://edherbst.net/bookmaterials/dsge-0.0.2.zip</description>
    </item>
    
    <item>
      <title>Particle Filters</title>
      <link>https://edherbst.net/teaching/lecture-particle-filters/particle-filters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://edherbst.net/teaching/lecture-particle-filters/particle-filters/</guid>
      <description>Introduction From Linear to Nonlinear (DSGE) Models While DSGE models are inherently nonlinear, the nonlinearities are often small and decision rules are approximately linear. {{{NEWLINE}}} One can add certain features that generate more pronounced nonlinearities:
stochastic volatility; markov switching coefficients; asymmetric adjustment costs; occasionally binding constraints. From Linear to Nonlinear (DSGE) Models Linear DSGE model leads to
\begin{eqnarray*} y_t &amp;amp;=&amp;amp; \Psi_0(\theta) + \Psi_1(\theta)t + \Psi_2(\theta) s_t + u_t, \quad u_t \sim N(0,\Sigma_u) ,\\ s_t &amp;amp;=&amp;amp; \Phi_1(\theta)s_{t-1} + \Phi_\epsilon(\theta) \epsilon_t, \quad \epsilon_t \sim N(0,\Sigma_\epsilon).</description>
    </item>
    
  </channel>
</rss>
