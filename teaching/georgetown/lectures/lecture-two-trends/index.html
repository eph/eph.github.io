<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ed Herbst/teaching/georgetown/lectures/lecture-two-trends/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="https://edherbst.net/hugo-theme-console/css/terminal-0.7.1.min.css">
    <link rel="stylesheet" href="https://edherbst.net/hugo-theme-console/css/animate-3.7.2.min.css">
    <link rel="stylesheet" href="https://edherbst.net/hugo-theme-console/css/console.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://edherbst.net/css/custom.css">
<link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
<script src="https://tikzjax.com/v1/tikzjax.js"></script>


    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <meta property="og:title" content="ECON 616: Lecture Two: Deterministic Trends, Nonstationary Processes" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://edherbst.net/teaching/georgetown/lectures/lecture-two-trends/" /><meta property="article:published_time" content="2025-03-13T17:50:57-04:00" />



<meta name="twitter:title" content="ECON 616: Lecture Two: Deterministic Trends, Nonstationary Processes"/>
<meta name="twitter:description" content="Intro
Background

Overview: Chapters 15-16 from Hamilton (1994).

Technical Details: Davidson and MacKinnon (2003)


Trends vs Cycles
A commond decomposition of macroeconomic time series is into trend and cycle.
If \(Y^T\) corresponds to real per capita GDP \(gdp_t\) of the
United States. According to this components approach to time
series, \(y_t\) is expressed as
\[
y_t = \ln gdp_t = trend_t &#43; fluctuations_t
\]
we will examine regression techniques that decompose \(y_t\) in a trend and a cyclical component."/>

</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              
              <a href='https://edherbst.net/teaching'>teaching</a>/<a href='https://edherbst.net/teaching/georgetown'>georgetown</a>/<a href='https://edherbst.net/teaching/georgetown/lectures'>lectures</a>/<a href='https://edherbst.net/teaching/georgetown/lectures/lecture-two-trends'>lecture-two-trends</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="https://edherbst.net/" typeof="ListItem">&lt;/&gt;</a></li>
                
                <li><a href="https://edherbst.net/research/" typeof="ListItem">research</a></li>
                
                <li><a href="https://edherbst.net/teaching/" typeof="ListItem">teaching</a></li>
                
                <li><a href="https://edherbst.net/etc/" typeof="ListItem">et cetera</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container animated zoomIn fast">
        
<h1>ECON 616: Lecture Two: Deterministic Trends, Nonstationary Processes</h1>
<h2 id="intro">Intro</h2>
<h3 id="background">Background</h3>
<ul>
<li><em>Overview:</em> Chapters 15-16 from <a href="#citeproc_bib_item_3">Hamilton (1994)</a>.
<br></li>
<li><em>Technical Details</em>: <a href="#citeproc_bib_item_1">Davidson and MacKinnon (2003)</a>
<br></li>
</ul>
<h3 id="trends-vs-cycles">Trends vs Cycles</h3>
<p>A commond decomposition of macroeconomic time series is into <strong>trend</strong> and <strong>cycle</strong>.</p>
<p>If \(Y^T\) corresponds to real per capita GDP \(gdp_t\) of the
United States. According to this components approach to time
series, \(y_t\) is expressed as
\[
y_t = \ln gdp_t = trend_t + fluctuations_t
\]
we will examine regression techniques that decompose \(y_t\) in a trend and a cyclical component.</p>
<h3 id="an-identification-problem">An identification problem</h3>
<p>what features of the time series do we regard as trend and what do
we regard as fluctuations around the trend?
<br>
Let&rsquo;s guess a linear deterministic time trend:
\[ y_t = \beta_1 + \beta_2 t + u_t \]</p>
<p>A decomposition of \(y_t\) into trend and fluctuations can be obtained
by estimating \(\beta_1\) and \(\beta_2\):</p>
<p>\begin{eqnarray}
y_t &amp;=&amp; \widehat{trend}_t + \widehat{fluctuations}_t \nonumber \\
&amp;=&amp; (\hat{\beta}_1 + \hat{\beta}_2 t) + ( y_t - \hat{\beta}_1 - \hat{\beta}_2t).
\end{eqnarray}</p>
<p>When \(y_t\) is logged, the coefficient \(\beta_2\) has the interpretation of an average growth rate.</p>
<h3 id="a-trend-cycle-decomposition-of-log-us-gnp">A Trend Cycle Decomposition of Log US GNP</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>[0;31m---------------------------------------------------------------------------[0m
</span></span><span style="display:flex;"><span>[0;31mNameError[0m                                 Traceback (most recent call last)
</span></span><span style="display:flex;"><span>[0;32m&lt;ipython-input-1-bf73901e5db8&gt;[0m in [0;36m&lt;module&gt;[0;34m[0m
</span></span><span style="display:flex;"><span>[1;32m     20[0m [0;31m# Assign the inferred column names[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
</span></span><span style="display:flex;"><span>[1;32m     21[0m [0mdata[0m[0;34m.[0m[0mcolumns[0m [0;34m=[0m [0mcolumn_names[0m[0;34m[0m[0;34m[0m[0m
</span></span><span style="display:flex;"><span>[0;32m---&gt; 22[0;31m [0mdata[0m[0;34m[[0m[0;34m&#39;Year&#39;[0m[0;34m][0m [0;34m=[0m [0mp[0m[0;34m.[0m[0mto_datetime[0m[0;34m([0m[0mdata[0m[0;34m[[0m[0;34m&#39;Year&#39;[0m[0;34m][0m[0;34m,[0m [0mformat[0m[0;34m=[0m[0;34m&#39;%Y&#39;[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
</span></span><span style="display:flex;"><span>[0m[1;32m     23[0m [0mdata[0m [0;34m=[0m [0mdata[0m[0;34m.[0m[0mreplace[0m[0;34m([0m[0;36m0[0m[0;34m,[0m [0mnp[0m[0;34m.[0m[0mnan[0m[0;34m)[0m[0;34m.[0m[0mset_index[0m[0;34m([0m[0;34m&#39;Year&#39;[0m[0;34m)[0m[0;34m.[0m[0mto_period[0m[0;34m([0m[0;34m&#39;A&#39;[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
</span></span><span style="display:flex;"><span>[1;32m     24[0m [0;32mfor[0m [0mc[0m [0;32min[0m [0mdata[0m[0;34m.[0m[0mcolumns[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[0;31mNameError[0m: name &#39;p&#39; is not defined
</span></span></code></pre></div><h3 id="deterministic-trend-model">Deterministic Trend Model</h3>
<p>Consider the <em>deterministic trend model</em>
\[
y_t = \beta_1 + \beta_2 t + u_t
\]
with \(\mathbb E[u_t]=0\) and \(var[u_t] = \sigma^2\).
There are several difficulties associated with the large sample analysis
of the OLS estimators \(\hat{\beta}_{1,T}\) and \(\hat{\beta}_{2,T}\).  Taking \(x_t = [1, t]&rsquo;\),
<br></p>
<ol>
<li>
<p>The matrix \(\frac{1}{T} \sum x_t x_t&rsquo;\) does not converge to a non-singular matrix \(Q\).</p>
</li>
<li>
<p>In a time series model, the disturbances \(u_t\) are in general
dependent.  This will change the limiting distribution of
quantities such as \(\sqrt{T} \frac{1}{T} \sum x_t u_t\).</p>
</li>
<li>
<p>If the \(u_t\)&rsquo;s are serially correlated, then the OLS estimator will in general be inefficient.</p>
</li>
</ol>
<h2 id="asymptotic-stuff">Asymptotic Stuff</h2>
<h3 id="rates-of-convergence-for-ols-estimator">Rates of Convergence for OLS Estimator</h3>
<p>Roughly speaking, convergence rates tell us how fast we can learn the
``true&rsquo;&rsquo; value of a parameter in a sampling experiment.
<br>
If &ldquo;standard&rdquo; OLS then the variance of the \(\hat\beta\) converges to zero at rate \(1/T\).
<br>
This isn&rsquo;t true for models with deterministic trends.
<br>
Let&rsquo;s look at the distributions of \(\sqrt{T}(\hat\beta_1 - \beta_1)\) and \(\sqrt{T}(\hat\beta_2 - \beta_2)\)</p>
<h3 id="a-monte-carlo">A Monte Carlo</h3>
<figure><img src="https://edherbst.net/ox-hugo/24f6b248fc8ecc254bd1131daa9dee322dd7b07b.png">
</figure>

<h3 id="some-math">Some math</h3>
<p>Facts:</p>
<p>\begin{eqnarray*}
\sum_{t=1}^T 1 = T,\quad \sum_{t=1}^T t  =  T(T+1)/2, \quad\sum_{t=1}^T t^2 = T(T+1)(2T+1)/6.
\end{eqnarray*}</p>
<p>(Assume \(u_t&rsquo;s\) are independently distributed.)</p>
<p>\[
\frac{1}{T} \sum x_t x_t&rsquo;  = \frac{1}{T}
\left( \begin{array}{cc} \sum 1 &amp; \sum t \\ \sum t &amp; \sum t^2 \end{array} \right)
\]
are not convergent!</p>
<p>On the other hand
\[
\frac{1}{T^{\textcolor{red}{3}}} \sum x_t x_t&rsquo; \longrightarrow
\left( \begin{array}{cc} 0 &amp; 0 \\ 0 &amp; 1/3 \end{array} \right)
\]
which is singular and not invertible!
<br>
<strong>Message</strong>: Trends change the rate of convergence of estimators!</p>
<h3 id="more-on-rates-of-convergence">More on Rates of Convergence</h3>
<p>It turns out that \(\hat{\beta}_{1,T}\) and \(\hat{\beta}_{2,T}\) have
different asymptotic rates of convergence. In particular, we will learn
faster about the slope of the trend line than the intercept.</p>
<p>To
analyze the asymptotic behavior of the estimators we define the matrix
\[
G_T = \left( \begin{array}{cc}  1 &amp; 0 \\ 0 &amp; T \end{array} \right).
\]
Note that the matrix is equivalent to its transpose, that is, \(G_T = G_T&rsquo;\).</p>
<h3 id="asymptotic-distributions">Asymptotic Distributions</h3>
<p>We will analyze the following quantity
\[
G_T(\hat{\beta}_T - \beta)
= \left( \frac{1}{T} \sum G_T^{-1} x_t x_t&rsquo;G_T^{-1} \right)^{-1} \left( \frac{1}{T}\sum G_T^{-1} x_t u_t \right).
\]
It can be easily verified that
\[
\frac{1}{T} \sum G_T^{-1} x_t x_t&rsquo; G_T^{-1}
= \frac{1}{T} \left( \begin{array}{cc} \sum 1 &amp; \sum t/T \\ \sum t/T &amp; \sum (t/T)^2 \end{array} \right)
\longrightarrow Q,
\]
where
\[
Q = \left( \begin{array}{cc}  1 &amp; 1/2 \\ 1/2 &amp; 1/3 \end{array} \right).
\]</p>
<h3 id="standardization">Standardization</h3>
<p>The term \(\frac{1}{T} \sum G_T^{-1} x_t u_t\) has the components
\(\frac{1}{T} \sum u_t\) and \(\frac{1}{T} \sum (t/T) u_t\) which converge
in probability to zero based on the weak law of large numbers for non-identically
distributed random variables.
<br>
<strong>Note</strong>: Without the proper standardization \(\frac{1}{T} \sum t u_t\)
will not converge to its expected value of zero. The variance of the
random variable \(T u_T\) is getting larger and larger with sample size which
prohibits the convergence of the sample mean to its expectation. \(\Box\)</p>
<h3 id="results">Results</h3>
<p><strong>Result</strong>: Suppose
\[
y_t = \beta_1 + \beta_2 t + u_t, \quad u_t \sim iid(0,\sigma^2).
\]
Let \(\hat{\beta}_{i,T}\), \(i=1,2\) be the OLS estimators of the intercept
and slope coefficient, respectively. Then</p>
<p>\begin{eqnarray}
\hat{\beta}_{1,T} -\beta_1 &amp; \stackrel{p}{\longrightarrow} &amp; 0 \label{eq_rint}\\
T(\hat{\beta}_{2,T} - \beta_2) &amp; \stackrel{p}{\longrightarrow} &amp; 0 \label{eq_rslp}. \quad \Box
\end{eqnarray}</p>
<h3 id="clt">CLT</h3>
<p>I&rsquo;m not going to show the details of proof for CLT, but</p>
<ul>
<li>
<p>We use a CLT for independently but not identically distributed random variables (Liapounov)</p>
</li>
<li>
<p>Also, Cramer and Wold device that can be used to deduce the convergence of a random vector</p>
</li>
</ul>
<p>based on the convergence of arbitrary linear combinations of its elements.</p>
<p><strong>Result</strong>
\[
y_t = \beta_1 + \beta_2 t + u_t, \quad u_t \sim iid(0,\sigma^2).
\]
Let \(\hat{\beta}_{i,T}\), \(i=1,2\) be the OLS estimators of the intercept
and slope coefficient, respectively. The sampling distribution of the OLS
estimators has the following large sample behavior
\[
\sqrt{T} G_T (\hat{\beta}_T - \beta) \Longrightarrow {\cal N}(0,\sigma^2Q^{-1})
\]</p>
<h3 id="note">Note</h3>
<p>This is equivalent to
\[
\left[ \begin{array}{c} \sqrt{T}(\hat{\beta}_{1,T} - \beta) \\ T^{3/2}(\hat{\beta}_{2,T} - \beta_2) \end{array} \right]
\Longrightarrow
{\cal N} \left( \left[ \begin{array}{c} 0 \\ 0 \end{array} \right],
\sigma^2 \left[ \begin{array}{cc} 4 &amp; -6 \\ -6 &amp; 12 \end{array} \right] \right). \quad \Box
\]</p>
<h3 id="having-said-all-this">Having Said All this</h3>
<p>When we consider the case where the variance is unknown:</p>
<p>\[
\hat \sigma^2 = \frac{1}{T-2}\sum(y_t - \hat\beta_1 - \hat\beta_2 t)^2
\]</p>
<p>Despite the fact that \(\beta_1\) and \(\beta_2\) have different asymptic
rates of convergence, the <strong>t statistics</strong> still have \(N(0,1)\) limited
distribution because the standard error estimates have offsetting
behaviour.</p>
<h3 id="ols-and-serial-dependence">OLS and Serial Dependence</h3>
<p>\[
y_t = \beta t + u_t
\]
\(u_t\) are serially correlated, that is, \(\mathbb E[u_{t}u_{t-h}] \not= 0\)
for some \(h \implies\) OLS not efficient.</p>
<p>Let&rsquo;s look at example with \(MA(1)\) errors.
\[
u_t = \epsilon_t + \theta \epsilon_{t-1}, \quad \epsilon_{t} \sim iid(0,\sigma^2_\epsilon).
\]
can verify</p>
<p>\begin{eqnarray}
\mathbb E[u_t^2] &amp; = &amp; \mathbb E[(\epsilon_t + \theta \epsilon_{t-1})^2] = (1+\theta^2)\sigma^2_\epsilon \\
\mathbb E[u_tu_{t-1}] &amp; = &amp; \mathbb E[ (\epsilon_t + \theta \epsilon_{t-1})(\epsilon_{t-1} + \theta \epsilon_{t-2})] = \theta \sigma^2_\epsilon \\
\mathbb E[u_tu_{t-h}] &amp; = &amp; 0 \quad h &gt; 1.
\end{eqnarray}</p>
<h3 id="the-ols-estimator">The OLS estimator</h3>
<p>\[
\hat{\beta}_T - \beta = \frac{\sum tu_t}{\sum t^2}.
\]
To find the limiting distribution, note that
\[
\frac{1}{T^3} \sum_{t=1}^T t^2 = \frac{T(T+1)(2T+1)}{6T} \longrightarrow \frac{1}{3}.
\]</p>
<h3 id="numerator">Numerator</h3>
<p>The numerator can be manipulated as follows</p>
<p>\begin{eqnarray}
\sum t u_t
&amp; = &amp; \sum t(\epsilon_t + \theta \epsilon_{t-1}) \nonumber \\
&amp; = &amp; \begin{array}{ccccc}
0 &amp; +\epsilon_1 &amp; +2\epsilon_2 &amp; + 3\epsilon_3 &amp; + \ldots  \\
+\theta \epsilon_0 &amp; + 2 \theta \epsilon_1 &amp; + 3 \theta \epsilon_2 &amp; + 4 \theta \epsilon_3 &amp; + \ldots
\end{array} \nonumber \\
&amp; = &amp; \sum_{t=1}^{T-1} (t + \theta(t+1)) \epsilon_t \; + \theta \epsilon_0 + T \epsilon_T \nonumber \\
&amp; = &amp; \sum_{t=1}^{T-1} (1+\theta) t \epsilon_t +  \sum_{t=1}^{T-1} \theta \epsilon_t + \theta \epsilon_0 + T \epsilon_T \nonumber \\
&amp; = &amp; \sum_{t=1}^T (1+\theta) t \epsilon_t  \underbrace{ - \theta T \epsilon_T + \theta \sum_{t=1}^T \epsilon_{t-1} }_{\mbox{asymp. negligible}}.
\end{eqnarray}</p>
<h3 id="ols-continued">OLS, Continued</h3>
<p>After standardization by \(T^{-3/2}\) we obtain</p>
<p>\begin{eqnarray*}
T^{-3/2} \sum tu_t = \frac{1}{\sqrt{T}} (1+\theta) \sum_{t=1}^T (t/T) \epsilon_t
- \frac{1}{\sqrt{T}} \theta \epsilon_T + \frac{\theta}{T} \frac{1}{\sqrt{T}} \sum_{t=1}^T \epsilon_{t-1}.
\end{eqnarray*}</p>
<ol>
<li>First term obeys CLT</li>
<li>Second Term goes to zero</li>
<li>Third Term goes to zero</li>
</ol>
<p>Thus ,
\[
T^{3/2}( \hat{\beta}_T - \beta) \Longrightarrow \big(0,3\sigma^2_\epsilon (1+\theta)^2 \big).
\]</p>
<h3 id="remark">Remark</h3>
<p>Consider the following model with \(iid\) disturbances
\[
y_t = \beta t + u_t, \quad u_t \sim iid(0,\sigma_\epsilon^2(1+\theta^2)).
\]
The unconditional variance of the disturbances is the same as in the model
with moving average disturbances. It can be verified that</p>
<p>\[
T^{3/2}( \hat{\beta}_T - \beta) \Longrightarrow \big(0,3\sigma^2_\epsilon (1+\theta^2) \big).
\]</p>
<p>If \(\theta\) is positive then the limit variance of the OLS estimator in the
model with \(iid\) disturbances is smaller than in the trend model with moving
average disturbances.
<br>
Positive serial correlated data are less informative than \(iid\) data.</p>
<h3 id="sampling-distributions">Sampling Distributions</h3>
<figure><img src="https://edherbst.net/ox-hugo/c6e16b171543925912f2c5bdd35661cca2c28ad4.png">
</figure>

<h2 id="stochastic-trends">Stochastic Trends</h2>
<h3 id="stochastic-trends">Stochastic Trends</h3>
<p>We looked at stationary model and deterministic trend models so far.
<br>
Now we will examine univariate models with a stochastic trend of the form</p>
<p>\[
y_t = \phi_0 + y_{t-1} + \epsilon_t \quad \epsilon_t \sim iid(0,\sigma^2)
\]</p>
<p>This particular model is called a <strong>random walk with drift</strong>.
<br>
The variable \(y_t\) is said to be <strong>integrated of order one</strong>.</p>
<h3 id="cointegration">Cointegration</h3>
<p>Moreover, we will consider bivariate models
with a common stochastic trend</p>
<p>\begin{eqnarray}
y_{1,t} &amp; = &amp; \gamma y_{2,t} + u_{1,t} \\
y_{2,t} &amp; = &amp; y_{2,t-1} + u_{2,t}
\end{eqnarray}</p>
<p>where \([u_{1,t}, u_{2,t}]&rsquo; \sim iid(0,\Omega)\).
Both \(y_{1,t}\) and \(y_{2,t}\) have a stochastic trend. However,
there exists a linear combination of \(y_{1,t}\) and \(y_{2,t}\), namely,
\[
y_{1,t} - \gamma y_{2,t} = u_t
\]
that is stationary. Therefore, \(y_{1,t}\) and \(y_{2,t}\) are called
<span class="underline">cointegrated</span>.</p>
<h3 id="background">Background</h3>
<p>In the  late 80s and early 90s, this was a super hot research area.</p>
<ul>
<li><a href="#citeproc_bib_item_2">Dickey and Fuller (1979)</a> examined the sampling distribution of
estimators for autoregressive time series with a unit root and
provided tables with critical values for unit root tests.</li>
</ul>
<!--listend-->
<ul>
<li>In <a href="#citeproc_bib_item_5">Phillips (1986)</a> and (1987) published two papers on spurious
regression and time series regressions with a unit root that employ
the mathematical theory of convergence of probability measures for
metric spaces. This marks a ``technological breakthrough&rsquo;&rsquo; and the
field started to grow at an exponential rate thereafter.</li>
</ul>
<h3 id="three-choices">Three Choices</h3>
<p>Consider the first order autoregressive model with mean zero:
\[
y_t = \phi y_{t-1} + \epsilon_t, \quad \epsilon_t \sim iid{\cal N}(0,\sigma^2)
\]
Three cases</p>
<ul>
<li>
<p>\(|\phi|&lt;1\): stationarity! we talked about this last week</p>
</li>
<li>
<p>\(|\phi|&gt;1\): explosive! We will not analyze explosive processes in
this course.</p>
</li>
<li>
<p>\(|\phi|=1\).  This is the unit root and will be the focus of this</p>
</li>
</ul>
<p>part of the lecture.  If \(\phi=1\) then the AR(1) model simplifies to</p>
<p>\[ y_t = y_{t-1} + \epsilon_t \]</p>
<p>With \(\Delta=1-L\), we have \(\Delta y_t = \epsilon_t\) form a stationary
process, the random walk is called integrated of order one,
denoted by \(I(1)\).</p>
<h3 id="difference-b-w-stationary-ar-and-unit-root">Difference b/w Stationary AR and Unit Root</h3>
<p>Suppose that the AR process is initialized by
\(y_0 \sim {\cal N}(0,1)\). Then \(y_t\) can be expressed as</p>
<p>\[ y_t = \phi^{t} y_0 + \sum_{\tau=1}^t \phi^{\tau -1}
\epsilon_{t+1-\tau} \]</p>
<ul>
<li>The unconditional mean of \(y_t\) is given by</li>
</ul>
<p>\[
\mathbb E[y_t] = \phi^{t-1} \mathbb E[y_0] + \sum_{\tau=1}^t \phi^{\tau -1} \mathbb E[\epsilon_\tau] = 0
\]</p>
<h3 id="differences-continued">Differences, continued</h3>
<p>The unconditional variance is \(y_t\) is given by</p>
<p>\begin{eqnarray}
var[y_t] &amp; = &amp; \phi^{2(t-1)} var[y_0] + \sum_{\tau=1}^t \phi^{2(\tau-1)} var[\epsilon_\tau]  \\
&amp; = &amp; \phi^{2(t-1)} var[y_0] + \sigma^2 \sum_{\tau=1}^t \phi^{2(\tau-1)}  \nonumber \\
&amp; = &amp; \left\{
\begin{array}{lclcl}
\phi^{2(t-1)} var[y_0] + \sigma^2 \frac{1 - \phi^{2t}}{1 - \phi^2}
&amp; \longrightarrow &amp; \frac{\sigma^2}{1-\phi^2} &amp; \mbox{if} &amp; |\phi| &lt; 1 \\
var[y_0] + \sigma^2 t &amp; \longrightarrow &amp; \infty &amp; \mbox{if} &amp; |\phi| =1
\end{array}
\right. \nonumber
\end{eqnarray}</p>
<p>as \(t \rightarrow \infty\).</p>
<h3 id="differences-continued">Differences, continued</h3>
<p>The conditional expectation of \(y_{t}\) given \(y_0\) is</p>
<p>\begin{eqnarray*}
\mathbb E[y_{t}|y_0] =  \phi^{\tau-1} y_0
\longrightarrow  \left\{
\begin{array}{lcl}
0 &amp; \mbox{if} &amp; |\phi| &lt; 1 \\
y_0 &amp; \mbox{if} &amp; \phi = 1
\end{array}
\right\}
\end{eqnarray*}</p>
<p>as \(t \rightarrow \infty\).</p>
<p>In the unit root case, the best prediction of future \(y_{t}\) is the
initial \(y_0\) at all horizons, that is, ``no change&rsquo;'.</p>
<p>In the stationary case, the conditional expectation converges to the
unconditional mean. For this reason, stationary processes are also
called ``mean reverting&rsquo;'.</p>
<h3 id="result">Result</h3>
<p>Stationary and unit root processes differ in their behavior over long time horizons.
<br>
Suppose that \(\sigma^2=1\), and \(y_0=1\). Then the conditional mean and
variance of a process \(y_t\) with \(\phi = 0.995\) is given by</p>
<table>
  <thead>
      <tr>
          <th>Horizon \(t\)</th>
          <th>1</th>
          <th>2</th>
          <th>5</th>
          <th>10</th>
          <th>20</th>
          <th>50</th>
          <th>100</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>\(\mathbb E[y_t\mid y_0]\)</td>
          <td>0.995</td>
          <td>0.990</td>
          <td>0.975</td>
          <td>0.951</td>
          <td>0.905</td>
          <td>0.778</td>
          <td>0.606</td>
      </tr>
      <tr>
          <td>\(\mathbb V[y_t \mid y_0]\)</td>
          <td>1.000</td>
          <td>1.990</td>
          <td>4.901</td>
          <td>9.563</td>
          <td>18.21</td>
          <td>39.52</td>
          <td>63.46</td>
      </tr>
  </tbody>
</table>
<p>If interestered in long run predictions, very important to distinguish these two cases.
<br>
<strong>But note</strong>: long run predictions face serious extrapolation problem.</p>
<h3 id="frequentist-approach">Frequentist Approach</h3>
<p>To get a unit root test of the null hypothesis
\(H_0: \phi = 1\), we have to find the sampling distribution of
a suitable test statistic such as the \(t\) ratio
\[
\frac{\hat{\phi}_T -1}{\sqrt{ \sigma^2 / \sum y_{t-1}^2 }}
\]
Under the generating mechanism
\[
y_t = \phi_0  + y_{t-1} + \epsilon_t, \quad iid(0,\sigma^2)
\]
<span class="underline">For stationary processes used a variety of WLLN and CLTs</span>, unfortunately, these don&rsquo;t apply.</p>
<h3 id="heuristic-overview-of-asymptotics">Heuristic Overview of Asymptotics</h3>
<p>Assume that \(\phi_0 = 0\), \(\sigma = 1\), and \(y_0 = 0\).
Thus, the process \(y_t\) can be represented as
\[
y_T = \sum_{t=1}^T \epsilon_t
\]
Summations will range from \(t=1\) to \(T\) unless stated otherwise.
The central limit theorem for \(iid\) random variables implies
\[
\frac{y_T}{\sqrt{T} } = \frac{1}{\sqrt{T}} \sum \epsilon_t
\Longrightarrow {\cal N}(0,1)
\]
This suggests that
\[
\frac{1}{T} \sum y_t = \frac{1}{\sqrt{T}} \sum \left[ \sqrt{ \frac{t}{T} } \frac{1}{\sqrt{t}} \sum_{\tau =1}^t \epsilon_\tau \right]
\]
will not converge to a constant in probability but instead to a random variable.</p>
<p>Need a more elegant approach!</p>
<h3 id="a-twist-on-our-framework">A Twist on our framework</h3>
<p>We used \(T = \{0, \pm 1, \pm 2, \ldots\}\).</p>
<p>Consider \(S = [0,1]\).  Consider random elements \(W(s)\) that correspond to functions this interval.</p>
<p>We will place some probability \(Q\) on these functions and show
that \(Q\) can be helpful in the approximation of the distribution of
\(\sum y_t\)</p>
<p>Defining probability distributions on function spaces
is a pain.</p>
<h3 id="wiener-measure">Wiener Measure</h3>
<p>Let \({\cal C}\) be the space of continuous functions on the interval
\([0,1]\).</p>
<p>We will define a probability distribution for the function space \({\cal C}\).</p>
<p>This probability distribution is called ``Wiener measure&rsquo;'.</p>
<p>Whenever we draw an element from the probability space we
obtain a function \(W(s)\), \(s \in [0,1]\). Let \(Q[ \cdot ]\) denote
the expectation operator under the Wiener measure.</p>
<h3 id="properties-of-w--s">Properties of \(W(s)\)</h3>
<ul>
<li>If we repeatedly draw functions under the Wiener measure and
evaluate these functions at a particular value \(s = s&rsquo;\), then \[ Q[
\{ W(s&rsquo;) \le w \} ] = \frac{1}{\sqrt{2\pi s&rsquo;}} \int_{-\infty}^{w}
e^{- u^2/2s&rsquo;} du\] that is, \[ W(s&rsquo;) \sim {\cal N}(0,s&rsquo;) \] If
\(s&rsquo;=0\) then the equations is interpreted to mean \(Q[\{W(0)=0 \}] =
1\). Thus \(W(0) = 0\) with probability one.</li>
</ul>
<h3 id="properties-of-w--s">Properties of \(W(s)\)</h3>
<ul>
<li>The random function \(W(s)\) has independent increments. If \[ 0 \le
s_1 \le s_2 \le \ldots \le s_k \le 1 \] Then the random variables \[
W(s_2) - W(s_1), \; W(s_3) - W(s_2)\; \ldots , \; W(s_k) -
W(s_{k-1}) \] are independent.</li>
<li>The random function \(W(s)\) is continuous on \(s \in [0,1]\).
Otherwise, contradiction.</li>
</ul>
<h3 id="more-of-w--s">More of \(W(s)\)</h3>
<p>It can be shown that there indeed exists a probability distribution
on \({\cal C}\) with these properties.</p>
<p>Rougly speaking, the Wiener
measure is to the theory of stochastic processes, what
the normal distribution is to the theory related to real valued random variables.</p>
<p>Note: \(W(1) \sim {\cal N}(0,1)\).</p>
<h3 id="relating-this-back-to-our-discrete-processes">Relating this back to our discrete processes</h3>
<p>Define the partial sum process
\[
Y_T(s) = \frac{1}{\sqrt{T}} \sum \{ t \le \lfloor Ts \rfloor \} \epsilon_t
\]
where \(\lfloor x \rfloor\) denotes the integer part of \(x\). Since we assumed
that \(\epsilon_t \sim iid(0,1)\), the partial sum process is a random step
function.</p>
<p>Interpolation:</p>
<p>\begin{eqnarray*}
\bar{Y}_T(s) = \frac{1}{\sqrt{T}} \sum \{ t \le \lfloor Ts \rfloor \} \epsilon_t
+ (Ts - \lfloor Ts \rfloor ) \epsilon_{\lfloor Ts \rfloor + 1} / \sqrt{T}
\end{eqnarray*}</p>
<h3 id="two-ways-to-randomly-generate-continuous-functions">Two ways to randomly generate continuous functions</h3>
<ul>
<li>
<p>Draw a function \(W(s)\) from the Wiener distribution. We did not examine how to do the sampling in practice, but since the Wiener distribution is well-defined, it is theoretically possible.</p>
</li>
<li>
<p>Generate a sequence \(\epsilon_1, \ldots, \epsilon_T\), where \(\epsilon_t \sim iid(0,1)\) and compute \(\bar{Y}_T(s)\).</p>
</li>
</ul>
<p>As \(T\longrightarrow\infty\), these are basically the same.</p>
<p><strong>Functional CLT</strong>: Let \(\epsilon_t \sim iid(0,\sigma^2)\). Then</p>
<p>\begin{eqnarray*}
Y_T(s) = \frac{1}{\sigma \sqrt{T} } \sum_{t=1}^T
\{ t \le \lfloor Ts \rfloor \} \epsilon_t
\Longrightarrow W(s) \quad \Box
\end{eqnarray*}</p>
<h3 id="simulation-of-wiener-process">Simulation of Wiener Process</h3>
<figure><img src="https://edherbst.net/ox-hugo/e8295697c39d73d543d54470d7d88d54e5d307e6.png">
</figure>

<h3 id="the-upshot">The upshot</h3>
<p>The sum
\[
\frac{1}{T} \sum y_{t-1} \epsilon_t
\]
convergences to a <span class="underline">stochastic integral</span>; i.e.,</p>
<p>Suppose that \(y_t = y_{t-1} + \epsilon_t\), where \(\epsilon_t \sim iid(0,\sigma^2)\)
and \(y_0=0\). Then
\[
\frac{1}{\sigma^2 T} \sum y_{t-1} \epsilon_t \Longrightarrow  \int W(s) dW(s)
\]
where \(W(s)\) denotes a standard Wiener process.</p>
<p>we can use this to develop tests!</p>
<h3 id="theorem">Theorem</h3>
<p>Suppose that \(y_t =  \phi y_{t-1} + \epsilon_t\), where \(\epsilon_t \sim iid(0,\sigma^2)\),
\(\phi=1\), and \(y_0=0\). The sampling distribution of the OLS estimator \(\hat{\phi}_T\)
of the autoregressive parameter \(\phi=1\) and the sampling distribution of
the corresponding \(t\)-statistic have the following asymptotic approximations</p>
<p>\begin{eqnarray}
z(\hat{\phi}_T) &amp; \Longrightarrow &amp;  \frac{\frac{1}{2} ( W(1)^2 -1 )  }{ \int_0^1 W(s)^2 ds } \\
t(\hat{\phi}_T) &amp; \Longrightarrow &amp;  \frac{\frac{1}{2} ( W(1)^2 -1 )  }{ \left[ \int_0^1 W(s)^2 ds \right]^{1/2} }
\end{eqnarray}</p>
<p>where \(W(s)\) denotes a standard Wiener process. \(\Box\)</p>
<h3 id="how-these-sampling-distributions-look">How These Sampling Distributions look</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">numpy</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">np</span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">import</span> <span style="color:#00f;font-weight:bold">matplotlib.pyplot</span> <span style="color:#a2f;font-weight:bold">as</span> <span style="color:#00f;font-weight:bold">plt</span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">statsmodels.tsa.ar_model</span> <span style="color:#a2f;font-weight:bold">import</span> AutoReg
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">scipy.stats</span> <span style="color:#a2f;font-weight:bold">import</span> norm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Parameters</span>
</span></span><span style="display:flex;"><span>T <span style="color:#666">=</span> <span style="color:#666">5000</span>  <span style="color:#080;font-style:italic"># Length of time series</span>
</span></span><span style="display:flex;"><span>N_sim <span style="color:#666">=</span> <span style="color:#666">2000</span>  <span style="color:#080;font-style:italic"># Number of simulations</span>
</span></span><span style="display:flex;"><span>phi_values <span style="color:#666">=</span> [<span style="color:#666">1</span>, <span style="color:#666">0.95</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Function to simulate AR(1) process and estimate</span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">simulate_and_estimate</span>(phi, T, N_sim):
</span></span><span style="display:flex;"><span>    estimates <span style="color:#666">=</span> np<span style="color:#666">.</span>zeros(N_sim)
</span></span><span style="display:flex;"><span>    <span style="color:#a2f;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(N_sim):
</span></span><span style="display:flex;"><span>        y <span style="color:#666">=</span> np<span style="color:#666">.</span>zeros(T)
</span></span><span style="display:flex;"><span>        <span style="color:#a2f;font-weight:bold">for</span> t <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(<span style="color:#666">1</span>, T):
</span></span><span style="display:flex;"><span>            y[t] <span style="color:#666">=</span> phi <span style="color:#666">*</span> y[t<span style="color:#666">-</span><span style="color:#666">1</span>] <span style="color:#666">+</span> np<span style="color:#666">.</span>random<span style="color:#666">.</span>normal()
</span></span><span style="display:flex;"><span>        model <span style="color:#666">=</span> AutoReg(y, lags<span style="color:#666">=</span><span style="color:#666">1</span>, trend<span style="color:#666">=</span><span style="color:#b44">&#39;n&#39;</span>)<span style="color:#666">.</span>fit()
</span></span><span style="display:flex;"><span>        estimates[i] <span style="color:#666">=</span> (model<span style="color:#666">.</span>params[<span style="color:#666">-</span><span style="color:#666">1</span>] <span style="color:#666">-</span> phi) <span style="color:#666">/</span> model<span style="color:#666">.</span>bse[<span style="color:#666">-</span><span style="color:#666">1</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#a2f;font-weight:bold">return</span> estimates
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Function to simulate the Wiener process distribution</span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">simulate_wiener</span>(phi, T, N_sim):
</span></span><span style="display:flex;"><span>    np<span style="color:#666">.</span>random<span style="color:#666">.</span>seed(<span style="color:#666">42</span>)
</span></span><span style="display:flex;"><span>    estimates <span style="color:#666">=</span> np<span style="color:#666">.</span>zeros(N_sim)
</span></span><span style="display:flex;"><span>    <span style="color:#a2f;font-weight:bold">for</span> i <span style="color:#a2f;font-weight:bold">in</span> <span style="color:#a2f">range</span>(N_sim):
</span></span><span style="display:flex;"><span>        W <span style="color:#666">=</span> np<span style="color:#666">.</span>random<span style="color:#666">.</span>normal(size<span style="color:#666">=</span>T)<span style="color:#666">.</span>cumsum() <span style="color:#666">/</span> np<span style="color:#666">.</span>sqrt(T)
</span></span><span style="display:flex;"><span>        W1 <span style="color:#666">=</span> W[<span style="color:#666">-</span><span style="color:#666">1</span>]
</span></span><span style="display:flex;"><span>        integral_W2 <span style="color:#666">=</span> np<span style="color:#666">.</span>sum(W<span style="color:#666">**</span><span style="color:#666">2</span>) <span style="color:#666">/</span> T
</span></span><span style="display:flex;"><span>        estimates[i] <span style="color:#666">=</span> <span style="color:#666">0.5</span> <span style="color:#666">*</span> (W1<span style="color:#666">**</span><span style="color:#666">2</span> <span style="color:#666">-</span> <span style="color:#666">1</span>) <span style="color:#666">/</span> integral_W2<span style="color:#666">**</span><span style="color:#666">0.5</span>
</span></span><span style="display:flex;"><span>    <span style="color:#a2f;font-weight:bold">return</span> estimates
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Run simulations</span>
</span></span><span style="display:flex;"><span>empirical_estimates <span style="color:#666">=</span> {<span style="color:#a2f">str</span>(phi): simulate_and_estimate(phi, T, N_sim) <span style="color:#a2f;font-weight:bold">for</span> phi <span style="color:#a2f;font-weight:bold">in</span> phi_values}
</span></span><span style="display:flex;"><span>phi_1_distribution <span style="color:#666">=</span> simulate_wiener(<span style="color:#666">1</span>, T, N_sim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Plot results</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>figure(figsize<span style="color:#666">=</span>(<span style="color:#666">14</span>, <span style="color:#666">6</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Plot for phi = 1</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>subplot(<span style="color:#666">1</span>, <span style="color:#666">2</span>, <span style="color:#666">1</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>hist(empirical_estimates[<span style="color:#b44">&#39;1&#39;</span>], bins<span style="color:#666">=</span><span style="color:#666">30</span>, density<span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">True</span>, alpha<span style="color:#666">=</span><span style="color:#666">0.7</span>, label<span style="color:#666">=</span><span style="color:#b44">&#39;Empirical for $\phi=1$&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># kde for the theoretical distribution from draws phi_1_distribution</span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">scipy.stats</span> <span style="color:#a2f;font-weight:bold">import</span> gaussian_kde
</span></span><span style="display:flex;"><span>kde <span style="color:#666">=</span> gaussian_kde(phi_1_distribution)
</span></span><span style="display:flex;"><span>x <span style="color:#666">=</span> np<span style="color:#666">.</span>linspace(<span style="color:#666">-</span><span style="color:#666">3</span>, <span style="color:#666">3</span>, <span style="color:#666">1000</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>plot(x, kde(x), label<span style="color:#666">=</span><span style="color:#b44">&#39;Asymptotic for $\phi=1$&#39;</span>, linestyle<span style="color:#666">=</span><span style="color:#b44">&#39;--&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>title(<span style="color:#b44">r</span><span style="color:#b44">&#39;Sampling Distribution for $\phi=1$&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>xlabel(<span style="color:#b44">r</span><span style="color:#b44">&#39;$\hat{\phi}$&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>grid()
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>legend()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#080;font-style:italic"># Plot for phi = 0.95</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>subplot(<span style="color:#666">1</span>, <span style="color:#666">2</span>, <span style="color:#666">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>hist(empirical_estimates[<span style="color:#b44">&#39;0.95&#39;</span>], bins<span style="color:#666">=</span><span style="color:#666">30</span>, density<span style="color:#666">=</span><span style="color:#a2f;font-weight:bold">True</span>, alpha<span style="color:#666">=</span><span style="color:#666">0.7</span>, label<span style="color:#666">=</span><span style="color:#b44">&#39;Empirical for $\phi=0.95$&#39;</span>)
</span></span><span style="display:flex;"><span>x <span style="color:#666">=</span> np<span style="color:#666">.</span>linspace(<span style="color:#666">-</span><span style="color:#666">3</span>, <span style="color:#666">3</span>, <span style="color:#666">1000</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>plot(x, norm<span style="color:#666">.</span>pdf(x, loc<span style="color:#666">=</span><span style="color:#666">0</span>, scale<span style="color:#666">=</span><span style="color:#666">1</span>), label<span style="color:#666">=</span><span style="color:#b44">&#39;Theoretical N(0.95, var)&#39;</span>, linestyle<span style="color:#666">=</span><span style="color:#b44">&#39;--&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>title(<span style="color:#b44">r</span><span style="color:#b44">&#39;Sampling Distribution for $\phi=0.95$&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>xlabel(<span style="color:#b44">r</span><span style="color:#b44">&#39;$\hat{\phi}$&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>grid()
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>legend()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#666">.</span>show()
</span></span></code></pre></div><h3 id="back-to">Back to <a href="#citeproc_bib_item_4">Nelson and Plosser (1982)</a></h3>
<p>log Real GNP: estimated autocorrelation functions of the level and deviations from the time trend</p>
<p>Looks good, but can give misleading results when the underlying series is integrated.
\[
y_t = \mu + \phi_1 y_{t-1} + \gamma t + \sum_{k=1}^K \phi_{\Delta k}\Delta y_{t-k} + u_t.
\]
want to test \(\phi_1 = 1\).</p>
<h3 id="results">Results</h3>
<ul>
<li>t-stat is the conventional t-stat!</li>
<li>Dickey-Fulley 5 percent critial value is about \(-3.5\).</li>
</ul>
<p>Thus, one cannot reject the null that real GNP is well described by a unit root process!</p>
<h2 id="cointegration">Cointegration</h2>
<h3 id="back-to-our-model">Back to Our Model</h3>
<p>We will now analyze a simple bivariate system of cointegrated processes.
Consider the model</p>
<p>\begin{eqnarray}
y_{1,t} &amp; = &amp; \gamma y_{2,t} + u_{1,t} \\
y_{2,t} &amp; = &amp; y_{2,t-1} + u_{2,t}
\end{eqnarray}</p>
<p>where \([u_{1,t},u_{2,t}]&rsquo; \sim iid(0,\Omega)\).</p>
<p>Clearly, \(y_{2,t}\) is
a random walk. Moreover, it can be easily verified that \(y_{1,t}\) follows
a unit root process.</p>
<p>\begin{eqnarray}
y_{1,t} - y_{1,t-1} = \gamma (y_{2,t} - y_{2,t-1}) + u_{1,t} - u_{1,t-1}
\end{eqnarray}</p>
<p>Therefore,</p>
<p>\begin{eqnarray}
y_{1,t} = y_{1,t-1} + \gamma u_{2,t} + u_{1,t} - u_{1,t-1}
\end{eqnarray}</p>
<p>Thus, both \(y_{1,t}\) and \(y_{2,t}\) are integrated processes.</p>
<h3 id="model-continued">Model Continued</h3>
<p>However, the linear combination</p>
<p>\begin{eqnarray}
[1,\; -\gamma] \left[ \begin{array}{c} y_{1,t} \\ y_{2,t} \end{array} \right]
= y_{1,t} - \gamma y_{2,t} = u_{1,t}
\end{eqnarray}</p>
<p>is stationary. Therefore, \(y_{1,t}\) and \(y_{2,t}\) are cointegrated.
<br>
The vector \([1,-\gamma]&rsquo;\) is called the cointegrating vector.
<br>
Note that the cointegrating vector is only unique up to normalization.</p>
<h3 id="rewriting-the-model">Rewriting the Model</h3>
<p>The model can be rewritten as a VAR(1)</p>
<p>\begin{eqnarray}
y_t = \Phi_1 y_{t-1} + \epsilon_t
\end{eqnarray}</p>
<p>The elements of the matrix \(\Phi_1\) and the definition of \(\epsilon_t\)
is given by</p>
<p>\begin{eqnarray}
\left[ \begin{array}{c} y_{1,t} \\ y_{2,t} \end{array} \right]
=
\left[ \begin{array}{cc} 0 &amp; \gamma \\ 0 &amp; 1 \end{array} \right]
\left[ \begin{array}{c} y_{1,t-1} \\ y_{2,t-1} \end{array} \right]
+
\left[ \begin{array}{c} u_{1,t} + \gamma u_{2,t} \\ u_{2,t} \end{array} \right]
\end{eqnarray}</p>
<p>The matrix \(\Phi_1\) is of reduced rank in this example of
cointegration.  More generally cointegrated system can be casted in
the form of a vector autoregression in levels of \(y_t\).
<br>
Although both \(y_{1,t}\) and \(y_{2,t}\) follow univariate random walks,
the cointegrated system cannot be expressed as a vector autoregression
in differences \([ \Delta y_{1,t}, \Delta y_{2,t} ]&rsquo;\). Consider</p>
<p>\begin{eqnarray}
\left[ \begin{array}{c} \Delta y_{1,t} \\ \Delta y_{2,t} \end{array} \right]
=
\left[ \begin{array}{cc} 1-L &amp; \gamma L \\ 0 &amp; 1 \end{array} \right]
\left[ \begin{array}{c} u_{1,t} \\ u_{2,t} \end{array} \right]
=
\Theta(L) u_t
\end{eqnarray}</p>
<p>Since \(|\Theta(1)|=0\) the moving average polynomial is not invertible and
no finite order VAR could describe \(\Delta y_t\).</p>
<h3 id="vecm">VECM</h3>
<p>The cointegrated model can be written in the so-called vector error correction model (VECM)
form:</p>
<p>\begin{eqnarray}
\left[ \begin{array}{c} \Delta y_{1,t} \\ \Delta y_{2,t} \end{array} \right]
=
\left[ \begin{array}{c} -1 \\ 0 \end{array} \right]
\left( \left[ \begin{array}{cc} 1 &amp; - \gamma \end{array} \right]
\left[ \begin{array}{c} y_{1,t-1} \\ y_{2,t-1} \end{array} \right]
\right)
+
\left[ \begin{array}{c} u_{1,t} + \gamma u_{2,t} \\ u_{2,t} \end{array} \right]
\end{eqnarray}</p>
<p>The term</p>
<p>\begin{eqnarray}
\left( \left[ \begin{array}{cc} 1 &amp; - \gamma \end{array} \right]
\left[ \begin{array}{c} y_{1,t-1} \\ y_{2,t-1} \end{array} \right]
\right)
= y_{1,t-1} - \gamma y_{2,t-1}
\end{eqnarray}</p>
<p>is called error correction term. In economic models it often reflects a
long-run equilibrium relationship such as a constant ratio of consumption and
output. If the economy is out of equilibrium in period
\(t-1\), that is, \(y_{1,t-1} - \gamma y_{2,t-1} \not= 0\), then the economy
adjusts toward its long-run equilibrium and \(\EE_{t-1}[\Delta y_{t}] \not= 0\).
If the ``true&rsquo;&rsquo; cointegrating vector is known, then both the left-hand-side
variables and the error correction term are stationary.</p>
<h2 id="bibliography">Bibliography</h2>
<h3 id="references">References</h3>
<h2 id="references-1">References</h2>
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a><span style="font-variant:small-caps;">Davidson, R., and J. G. MacKinnon</span>. (2003): <i>Econometric Theory and Methods</i>, Oxford University Press.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a><span style="font-variant:small-caps;">Dickey, D. A., and W. A. Fuller</span>. (1979): <a href="https://doi.org/10.1080/01621459.1979.10482531">“Distribution of the Estimators for Autoregressive Time Series with a Unit Root,</a>” <i>Journal of the American Statistical Association</i>, 74, 427–31.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a><span style="font-variant:small-caps;">Hamilton, J.</span> (1994): <i>Time Series Analysis</i>, Princeton, New Jersey: Princeton University Press.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a><span style="font-variant:small-caps;">Nelson, C. R., and C. R. Plosser</span>. (1982): <a href="https://doi.org/10.1016/0304-3932(82)90012-5">“Trends and Random Walks in Macroeconmic Time Series,</a>” <i>Journal of Monetary Economics</i>, 10, 139 162.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_5"></a><span style="font-variant:small-caps;">Phillips, P.</span> (1986): <a href="https://EconPapers.repec.org/RePEc:eee:econom:v:33:y:1986:i:3:p:311-340">“Understanding Spurious Regressions in Econometrics,</a>” <i>Journal of Econometrics</i>, 33, 311–40.</div>
</div>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] },
  tex2jax: {
      inlineMath: [['$','$'],['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script type="text/javascript"
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
