<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ed Herbst/teaching/georgetown/lectures/lecture-machine-learning/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/terminal-0.7.1.min.css">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/animate-3.7.2.min.css">
    <link rel="stylesheet" href="http://localhost:1313/hugo-theme-console/css/console.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="http://localhost:1313/css/custom.css">
<link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
<script src="https://tikzjax.com/v1/tikzjax.js"></script>


    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <meta property="og:title" content="ECON 616: Machine Learning" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/teaching/georgetown/lectures/lecture-machine-learning/" /><meta property="article:published_time" content="2025-03-13T21:19:23-04:00" />



<meta name="twitter:title" content="ECON 616: Machine Learning"/>
<meta name="twitter:description" content="Intro &#43; Defintions
Background

Stuff written by economists: Varian (2014),
Mullainathan and Spiess (2017), Athey (2018)

Useful books: Hastie, Tibshirani, and Friedman (2009),

Gentle introduction: Machine Learning on http://coursera.org;
many other things on the internet (of varying quality).

Computation: scikit-learn (python).

&ldquo;Machine Learning&rdquo; definition


Hard to define; context dependent;



Athey (2018):

&hellip; a field that develops algorithms designed to applied to
datasets with the main areas of focus being prediction
(regression), classification, and clustering or grouping tasks."/>

</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              
              <a href='http://localhost:1313/teaching'>teaching</a>/<a href='http://localhost:1313/teaching/georgetown'>georgetown</a>/<a href='http://localhost:1313/teaching/georgetown/lectures'>lectures</a>/<a href='http://localhost:1313/teaching/georgetown/lectures/lecture-machine-learning'>lecture-machine-learning</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="http://localhost:1313/" typeof="ListItem">&lt;/&gt;</a></li>
                
                <li><a href="http://localhost:1313/research/" typeof="ListItem">research</a></li>
                
                <li><a href="http://localhost:1313/teaching/" typeof="ListItem">teaching</a></li>
                
                <li><a href="http://localhost:1313/etc/" typeof="ListItem">et cetera</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container animated zoomIn fast">
        
<h1>ECON 616: Machine Learning</h1>
<h2 id="intro-plus-defintions">Intro + Defintions</h2>
<h3 id="background">Background</h3>
<ul>
<li>Stuff written by economists: <a href="#citeproc_bib_item_8">Varian (2014)</a>,
<a href="#citeproc_bib_item_6">Mullainathan and Spiess (2017)</a>, <a href="#citeproc_bib_item_1">Athey (2018)</a>
<br></li>
<li>Useful books: <a href="#citeproc_bib_item_4">Hastie, Tibshirani, and Friedman (2009)</a>,
<br></li>
<li>Gentle introduction: Machine Learning on <a href="http://coursera.org">http://coursera.org</a>;
many other things on the internet (of varying quality).
<br></li>
<li>Computation: <code>scikit-learn</code> (python).</li>
</ul>
<h3 id="machine-learning-definition">&ldquo;Machine Learning&rdquo; definition</h3>
<ul>
<li>
<p>Hard to define; context dependent;
<br></p>
</li>
<li>
<p><a href="#citeproc_bib_item_1">Athey (2018)</a>:</p>
<blockquote>
<p>&hellip; a field that develops algorithms designed to applied to
datasets with the main areas of focus being prediction
(regression), classification, and clustering or grouping tasks.</p></blockquote>
 <br>
</li>
<li>
<p>Broadly speaking, two branches:</p>
<ul>
<li><strong>Supervised:</strong> dependent variables known (think predicting
output growth)</li>
<li><strong>Unsupervised:</strong> dependent variables unknown (think classifying
recessions)</li>
</ul>
</li>
</ul>
<h3 id="a-dictionary">A Dictionary</h3>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Econometrics</th>
          <th>ML</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>\(\underbrace{y}_{T\times 1} = \{y_{1:T}\}\)</td>
          <td>Endogenous</td>
          <td>outcome</td>
      </tr>
      <tr>
          <td>\(\underbrace{X}_{T\times n} = \{x_{1:T}\}\)</td>
          <td>Exogenous</td>
          <td>Feature</td>
      </tr>
      <tr>
          <td>\(1:T\)</td>
          <td>&ldquo;in sample&rdquo;</td>
          <td>&ldquo;training&rdquo;</td>
      </tr>
      <tr>
          <td>\(T:T+V \)</td>
          <td>??? not enough data!</td>
          <td>&ldquo;validation&rdquo;</td>
      </tr>
      <tr>
          <td>\(T+V:T+V+O\)</td>
          <td>&ldquo;out of sample&rdquo;</td>
          <td>&ldquo;testing&rdquo;</td>
      </tr>
  </tbody>
</table>
<p>Today I&rsquo;ll concentrate on prediction (regression) problems.
\[
\hat y = f(X;\theta)
\]
Economists would call this modeling the <code>conditional expectation</code>,
MLers the <code>hypothesis function</code>.</p>
<h3 id="it-all-starts-with-a-loss-funciton">It all starts with a loss funciton</h3>
<p>Generally speaking, we can write (any) estimation problem as
essentially a loss minizimation problem.
<br>
Let \(L\)
\[
L(\hat y, y) = L(f(X;\theta), y)
\]
Be a <code>loss function</code> (sometimes called a &ldquo;cost&rdquo; function).
<br>
Estimation in ML: pick \(\theta\) to minimize loss.
<br>
ML more concerned with minimizing my loss than inference on
\(\theta\) per se.
<br>
Forget standard errors&hellip;</p>
<h3 id="gradient-descent">Gradient Descent</h3>
<ul>
<li>In practice, it is often not possible to minimize the loss
function analytically.
<br></li>
<li>In fact, most machine learning models correspond to functions
\(f(\cdot;\theta)\) that are highly nonlinear in \(\theta\).
<br></li>
<li>In addition to an explosition of datasets, a large part of the
success of machine learning algorithms is the development of
robust minimization routines.
<br></li>
<li>The first one everyone learns is called <code>gradient descent</code>:
\[ \theta &rsquo; = \theta + \alpha \frac{d L(\hat y,y)}{d \theta} \]</li>
<li>You can use this for OLS when \(N &gt; T\).</li>
</ul>
<h3 id="example-forecasting-inflation">Example: Forecasting Inflation</h3>
<p>Let&rsquo;s consider forecasting (GDP deflator) inflation.</p>
<figure><img src="http://localhost:1313/ox-hugo/1d19d17054cfd10cfe450afd4266834e63b02472.png">
</figure>

<h3 id="linear-regression">Linear Regression</h3>
<ul>
<li>Consider forecasting inflation using only it&rsquo;s lag and constant.</li>
<li>Training sample: 1985-2000</li>
<li>Testing sample: 2001-2015</li>
<li>scikit-learn code</li>
</ul>
<!--listend-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.linear_model</span> <span style="color:#a2f;font-weight:bold">import</span> LinearRegression
</span></span><span style="display:flex;"><span>linear_model_univariate <span style="color:#666">=</span> LinearRegression()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_start, train_end <span style="color:#666">=</span> <span style="color:#b44">&#39;1985&#39;</span>, <span style="color:#b44">&#39;2000&#39;</span>
</span></span><span style="display:flex;"><span>inf[<span style="color:#b44">&#39;inf_L1&#39;</span>] <span style="color:#666">=</span> inf<span style="color:#666">.</span>GDPDEF<span style="color:#666">.</span>shift(<span style="color:#666">1</span>)
</span></span><span style="display:flex;"><span>inf <span style="color:#666">=</span> inf<span style="color:#666">.</span>dropna(how<span style="color:#666">=</span><span style="color:#b44">&#39;any&#39;</span>)
</span></span><span style="display:flex;"><span>inftrain <span style="color:#666">=</span> inf[train_start:train_end]
</span></span><span style="display:flex;"><span>Xtrain,ytrain <span style="color:#666">=</span> (inftrain<span style="color:#666">.</span>inf_L1<span style="color:#666">.</span>values<span style="color:#666">.</span>reshape(<span style="color:#666">-</span><span style="color:#666">1</span>,<span style="color:#666">1</span>),
</span></span><span style="display:flex;"><span>                 inftrain<span style="color:#666">.</span>inf)
</span></span><span style="display:flex;"><span>fitted_ols <span style="color:#666">=</span> linear_model_univariate<span style="color:#666">.</span>fit(Xtrain,ytrain)
</span></span></code></pre></div><h3 id="many-regressors">Many regressors</h3>
<p>Let&rsquo;s add the individual spf forecasts to our regression.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>/home/eherbst/miniconda3/lib/python3.8/site-packages/openpyxl/worksheet/header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored
</span></span><span style="display:flex;"><span>  warn(&#34;&#34;&#34;Cannot parse header or footer so it will be ignored&#34;&#34;&#34;)
</span></span></code></pre></div><figure><img src="http://localhost:1313/ox-hugo/068fae1f02439ee45e31ae1c9619441847e0a9c4.png">
</figure>

<h3 id="estimating-this-in-scikit-learn-is-easy">Estimating this in scikit learn is easy</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spf_flatted_zero <span style="color:#666">=</span> spf_flat<span style="color:#666">.</span>fillna(<span style="color:#666">0.</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>spfX <span style="color:#666">=</span> spf_flatted_zero[train_forecasters][train_start:train_end]
</span></span><span style="display:flex;"><span>spfXtrain <span style="color:#666">=</span> np<span style="color:#666">.</span>c_[Xtrain, spfX]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>linear_model_spf <span style="color:#666">=</span> LinearRegression()
</span></span><span style="display:flex;"><span>fitted_ols_spf <span style="color:#666">=</span> linear_model_spf<span style="color:#666">.</span>fit(spfXtrain,ytrain)
</span></span></code></pre></div><div class="table-caption">
  <span class="table-number">Table 1:</span>
  Mean Squared Errors
</div>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Train</th>
          <th>Test</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>LS-univariate</td>
          <td>0.59</td>
          <td>2.28</td>
      </tr>
      <tr>
          <td>LS-SPF</td>
          <td>0</td>
          <td>2.1</td>
      </tr>
  </tbody>
</table>
<h3 id="regularization">Regularization</h3>
<p>We&rsquo;ve got way too many variables &ndash; our model does horrible out of
sample!
<br>
Their are many regularization techniques available for variable selection
<br>
Conventional: AIC, BIC
<br>
Alternative approach: <strong>Penalized regression</strong>.
<br>
Consider the loss function:
\[
L(\hat y,y) = \frac{1}{2T} \sum_{t=1}^T (f(x_t;\theta) - y)^2 +
\lambda \sum_{i=1}^N \left[(1-\alpha)|\theta_i| + \alpha|\theta_i|^2\right].
\]
This is called <strong>elastic net regression.</strong> When \(\lambda = 0\), we&rsquo;re back to OLS.
<br>
Many special cases.</p>
<h3 id="ridge-regression">Ridge Regression</h3>
<p>The ridge regression <a href="#citeproc_bib_item_5">Hoerl and Kennard (2004)</a> is special case where \(\alpha
= 1\).
<br>
Long (1940s) used in statistics and econometrics.
<br>
This is sometimes called (or is a special case of) &ldquo;Tikhonov regularization&rdquo;
<br>
It&rsquo;s an L2 penalty, so it&rsquo;s won&rsquo;t force parameters to be exactly
zero.
<br>
Can be formulatd as Bayesian linear regression.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.linear_model</span> <span style="color:#a2f;font-weight:bold">import</span> Ridge
</span></span><span style="display:flex;"><span>fitted_ridge <span style="color:#666">=</span> Ridge()<span style="color:#666">.</span>fit(spfXtrain,ytrain)
</span></span></code></pre></div><h3 id="ls-vs-ridge--lambda-1--coefficients">LS vs Ridge (\(\lambda = 1\)) Coefficients</h3>
<figure><img src="http://localhost:1313/ox-hugo/994e0d39c1619f0074161c4da74b641e157dc082.png">
</figure>

<h3 id="lasso-regression">Lasso Regression</h3>
<p>Set \(\alpha = 0\)</p>
<ul>
<li>This is an \(L1\) penalty &ndash; forces small coefficients to exactly 0.
<br></li>
<li>Greatly reduces model complexity.
<br></li>
<li>Can you give economic interpretation to the parameters?
<br></li>
<li>Bayesian interpetation: Laplace prior in \(\theta\).</li>
</ul>
<!--listend-->
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.linear_model</span> <span style="color:#a2f;font-weight:bold">import</span> Lasso
</span></span><span style="display:flex;"><span>fitted_lasso <span style="color:#666">=</span> Lasso()<span style="color:#666">.</span>fit(spfXtrain,ytrain)
</span></span></code></pre></div><h3 id="ls-vs-lasso--lambda-1--coefficients">LS vs Lasso (\(\lambda = 1\)) Coefficients</h3>
<figure><img src="http://localhost:1313/ox-hugo/7f0016ecd5223871ce78983fed3c8bcd36c930c5.png">
</figure>

<h3 id="picking-lambda">Picking \(\lambda\)</h3>
<ul>
<li>
<p>Use &ldquo;rule of thumb&rdquo; given subject matter.</p>
</li>
<li>
<p><strong>Tradition validation:</strong> Use many \(\lambda\) on your test sample,
Assess accuracy of each on validation sample, pick one which
gives minimum loss.</p>
</li>
<li>
<p><strong>Cross validation:</strong></p>
<ol>
<li>Divide sample in \(K\) parts:</li>
<li>For each \(k \in K\), pick \(\lambda_k\), fit model <em>using the
\(K-k\) sample</em>.</li>
<li>Plot Loss against \(\lambda_k\), pick \(\lambda\) which yields minimum.</li>
</ol>
</li>
<li>
<p><a href="#citeproc_bib_item_3">Chernozhukov et al. (2018)</a> derive &ldquo;Oracle&rdquo; properties for LASSO, pick \(\lambda\)
based on this.</p>
</li>
</ul>
<div class="table-caption">
  <span class="table-number">Table 2:</span>
  Mean Squared Error, \(\lambda = 1\)
</div>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Train</th>
          <th>Train</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Least Squares (Univariate)</td>
          <td>0.35</td>
          <td>0.71</td>
      </tr>
      <tr>
          <td>Least Squares (SPF)</td>
          <td>0.0</td>
          <td>0.68</td>
      </tr>
      <tr>
          <td>Least Squares (SPF-Ridge)</td>
          <td>0.0003</td>
          <td>0.67</td>
      </tr>
      <tr>
          <td>Least Squares (SPF-Lasso)</td>
          <td>0.59</td>
          <td>0.96</td>
      </tr>
  </tbody>
</table>
<h2 id="support-vector-machines">Support Vector Machines</h2>
<h3 id="support-vector-machines">Support Vector Machines</h3>
<ul>
<li>While Elastic net, lasso, and ridge were designed around
regularization, other machine learning techniques are designed to
fit more flexible models.</li>
<li><strong>Support Vector Machines</strong> are typically used in classification
problems.
<br></li>
<li>Essentially SVM constructs a seperating hyperplane (hello 2nd
basic welfare theorem), to optimally seperate (&ldquo;classify&rdquo;)
points.
<br></li>
<li>What&rsquo;s cool about the support vector machine is that you can use
a kernel trick, so your hyperplanes need not correspond to lines
in euclidean space.
<br></li>
<li>For regression, the hyperplane will be prediction.</li>
</ul>
<h3 id="support-vector-machines">Support Vector Machines</h3>
<ul>
<li>Explicit form:
\[
f(x;\theta) = \sum_{i=1}^n \theta_i h(x) + \theta_0.
\]</li>
<li>\(\epsilon\) insensitive loss: function does not penalize
predictions which are in an \(\epsilon\) of the, otherwise the
penalized by a factor relatid to \(C\) (comes from dual problem).
<br></li>
<li>Key choice here: the choice of kernel
<br></li>
<li>\(\epsilon\), \(C\) chosen by (cross) validation.</li>
</ul>
<h3 id="estimating-support-vector-machine">Estimating Support Vector Machine</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.svm</span> <span style="color:#a2f;font-weight:bold">import</span> SVR
</span></span><span style="display:flex;"><span>fitted_svm <span style="color:#666">=</span> SVR()<span style="color:#666">.</span>fit(Xtrain,ytrain)
</span></span></code></pre></div><div class="table-caption">
  <span class="table-number">Table 3:</span>
  Mean Squared Error, \(\Lambda = 1\)
</div>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Train</th>
          <th>Train</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Least Squares (Univariate)</td>
          <td>0.35</td>
          <td>0.71</td>
      </tr>
      <tr>
          <td>Least Squares (SVM)</td>
          <td>0.06</td>
          <td>0.73</td>
      </tr>
  </tbody>
</table>
<h2 id="other-ml-techniques">Other ML Techniques</h2>
<h3 id="other-popular-ml-techniques">Other Popular ML Techniques</h3>
<ul>
<li>
<p><strong>Forests</strong>: partition feature space, fit individual models
condition on subsample.</p>
</li>
<li>
<p>Obviously, you can partition ad infinitum to obtain perfect
predictions.</p>
</li>
<li>
<p><strong>Random forest:</strong>  pick random subspace/set; average over these
random models.</p>
</li>
<li>
<p>Long literature in econometrics about model averaging
<a href="#citeproc_bib_item_2">Bates and Granger (1969)</a>.</p>
</li>
<li>
<p>Further refinements: bagging, boosting.</p>
</li>
</ul>
<h2 id="neural-networks">Neural Networks</h2>
<h3 id="neural-networks">Neural Networks</h3>
<p>Let&rsquo;s construct a hypothesis function using a neural network.
<br>
Suppose that we have \(N\) features in \(x_t\).
<br>
(Let \(x_{0,t}\) be the intercept.)
<br>
Neural Networks are modeled after the way neurons work in a brain as basical
computational units.</p>
<ul>
<li>Inputs (dendrites) channeled to outputs (axons)</li>
<li>Here the input is \(x_t\) and the output is \(f(x_t;\theta)\).</li>
<li>The neuron maps the inputs to outputs using a (nonlinear)
<strong>activation function</strong> \(\(g\)\).</li>
<li>By adding <strong>layers</strong> of neurons, we can create very (arbitrary)
complex prediction models (all &ldquo;logical gates&rdquo;).</li>
</ul>
<h3 id="neural-networks-continued">Neural Networks, continued</h3>
<p>Drop the \(t\) subscript. Consider:
\[
\left[\begin{array}{c} x_{0} \\ \vdots \\ x_{N} \end{array}\right]
\rightarrow
[~]
\rightarrow
f(x;\theta)
\]
\(a_i^j\) activation of unit \(i\) in layer \(j\).</p>
<p>\(\beta^j\) matrix of weights controlling function mapping layer \(j\)
to layer \(j+1\).
\[
\left[\begin{array}{c} x_{0} \\ \vdots \\ x_{N} \end{array}\right]
\rightarrow
\left[\begin{array}{c} a_{0}^2 \\ \vdots \\ a_{N}^2 \end{array}\right]
\rightarrow
f(x;\theta)
\].</p>
<h3 id="neural-networks-in-a-figure">Neural Networks in a figure</h3>
<figure><img src="http://localhost:1313/ox-hugo/neural_net.png">
</figure>

<h3 id="neural-networks-continued">Neural Networks Continued</h3>
<p>If \(N = 2\) and our neural network has \(1\) hidden layer.</p>
<p>\begin{eqnarray}
a_1^2 &amp;=&amp; g(\theta_{10}^1 x_0 + \theta_{11}^1 x_1 + \theta_{12}^1 x_2) \nonumber \\
a_2^2 &amp;=&amp; g(\theta_{20}^1 x_0 + \theta_{21}^1 x_1 + \theta_{22}^1 x_2) \nonumber \\
f(x;\theta) &amp;=&amp; g(\theta_{10}^2 a_0^2 + \theta_{11}^2 a_1^2 + \theta_{12}^2 a_2^2 \\
\end{eqnarray}</p>
<p>(\(a_0^j\) is always a constant (&ldquo;bias&rdquo;) by convention.)
<br>
Matrix of coefficients \(\theta^j\) sometimes called <strong>weights</strong>
<br>
Depending on \(g\), \(f\) is highly nonlinear in \(x\)! Good and bad &hellip;
<br></p>
<h3 id="which-activation-function">Which activation function?</h3>
<table>
  <thead>
      <tr>
          <th>name</th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>linear</td>
          <td>\(\theta x\)</td>
      </tr>
      <tr>
          <td>sigmoid</td>
          <td>\(1/(1+e^{-\theta x}\)</td>
      </tr>
      <tr>
          <td>tanh</td>
          <td>\(tanh(\theta x)\)</td>
      </tr>
      <tr>
          <td>rectified linear unit</td>
          <td>\(max(0,\theta x)\)</td>
      </tr>
      <tr>
          <td>&hellip;</td>
          <td>&hellip;</td>
      </tr>
  </tbody>
</table>
<p>How to pick \(g\)&hellip;?</p>
<ul>
<li>Dependent on problem: prediction vs classification.</li>
<li>Think about derivate of cost/loss wrt deep parameters.</li>
<li>Trial and error</li>
</ul>
<h3 id="how-to-estimate-this-model-dot">How to estimate this model.</h3>
<p>Just like any other ML model: minimize the loss!
<br>
Gradient descent needs a derivative
<br>
<strong>back propagation</strong> algorithm</p>
<h3 id="application">Application: <a href="#citeproc_bib_item_7">Nakamura (2005)</a></h3>
<ul>
<li><a href="#citeproc_bib_item_7">Nakamura (2005)</a> considers (GDP deflator) inflation forecasting
with a neural network.
<br></li>
<li>Model has <strong>1 hidden layer</strong>, and uses a <strong>hyperbolic tangent</strong>
activation function
<br></li>
<li>Can be explicitly written as:
\[
\hat\pi_{t+h} = w_{2,1}\tanh(w_{1,1}&rsquo; x_{t} + b_{1,1}) + w_{2,2}\tanh(w_{1,2}'
x_{t} + b_{1,2}) + b_{2,1}
\]
<br></li>
<li>\(x_{t}\) is a vector of \(t-1\) variables.  For simplicity, I&rsquo;ll
consider \(x_{t-1} = \pi_{t-1}\).</li>
</ul>
<h3 id="scikit-learn-code">scikit-learn code</h3>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">sklearn.neural_network</span> <span style="color:#a2f;font-weight:bold">import</span> MLPRegressor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>NN <span style="color:#666">=</span> MLPRegressor(hidden_layer_sizes<span style="color:#666">=</span>(<span style="color:#666">2</span>,),
</span></span><span style="display:flex;"><span>                  activation<span style="color:#666">=</span><span style="color:#b44">&#39;tanh&#39;</span>,
</span></span><span style="display:flex;"><span>                  alpha<span style="color:#666">=</span><span style="color:#666">1e-6</span>,
</span></span><span style="display:flex;"><span>                  max_iter<span style="color:#666">=</span><span style="color:#666">10000</span>,
</span></span><span style="display:flex;"><span>                  solver<span style="color:#666">=</span><span style="color:#b44">&#39;lbfgs&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fitted_NN <span style="color:#666">=</span> NN<span style="color:#666">.</span>fit(Xtrain,ytrain)
</span></span></code></pre></div><h3 id="neural-network-vs-dot-ar--1--predicted-values">Neural Network vs. AR(1): Predicted Values</h3>
<figure><img src="http://localhost:1313/ox-hugo/27aa484185c7d77b4729264ddc2d7262d441ba42.png">
</figure>

<h3 id="what-is-the-right-method-to-use">What is the &ldquo;right&rdquo; method to use</h3>
<p>You might have guessed&hellip;
<br>
<a href="#citeproc_bib_item_9">Wolpert and Macready (1997)</a>: A <strong>universal learning</strong> algorithm does <em>cannot</em> exist.
<br>
Need prior knowledge about problem&hellip;
<br>
This is has been present in econometrics for a very long time&hellip;
<br>
There&rsquo;s no free lunch!.</p>
<h2 id="bibliography">Bibliography</h2>
<h3 id="references">References</h3>
<h2 id="references-1">References</h2>
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a><span style="font-variant:small-caps;">Athey, S.</span> (2018): <a href="https://EconPapers.repec.org/RePEc:nbr:nberch:14009">“The Impact of Machine Learning on Economics,</a>” in <i>The Economics of Artificial Intelligence: An Agenda</i>, National Bureau of Economic Research, Inc.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a><span style="font-variant:small-caps;">Bates, J. M., and C. W. J. Granger</span>. (1969): <a href="https://doi.org/10.2307/3008764">“The Combination of Forecasts,</a>” <i>Or</i>, 20, 451.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a><span style="font-variant:small-caps;">Chernozhukov, V., W. K. Härdle, C. Huang, and W. Wang</span>. (2018): <a href="https://doi.org/10.2139/ssrn.3188362">“Lasso-Driven Inference in Time and Space,</a>” <i>Ssrn Electronic Journal</i>, .</div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a><span style="font-variant:small-caps;">Hastie, T., R. Tibshirani, and J. Friedman</span>. (2009): <a href="https://doi.org/10.1007/978-0-387-84858-7">“The Elements of Statistical Learning,</a>” <i>Springer Series in Statistics</i>, .</div>
  <div class="csl-entry"><a id="citeproc_bib_item_5"></a><span style="font-variant:small-caps;">Hoerl, A. E., and R. W. Kennard</span>. (2004): <a href="https://doi.org/10.1002/0471667196.ess2280">“Ridge Regression,</a>” <i>Encyclopedia of Statistical Sciences</i>, .</div>
  <div class="csl-entry"><a id="citeproc_bib_item_6"></a><span style="font-variant:small-caps;">Mullainathan, S., and J. Spiess</span>. (2017): <a href="https://doi.org/10.1257/jep.31.2.87">“Machine Learning: An Applied Econometric Approach,</a>” <i>Journal of Economic Perspectives</i>, 31, 87 106.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_7"></a><span style="font-variant:small-caps;">Nakamura, E.</span> (2005): <a href="https://doi.org/10.1016/j.econlet.2004.09.003">“Inflation Forecasting Using a Neural Network,</a>” <i>Economics Letters</i>, 86, 373 378.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_8"></a><span style="font-variant:small-caps;">Varian, H. R.</span> (2014): <a href="https://doi.org/10.1257/jep.28.2.3">“Big Data: New Tricks for Econometrics,</a>” <i>Journal of Economic Perspectives</i>, 28, 3 28.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_9"></a><span style="font-variant:small-caps;">Wolpert, D., and W. Macready</span>. (1997): <a href="https://doi.org/10.1109/4235.585893">“No Free Lunch Theorems for Optimization,</a>” <i>Ieee Transactions on Evolutionary Computation</i>, 1, 67 82.</div>
</div>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] },
  tex2jax: {
      inlineMath: [['$','$'],['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script type="text/javascript"
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
