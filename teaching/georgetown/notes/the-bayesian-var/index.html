<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ed Herbst/teaching/georgetown/notes/the-bayesian-var/</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="https://edherbst.net/hugo-theme-console/css/terminal-0.7.1.min.css">
    <link rel="stylesheet" href="https://edherbst.net/hugo-theme-console/css/animate-3.7.2.min.css">
    <link rel="stylesheet" href="https://edherbst.net/hugo-theme-console/css/console.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
    <link rel="stylesheet" href="https://edherbst.net/css/custom.css">
<link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
<script src="https://tikzjax.com/v1/tikzjax.js"></script>


    
      <!--[if lt IE 9]>
          <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
          <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
       <meta property="og:title" content="Bayesian VARs" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://edherbst.net/teaching/georgetown/notes/the-bayesian-var/" /><meta property="article:published_time" content="2025-03-14T09:50:50-04:00" />



<meta name="twitter:title" content="Bayesian VARs"/>
<meta name="twitter:description" content="
Lecture Objective: Basic Introduction to Bayesian VARs with a short tour of structural identification from a Bayesian perspective.

Additional Readings:
The handbook chapter by Del Negro and Schorfheide (2011) is very good; much of these notes are abstracted from that.

Now we&rsquo;re going to start putting things together to learn to estimate a cornerstone (Bayesian) model of macroeconomics, the autoregression (VAR.)  VARs were first introduced to macroeconomics by Christopher A. Sims in Sims (1980) in an extremely important paper: &ldquo;Macroeconomics and Reality.&rdquo;  Sims attacked the prevailing macroeconometric models of the day&mdash;including those used at the pFed&mdash;which featured systems of equations with many coefficients imposed, often as a zero.  These models were estimated equation-by-equation, and often gave a (false) impression of both estimation precision and about the importance of particular transmission channels.  Sims instead used a flexible VAR which modeled the contemporaneous and dynamic depedence for a time series of \(n\) variables \(y_t\).  The VAR of order \(p\) follows the set of linear difference equations:"/>

</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
          <header class="terminal-logo">
            <div class="logo terminal-prompt">
              
              
              
              <a href='https://edherbst.net/teaching'>teaching</a>/<a href='https://edherbst.net/teaching/georgetown'>georgetown</a>/<a href='https://edherbst.net/teaching/georgetown/notes'>notes</a>/<a href='https://edherbst.net/teaching/georgetown/notes/the-bayesian-var'>the-bayesian-var</a>/</div></header>
          <nav class="terminal-menu">
            <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                
                <li><a href="https://edherbst.net/" typeof="ListItem">&lt;/&gt;</a></li>
                
                <li><a href="https://edherbst.net/research/" typeof="ListItem">research</a></li>
                
                <li><a href="https://edherbst.net/teaching/" typeof="ListItem">teaching</a></li>
                
                <li><a href="https://edherbst.net/etc/" typeof="ListItem">et cetera</a></li>
                
            </ul>
          </nav>
        </div>
    </div>

    <div class="container animated zoomIn fast">
        
<h1>Bayesian VARs</h1>
<div class="additional">
<p><strong>Lecture Objective</strong>: Basic Introduction to Bayesian VARs with a short tour of structural identification from a Bayesian perspective.
<br /><br />
<strong>Additional Readings:</strong>
The handbook chapter by <a href="#citeproc_bib_item_4">Del Negro and Schorfheide (2011)</a> is very good; much of these notes are abstracted from that.</p>
</div>
<p>Now we&rsquo;re going to start putting things together to learn to estimate a cornerstone (Bayesian) model of macroeconomics, the autoregression (VAR.)  VARs were first introduced to macroeconomics by Christopher A. Sims in <a href="#citeproc_bib_item_9">Sims (1980)</a> in an extremely important paper: &ldquo;Macroeconomics and Reality.&rdquo;  Sims attacked the prevailing macroeconometric models of the day&mdash;including those used at the pFed&mdash;which featured systems of equations with many coefficients imposed, often as a zero.  These models were estimated equation-by-equation, and often gave a (false) impression of both estimation precision and about the importance of particular transmission channels.  Sims instead used a flexible VAR which modeled the contemporaneous and dynamic depedence for a time series of \(n\) variables \(y_t\).  The VAR of order \(p\) follows the set of linear difference equations:</p>
<p>\begin{align}
y_t = \Phi_0 + \Phi_1 y_{t-1} + \ldots + \Phi_p y_{t-p} + u_t.
\end{align}</p>
<p>The innovations of \(u_t\) strictly speaking need only be white noise, but we&rsquo;ll assume that they are independently and identically multivariate normally distributed,</p>
<p>\begin{align}
u_t \stackrel{i.i.d.}{\sim} \mathcal N\left(0, \Sigma\right).
\end{align}</p>
<p>So the VAR(\(p\)) mimics the previously discussed AR(\(p\)) with the difference being that \(y_t\) is now an \(n\times 1\) vector rather than a scalar.</p>
<h2 id="multivariate-time-series-basics">Multivariate Time Series Basics</h2>
<p>For theoretical analysis, it is often convenient to express the VAR(p) in the so-called companion form.</p>
<p>\begin{eqnarray}
\hspace*{-0.5in}
\left[ \begin{array}{c} y_t \\ y_{t-1} \\ \vdots \\ y_{t-p+1} \end{array} \right]
= \left[ \begin{array}{c} \Phi_0 \\ 0 \\ \vdots \\ 0 \end{array} \right]</p>
<ul>
<li>\left[ \begin{array}{ccccc}
\Phi_1 &amp; \Phi_2 &amp; \cdots &amp; \Phi_{p-1} &amp; \Phi_p \\
I      &amp;  0     &amp; \cdots &amp;  0         &amp; 0      \\
\vdots &amp; \vdots &amp; \ddots &amp;  0         &amp; 0      \\
0      &amp;  0     &amp; \cdots &amp;  I         &amp; 0
\end{array}
\right]
\left[ \begin{array}{c} y_{t-1} \\ y_{t-2} \\ \vdots \\ y_{t-p} \end{array} \right]
+ \left[ \begin{array}{c} u_t \\ 0 \\ \vdots \\ 0 \end{array} \right]
\label{e_varcomp}
\nonumber
\end{eqnarray}</li>
</ul>
<p>Let \(\xi_t = [y_t&rsquo;, y_{t-1}&rsquo;, \ldots, y_{t-p+1}&rsquo;]&rsquo;\). The VAR can be rewritten as</p>
<p>\begin{eqnarray}
\xi_t = F_0 + F_1 \xi_{t-1} + \nu_t
\end{eqnarray}</p>
<p>where the definitions of \(F_0\), \(F_1\), and \(\nu_t\) can be deduced from the previous equation.  We have transformed our VAR(\(p\)) into a first-order vector autoregression (VAR(1)) in the companion form.   We can easy recover the original time series by defining the \(n \times np\) matrix \(M_n = [I,0]\) where \(I\) is an \(n \times n\) identity matrix, so that \(y_t = M_n \xi_t\).  The companion form is useful in two respects: (1) to define stationarity in the context of a VAR; (2) to convince ourselves that without loss of much generality we can restrict econometric analyses to VAR(1) specifications.</p>
<p><strong>Result</strong> For a vector autoregression to be covariance stationary it is necessary that all eigenvalues of the matrix
\(F_1\) are less than one in absolute value. \(\Box\)</p>
<h3 id="example">Example</h3>
<p>Consider the univariate AR(2) process
\[
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + u_t
\]
The AR(2) process can be written in companion form as a VAR(1) where \(\xi_t = [y_t , y_{t-1}]&rsquo;\) and
\[
F_1 = \left[ \begin{array}{cc}
\phi_1 &amp; \phi_2 \\
1  &amp; 0
\end{array}
\right]
\]
The eigenvalues \(\lambda\) of the matrix \(F_1\) satisfy the condition
\[
det( F_1 - \lambda I) = 0
\iff
(\phi_1 - \lambda)(-\lambda) - \phi_2 = 0
\]
Provided that \(\lambda \not= 0\) the equation can be rewritten as
\[
0 = 1 - \phi_1 \frac{1}{\lambda} - \phi_2 \frac{1}{\lambda^2}
\]
Thus, the condition \(|\lambda| &lt; 1\) is, at least in this example, equivalent to the condition that all the roots of the polynomial \(\phi(z)\) are greater than one in absolute value. A generalization of this example can be found in Hamilton (1994, Chapter 1). \(\Box\)</p>
<h3 id="describing-a-covariance-stationary-var--p--process">Describing a Covariance Stationary VAR(p) Process</h3>
<p>Consider a VAR(p). The expected value of \(y_t\) has to satisfy the vector difference equation</p>
<p>\begin{eqnarray}
{\mathbb E}[y_t] = \Phi_0 + \Phi_1 {\mathbb E}[y_{t-1}] + \ldots \Phi_p {\mathbb E}[y_{t-p}] \quad \mbox{for all} \; t
\end{eqnarray}</p>
<p>If the eigenvalues of \(F_1\) are all less than one in absolute values and the VAR was initialized in the infinite past, then the expected value is given by</p>
<p>\begin{eqnarray}
{\mathbb E}[y_t] = [ I - \Phi_1 - \ldots \Phi_t]^{-1} \Phi_0
\end{eqnarray}</p>
<p>To calculate the autocovariances we will assume that \(\Phi_0 = 0\).
Consider the companion form</p>
<p>\begin{eqnarray}
\xi_t = F_1 \xi_{t-1} + \nu_t
\end{eqnarray}</p>
<p>If the eigenvalues of \(F_1\) are all less than one in absolute value
and the VAR was initialized in the infinite past, than the
autocovariance matrix of order zero has to satisfy the equation</p>
<p>\begin{eqnarray}
\Gamma_{\xi \xi,0} = {\mathbb E}[\xi_t \xi_t&rsquo;] = F_1 \Gamma_{\xi \xi,0} F_1&rsquo; + {\mathbb E}[ \nu_t \nu_t&rsquo;]
\end{eqnarray}</p>
<p>Obtaining a closed form solution for \(\Gamma_{\xi \xi,0}\) is
a bit more complicated than in the univariate AR(1) case.</p>
<h4 id="some-facts">Some Facts</h4>
<p>\begin{definition} Let $A$ and $B$ be  $2 \times 2$ matrices with the
elements
\[
A = \left[ \begin{array}{cc}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22}
\end{array}
\right], \quad
B = \left[ \begin{array}{cc}
b_{11} &amp; b_{12} \\
b_{21} &amp; b_{22}
\end{array}
\right]
\]
The $vec$ operator is defined as the operator that stacks the
columns of a matrix, that is,
\[
vec(A) = [ a_{11}, a_{21}, a_{12}, a_{22} ]'
\]
and the Kronecker product is defined as
\[
A \otimes B = \left[ \begin{array}{cc}
a_{11}B &amp; a_{12}B \\
a_{21}B &amp; a_{22}B
\end{array}
\right] \quad \Box
\]
\end{definition}</p>
<p>\begin{lemma} Let $A$, $B$, $C$ be matrices whose dimension
are such that the product $ABC$ exists. Then
$vec(ABC) = (C&rsquo; \otimes A)vec(B) \quad \Box$
\end{lemma}</p>
<p>A closed form solution for the elements of the covariance
matrix of \(\xi_t\) can be obtained as follows</p>
<p>\begin{eqnarray}
vec(\Gamma_{\xi \xi,0}) &amp; = &amp;(F_1 \otimes F_1) vec(\Gamma_{\xi\xi,0}) + vec( {\mathbb E}[\nu_t \nu_t&rsquo;] ) \nonumber \\
&amp; = &amp; [ I - (F_1 \otimes F_1)]^{-1} vec( {\mathbb E}[\nu_t \nu_t&rsquo;] )
\end{eqnarray}</p>
<p>Since</p>
<p>\begin{eqnarray}
{\mathbb E}[ \xi_t \xi_{t-h}&rsquo; ] = F {\mathbb E}[\xi_{t-1} \xi_{t-h}&rsquo;] + {\mathbb E}[\nu_t \xi_{t-h}&rsquo;]
\end{eqnarray}</p>
<p>we can deduce that</p>
<p>\begin{eqnarray}
\Gamma_{\xi \xi,h} = F^h_1 \Gamma_{\xi \xi,0}
\end{eqnarray}</p>
<p>To obtain the autocovariance \(\Gamma_{\xi \xi,-h}\) we have to keep
track of a transpose in the general matrix case:</p>
<p>\begin{eqnarray}
\Gamma_{\xi \xi,-h} = {\mathbb E}[ \xi_{t-h} \xi_t&rsquo; ] = \bigg[ {\mathbb E}[ \xi_t \xi_{t-h}&rsquo;] \bigg]&rsquo; = \Gamma_{\xi \xi,h}'
\end{eqnarray}</p>
<p>Once we have calculate that autocovariances for the companion
form process \(\xi_t\) it is straightforward to obtain the autocovariances
for the \(y_t\) process. Since \(y_t = M_n \xi_t\) it follows that</p>
<p>\begin{eqnarray}
\Gamma_{yy,h} = {\mathbb E}[y_t y_{t-h}&rsquo;] = {\mathbb E}[ M_n \xi_t \xi_{t-h}&rsquo; M_n&rsquo;] = M_n \Gamma_{\xi \xi,h} M_n'
\end{eqnarray}</p>
<p><strong>Result</strong>: Consider the vector autoregression
\[
y_t = \Phi_0 + \Phi_1 y_{t-1} + \ldots + \Phi_p y_{t-p} + u_t
\]
where \(u_t \sim iid {\cal N}(0,\Sigma)\) with
companion form
\[
\xi_t = F_0 + F_1 \xi_{t-1} + \nu_t
\]
Suppose that the eigenvalues of \(F_1\) are all less than
one in absolute values and that the vector autoregression was initialized
in the infinite past. Under these assumptions the vector process
\(y_t\) is covariance stationary with the moments</p>
<p>\begin{eqnarray}
{\mathbb E}[y_t] &amp; = &amp; [ I - \Phi_1 - \ldots \Phi_t]^{-1} \Phi_0 \\
\Gamma_{yy,h} &amp; = &amp; M_n \Gamma_{\xi \xi,h} M_n&rsquo; \quad \forall h
\end{eqnarray}</p>
<p>where</p>
<p>\begin{eqnarray}
vec(\Gamma_{\xi\xi,0}) &amp; = &amp; [ I - (F_1 \otimes F_1)]^{-1} vec( {\mathbb E}[\nu_t \nu_t&rsquo;] ) \\
\Gamma_{\xi \xi,h} &amp; = &amp; F_1^h \Gamma_{\xi \xi,0} \quad h &gt; 0 \quad \Box
\end{eqnarray}</p>
<h2 id="the-likelihood-of-a-var--p">The Likelihood of a VAR(p)</h2>
<p>We will now derive the likelihood function for a Gaussian VAR(p), conditional
on initial observations \(y_0, \ldots, y_{-p+1}\).
The density of \(y_t\) conditional on \(y_{t-1}, y_{t-2}, \ldots\) and the
coefficient matrices \(\Phi_0, \Phi_1, \ldots, \Sigma\) is of the form</p>
<p>\begin{eqnarray}
\hspace*{-0.5in}
p(y_t|Y^{t-1}, \Phi_0, \ldots, \Sigma)
&amp;\propto&amp; |\Sigma|^{-1/2} \exp \bigg\{ - \frac{1}{2}
( y_t - \Phi_0 - \Phi_1 y_{t-1} - \ldots - \Phi_p y_{t-p} )&rsquo; \nonumber \\
&amp;~&amp; \times \Sigma^{-1} ( y_t - \Phi_0 - \Phi_1 y_{t-1} - \ldots - \Phi_p y_{t-p} ) \bigg\}
\end{eqnarray}</p>
<p>Define the \((np+1) \times 1\) vector \(x_t\) as
\[
x_t = [ 1, y_{t-1}&rsquo;, \ldots, y_{t-p}&rsquo;]'
\]
Moreover, define the matrixes
\[
Y = \left[ \begin{array}{c}
y_1&rsquo; \\ \vdots \\y_T&rsquo; \end{array} \right], \quad
X = \left[ \begin{array}{c}
x_1&rsquo; \\ \vdots \\x_T&rsquo; \end{array} \right], \quad
\Phi = [ \Phi_0, \Phi_1, \ldots, \Phi_p]'
\]
The conditional density of \(y_t\) can be written in more compact
notation as</p>
<p>\begin{eqnarray}
p(y_t| Y^{t-1}, \Phi, \Sigma)
\propto |\Sigma|^{-1/2} \exp \left\{ - \frac{1}{2}
( y_t&rsquo; - x_t&rsquo;\Phi )
\Sigma^{-1} ( y_t&rsquo; - x_t&rsquo;\Phi )&rsquo; \right\}
\end{eqnarray}</p>
<p>To manipulate the density we will use some matrix algebra facts.</p>
<p><strong>Facts</strong>:</p>
<ol>
<li>Let \(a\) be a \(n \times 1\) vector, \(B\) be a symmetric positive definite
\(n \times n\) matrix, and \(tr\) the trace operator that sums the diagonal
elements of a matrix. Then
\[
a&rsquo;Ba = tr[Baa&rsquo;]
\]</li>
<li>Let \(A\) and \(B\) be two \(n \times n\) matrices, then
\[
tr[A+B] = tr[A] + tr[B]
\]</li>
</ol>
<p>In a first step, we will replace the inner product in the expression for the
conditional density by the trace of the outer product</p>
<p>\begin{eqnarray}
p(y_t| Y^{t-1}, \Phi, \Sigma)
\propto |\Sigma|^{-1/2} \exp \left\{ - \frac{1}{2}
tr[ \Sigma^{-1}( y_t&rsquo; - x_t&rsquo;\Phi )&rsquo;( y_t&rsquo; - x_t&rsquo;\Phi )] \right\}
\end{eqnarray}</p>
<p>In the second step, we will take the product of the conditional densities
of \(y_1, \ldots, y_T\) to obtain the joint density. Let \(Y_0\) be a vector
with initial observations</p>
<p>\begin{eqnarray}
p(Y|\Phi,\Sigma, Y_0)
&amp;=&amp; \prod_{t=1}^T p(y_t|Y^{t-1}, Y_0, \Phi, \Sigma) \nonumber \\
&amp;\propto&amp; |\Sigma|^{-T/2}  \exp \left\{ -\frac{1}{2} \sum_{t=1}^T
tr[\Sigma^{-1}( y_t&rsquo; - x_t&rsquo;\Phi )&rsquo;( y_t&rsquo; - x_t&rsquo;\Phi )] \right\} \nonumber \\
&amp;\propto&amp;  |\Sigma|^{-T/2}  \exp \left\{ -\frac{1}{2}
tr\left[\Sigma^{-1}\sum_{t=1}^T( y_t&rsquo; - x_t&rsquo;\Phi )&rsquo;( y_t&rsquo; - x_t&rsquo;\Phi )\right] \right\} \nonumber \\
&amp;\propto&amp; |\Sigma|^{-T/2} \exp \left\{ -\frac{1}{2}
tr [ \Sigma^{-1} (Y-X\Phi)&rsquo;(Y-X\Phi) ] \right\}
\end{eqnarray}</p>
<p>Define the ``OLS&rsquo;&rsquo; estimator</p>
<p>\begin{eqnarray}
\hat{\Phi} = (X&rsquo;X)^{-1} X&rsquo;Y
\end{eqnarray}</p>
<p>and the sum of squared OLS residual matrix</p>
<p>\begin{eqnarray}
S = (Y - X \hat{\Phi})&rsquo;(Y- X \hat{\Phi})
\end{eqnarray}</p>
<p>It can be verified that</p>
<p>\begin{eqnarray}
(Y - X\Phi)&rsquo;(Y-X\Phi) = S + (\Phi - \hat{\Phi})&lsquo;X&rsquo;X(\Phi- \hat{\Phi})
\end{eqnarray}</p>
<p><strong>Problem:</strong> Verify this!</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#666">=</span> <span style="color:#666">1</span>
</span></span></code></pre></div><p>This leads to the following representation of the likelihood function</p>
<p>\begin{eqnarray}
p(Y|\Phi,\Sigma, Y_0)
&amp;\propto&amp;  |\Sigma|^{-T/2} \exp \left\{ -\frac{1}{2} tr[\Sigma^{-1} S] \right\}\nonumber \\
&amp;~&amp;\times  \exp \left\{ -\frac{1}{2} tr[ \Sigma^{-1}(\Phi - \hat{\Phi})&lsquo;X&rsquo;X(\Phi- \hat{\Phi})] \right\}
\end{eqnarray}</p>
<h3 id="an-alternative-representation">An Alternative Representation</h3>
<p>Let \(\beta = vec(\Phi)\) and \(\hat{\beta} = vec(\hat{\Phi})\). It can be verified that</p>
<p>\begin{eqnarray}
tr[ \Sigma^{-1}(\Phi - \hat{\Phi})&lsquo;X&rsquo;X(\Phi- \hat{\Phi})]
= (\beta - \hat{\beta})&rsquo;[ \Sigma \otimes (X&rsquo;X)^{-1} ]^{-1} (\beta - \hat{\beta})
\end{eqnarray}</p>
<p>and the likelihood function has the alternative representation</p>
<p>\begin{eqnarray}
p(Y|\Phi,\Sigma, Y_0)
&amp;\propto&amp;  |\Sigma|^{-T/2} \exp \left\{ -\frac{1}{2} tr[\Sigma^{-1} S] \right\} \nonumber \\
&amp;~&amp;\times \exp \left\{ -\frac{1}{2} (\beta - \hat{\beta})&rsquo;[ \Sigma \otimes (X&rsquo;X)^{-1} ]^{-1} (\beta - \hat{\beta}) \right\} \nonumber
\end{eqnarray}</p>
<h2 id="bayesian-inference">Bayesian Inference</h2>
<p>Before we do Bayesian inference we need to get some multivariate
statistical tools.</p>
<h3 id="the-inverse-wishart-distribution--wikipedia">The Inverse Wishart Distribution (<a href="https://en.wikipedia.org/wiki/Inverse-Wishart_distribution">wikipedia</a>)</h3>
<p>We need to think about probability distributions of \(\Sigma\).  This
a little complicated because the space of admissible (positive
definite) covariance matrix is a particular manifold inside
\(\mathbb R^{n^2}\).  Luckily, there exists such a probability
distribution, called the inverse Wishart distribution. This
distribution is the multivariate version of the inverted gamma
distribution.  Let \(W\) be a \(n \times n\) positive definite random
matrix. \(W\) has the inverted Wishart \(IW(S,\nu)\) distribution if
its density is of the form</p>
<p>\begin{eqnarray}
p(W|S,\nu) \propto |S|^{\nu/2} |W|^{-(\nu+n+1)/2}\exp \left\{ - \frac{1}{2} tr[W^{-1}S] \right\}
\end{eqnarray}</p>
<p>The Wishart distribution arises in the Bayesian analysis of multivariate regression models.
To sample a \(W\) from an inverted Wishart \(IW(S,\nu)\) distribution, draw \(n \times 1\) vectors \(Z_1,\ldots, Z_\nu\)
from a multivariate normal \({\cal N}(0,S^{-1})\) and let
\[
W = \left[ \sum_{i=1}^\nu Z_iZ_i&rsquo;\right]^{-1}
\]
Note: to generate a draw \(Z\) from a multivariate \({\cal N}(\mu,\Sigma)\), decompose \(\Sigma = CC&rsquo;\),
where \(C\) is the lower triangular Cholesky decomposition matrix. Then let \(Z = \mu + C {\cal N}(0,{\cal I})\).</p>
<p><em>Caution</em>. The inverse Wishart distribution has haters on the internet.</p>
<h3 id="matrix-normal-distribution--wikipedia">Matrix Normal Distribution (<a href="https://en.wikipedia.org/wiki/Matrix_normal_distribution">wikipedia</a>)</h3>
<p>Let \(Z\) by a \(T \times n\) matrix.  \(Z\) follows a <em>matrix normal</em>
distribution with parameters \(\Mu\) (an \(T\times n\) matrix), \(\Sigma\)
(an \(n \times n\) positive definite matrix), and \(\Omega\) (a \(T
\times T\) positive definite matrix), if it has density:</p>
<p>\begin{align}
p(Z|M, \Sigma, \Omega) = (2\pi)^{-Tn/2} |\Sigma|^{-T/2} |\Omega|^{-n/2}
\exp\left\{-\frac12\mbox{tr}(\Sigma^{-1} (Z - M)&rsquo; \Omega^{-1} (Z - M))\right\}.
\end{align}</p>
<p>If \(Z\) follows a matrix normal distribution, then \(vec(Z)\) follows a
multivariate normal distribution with mean \(vec(M)\) and variance
\(\Omega \otimes \Sigma\).  To see this note that the term in the
exponent can be rewritten as:</p>
<p>\begin{align}
\mbox{tr}(\Sigma^{-1} (Z - M)&rsquo; \Omega^{-1} (Z - M))
&amp; =  vec\left((Z - M)\Sigma^{-1} \right)&rsquo; vec\left(\Omega^{-1} (Z - M) \right) \nonumber \\
&amp; = \left[ \left(\Sigma^{-1} \otimes  I_T \right)vec\left(Z - M\right) \right]&rsquo;
\left[ \left(I_n \otimes \Omega^{-1} \right) vec\left(Z - M\right)\right] \nonumber \\
&amp; = vec\left(Z-M\right)&rsquo;\left(\Sigma^{-1} \otimes I_T \right) \left(I_n \otimes \Omega^{-1} \right)vec\left(Z - M\right) \nonumber \\
&amp;= \left(vec(Z) - vec(M)\right)&rsquo;\left(\Sigma \otimes \Omega \right)^{-1} \left(vec(Z) - vec(M)\right)
\end{align}</p>
<p>and also noting that \(|\Sigma|^{T}|\Omega|^{n} = |\Sigma \otimes
\Omega|\).  Often \(\Omega\) is called the &ldquo;among row&rdquo; covariance
matrix and \(\Sigma\) is called the &ldquo;among column&rdquo; covariance matrix.
So increasing \(\Sigma_{1,1}\) will increase the variance of
\(Z_{\cdot1}\) and increasing \(\Sigma_{1,2}\) with increase the
covariance between \(Z_{\cdot1}\) and \(Z_{\cdot2}\).</p>
<h3 id="the-likelihood-of-var--p--as-a-matrix-normal">The Likelihood of VAR(p) As A Matrix Normal</h3>
<p>Let&rsquo;s consider a VAR with \(n\) variables \( (y_t\) is \(n\times 1)\) and \(p\) lags.</p>
<p>\begin{align}
y_t = \Phi_0 + \Phi_1 y_{t-1} + \ldots + \Phi_p y_{t-p} + u_t, \quad u_t \sim N\left(0,\Sigma\right).
\end{align}</p>
<p>Let \(x_t = [1,y_{t-1}&rsquo;,\ldots,y_{t-p}&rsquo;]&rsquo;\) and \(\Phi = [\Phi_0,\Phi_1,\ldots,\Phi_p]\):
\[
y_t&rsquo; = x_t&rsquo;\Phi + u_t&rsquo;
\]
Let,</p>
<p>\begin{align}
Y = \left[\begin{array}{c} y_1&rsquo; &amp; \vdots &amp; y_T&rsquo; \end{array}\right],
X = \left[\begin{array}{c} x_1&rsquo; &amp; \vdots &amp; x_T&rsquo; \end{array}\right], \mbox{ and }
U = \left[\begin{array}{c} u_1&rsquo; &amp; \vdots &amp; u_T&rsquo; \end{array}\right].
\end{align}</p>
<p>We have</p>
<p>\begin{align}
Y = X \Phi + U.
\end{align}</p>
<p>Notice that the columsn of \(U\) has a covariance determined by
\(\Sigma\), while the rows are independent.  So \(U\) follows a matrix
normal distribution with mean zero and covariance parameters
\(\Sigma\) and \(I_T\).  Thus, \(Y\) follows a matrix normal distribution
with likelihood given by:</p>
<p>\begin{align}
p(Y|\Phi,\Sigma) = (2\pi)^{-np/2} |\Sigma|^{-T/2} \exp\left\{-\frac12 tr\left[\Sigma^{-1} \left(Y-X\Phi\right)&rsquo;\left(Y - X\Phi\right)\right]\right\}.
\end{align}</p>
<h3 id="dummy-observation-priors">Dummy Observation Priors</h3>
<p>VARs are very flexible time series models, but their flexiblity comes at a cost.  The Bayesian paradigm is especially well suited</p>
<p>There are many ways to structure prior distributions for VARs. Here we&rsquo;ll talk about using pseudo-data, also known as dummy observations. Suppose we have \(T^*\) dummy observations
\((Y^*,X^*)\). The likelihood function for the dummy observations is of the form</p>
<p>\begin{eqnarray}
\lefteqn{ p( Y^* | \Phi,\Sigma ) =  (2 \pi)^{-nT^*/2} |\Sigma|^{- T^*/2}} \label{eq_priordummy1} \\
&amp;&amp; \exp
\left\{ - \frac{1}{2} tr[ \Sigma^{-1}(Y^{*&rsquo;}Y^* - \Phi&rsquo;X^{*&rsquo;}Y^*
- Y^{*&rsquo;}X^{*} \Phi + \Phi&rsquo; X^{*&rsquo;}X^* \Phi)] \right\}. \nonumber
\end{eqnarray}</p>
<p>Combining~(\ref{eq_priordummy1}) with the improper prior \(p(\Phi,\Sigma) \propto | \Sigma|^{-(n+1)/2}\) yields</p>
<p>\begin{eqnarray}
\hspace*{-0.6in}
p(\Phi,\Sigma | Y^*) &amp;=&amp; c_*^{-1} | \Sigma|^{- \frac{ T^*+n+1}{2}}\nonumber \\
&amp;~&amp;        exp  \left\{ - \frac{1}{2} tr[ \Sigma^{-1}(Y^{*&rsquo;}Y^* - \Phi&rsquo;X^{*&rsquo;}Y^*
- Y^{*&rsquo;}X^{*} \Phi + \Phi&rsquo; X^{*&rsquo;}X^* \Phi)] \right\},\nonumber
\end{eqnarray}</p>
<p>which can be interpreted as a prior density for \(\Phi\) and \(\Sigma\).</p>
<p>Define</p>
<p>\begin{eqnarray*}
\hat{\Phi}^* &amp;=&amp; (X^{*&rsquo;}X^*)^{-1}X^{*&rsquo;}Y^* \\
S^*          &amp;=&amp; (Y^* - X^* \hat{\Phi}^*)&rsquo;(Y^* - X^* \hat{\Phi}^*).
\end{eqnarray*}</p>
<p>It can be verified that the prior \(p(\Phi,\Sigma | Y^*)\) is of the Inverted Wishart-Normal \({\cal IW}-{\cal N}\) form</p>
<p>\begin{eqnarray}
\Sigma &amp; \sim &amp; {\cal IW} \bigg( S^*, T^*-k \bigg) \\
\Phi | \Sigma  &amp; \sim &amp; {\cal N} \bigg( \Phi^*,
\Sigma \otimes (X^{*&rsquo;} X^{*})^{-1} \bigg).
\end{eqnarray}</p>
<p><strong>Problem:</strong> Verify this!</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#666">=</span> <span style="color:#666">1</span>
</span></span></code></pre></div><p>The appropriate normalization constant for the prior density is given by</p>
<p>\begin{eqnarray}
c_* &amp;=&amp; (2 \pi)^{\frac{nk}{2}} | X^{*&rsquo;}X^*|^{-\frac{n}{2}}
| S^*|^{- \frac{ T^* -k}{2} } \\
&amp;&amp;    2^{\frac{n(T^* -k)}{2}} \pi^{ \frac{n(n-1)}{4} }
\prod_{i=1}^n \Gamma[(T^* - k+1 - i)/2], \nonumber
\end{eqnarray}</p>
<p>\(k\) is the dimension  of \(x_t\) and \(\Gamma[\cdot]\) denotes the gamma function.</p>
<p><strong>Problem:</strong> Verify this!</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#666">=</span> <span style="color:#666">1</span>
</span></span></code></pre></div><p>Details of this calculation can be found in cite:zellner1971.
The implementation of priors through dummy variables is often called mixed estimation
and dates back to <a href="#citeproc_bib_item_11">Theil and Goldberger (1961)</a>.</p>
<h3 id="timeline-of-the-minnesota-prior-glp">Timeline of the Minnesota Prior GLP</h3>
<p>The baseline prior is a version of the so-called Minnesota prior, first introduced in <a href="#citeproc_bib_item_7">Litterman (1979)</a> and later refined in <a href="#citeproc_bib_item_8">Litterman (1980)</a>.</p>
<p>The first prior of this type is known as ‚Äúsum-of-coefficients‚Äù prior and was originally proposed by cite:Doan1984</p>
<p>cite:sims1993nine known as ‚Äúdummy-initial-observation‚Äù</p>
<p>his deterministic component is defined as
œÑt ‚â° Ep
(
yt|y1, &hellip;, yp, ÀÜŒ≤
)
, i.e. the expectation of future y‚Äôs given the initial conditions
and the value of the estimated VAR coefficients. According to cite:sims1992bayesian, in un-
restricted VARs, œÑt has a tendency to exhibit temporal heterogeneity‚Äîa markedly
different behavior at the beginning and the end of the sample‚Äîand to explain an im-
plausibly high share of the variation of the variables over the sample.</p>
<h3 id="minnesota-prior">Minnesota Prior</h3>
<p>VARs really began to take off for forecasting and other purposes when combined with Bayesian methods.  The reason why Bayes is so helpful is that VARs contain <em>a lot</em> of parameters, and we don&rsquo;t have (relatively speaking) a lot of aggregate macroeconomic data.  So frequentist methods tended to overfit and make poor out of sample predictions.  A prior distribution for Bayesian VARs which avoided this problem was developed in the early 1980s by researchers at the University of Minnesota and the Federal Reserve Bank of Minneapolis, dubbed the Minnesota Prior.  It can be implemented using dummy observations. What follows is a brief description, see <a href="#citeproc_bib_item_5">Doan, Litterman, and Sims (1984)</a> for details.</p>
<p>Consider the following Gaussian bivariate VAR(2).</p>
<p>\begin{eqnarray}
\hspace{-0.5in}   \left[ \begin{array}{c} y_{1,t} \\ y_{2,t} \end{array} \right]
= \left[ \begin{array}{c} \alpha_1 \\ \alpha_2 \end{array} \right]
+ \left[ \begin{array}{cc} \beta_{11} &amp; \beta_{12} \\ \beta_{21} &amp; \beta_{22} \end{array} \right]
\left[ \begin{array}{c} y_{1,t-1} \\ y_{2,t-1} \end{array} \right]
+ \left[ \begin{array}{cc} \gamma_{11} &amp; \gamma_{12} \\ \gamma_{21} &amp; \gamma_{22} \end{array} \right]
\left[ \begin{array}{c} y_{1,t-2} \\ y_{2,t-2} \end{array} \right]
+ \left[ \begin{array}{c} u_{1,t} \\ u_{2,t} \end{array} \right]
\end{eqnarray}</p>
<p>Define \(y_t = [y_{1,t}, y_{2,t}]&rsquo;\), \(x_t = [y_{t-1}&rsquo;, y_{t-2}&rsquo;,1]&rsquo;\),
and \(u_t = [ u_{1,t}, u_{2,t}]&rsquo;\) and</p>
<p>\begin{eqnarray}
\Phi = \left[ \begin{array}{cc}
\beta_{11}  &amp; \beta_{21} \\
\beta_{12}  &amp; \beta_{22} \\
\gamma_{11} &amp; \gamma_{21} \\
\gamma_{12} &amp; \gamma_{22} \\
\alpha_1 &amp; \alpha_2 \end{array}
\right].
\end{eqnarray}</p>
<p>The VAR can be rewritten as follows</p>
<p>\begin{eqnarray}
y_t&rsquo; = x_t&rsquo; \Phi + u_t&rsquo; , \quad t=1,\ldots,T, \quad u_t \sim iid{\cal N}(0,\Sigma)
\end{eqnarray}</p>
<p>or in matrix form</p>
<p>\begin{eqnarray}
Y = X \Phi + U.
\end{eqnarray}</p>
<p>Based on a short pre-sample \(Y_0\) (typically the observations used to initialized the lags of the VAR) one calculates: \(s = std(Y_0)\) and \(\bar{y} = mean(Y_0)\). In addition there are a number of tuning parameters for the prior</p>
<ul>
<li>\(\tau\) is the overall tightness of the prior. Large values imply a small prior covariance
matrix.</li>
<li>\(d\): the variance for the coefficients of lag \(h\) is scaled down by the factor \(l^{-2d}\).</li>
<li>\(w\): determines the weight for the prior on \(\Sigma\). Suppose that \(Z_i = {\cal N}(0,\sigma^2)\). Then an estimator for \(\sigma^2\) is \(\hat{\sigma}^2 = \frac{1}{w} \sum_{i=1}^w Z_i^2.\)   The larger \(w\), the more informative the estimator, and in the context of the VAR, the
tighter the prior.</li>
<li>\(\lambda\) and \(\mu\): additional tuning parameters.</li>
</ul>
<p>Dummies for the \(\beta\) coefficients:
\[
\left[ \begin{array}{cc} \tau s_1 &amp; 0 \\
0 &amp; \tau s_2
\end{array} \right]
= \left[ \begin{array}{ccccc} \tau s_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0  &amp; \tau s_2 &amp; 0 &amp; 0 &amp; 0
\end{array} \right] \Phi + u&rsquo;
\]
The first observation implies, for instance, that
\hspace{-0.50in}</p>
<p>\begin{eqnarray*}
\hspace{-0.50in}
\tau s_1 &amp;=&amp; \tau s_1 \beta_{11} + u_1 \quad \Longrightarrow  \beta_{11} = 1 - \frac{u_1}{\tau s_1} \quad
\Longrightarrow  \quad \beta_{11} \sim {\cal N} \left( 1, \frac{\Sigma_{u,11}}{\tau^2 s_1^2} \right) \\
0  &amp;=&amp; \tau s_1 \beta_{21} + u_2 \quad \Longrightarrow  \beta_{21} = - \frac{u_2}{\tau s_1} \quad
\Longrightarrow  \quad \beta_{21} \sim {\cal N} \left( 0, \frac{\Sigma_{u,22}}{\tau^2 s_1^2} \right)
\end{eqnarray*}</p>
<pre><code>Dummies for the \\(\gamma\\) coefficients
</code></pre>
<p>\[
\left[ \begin{array}{cc} 0 &amp; 0 \\
0 &amp; 0
\end{array} \right]
= \left[ \begin{array}{ccccc} 0 &amp; 0 &amp; \tau s_1 2^d &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  &amp; \tau s_2 2^d &amp; 0
\end{array} \right] \Phi + u'
\]
The prior for the covariance matrix is implemented by
\[
\left[ \begin{array}{cc} s_1 &amp; 0 \\
0   &amp; s_2
\end{array} \right]
= \left[ \begin{array}{ccccc} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0  \\
0 &amp; 0 &amp; 0  &amp; 0 &amp; 0
\end{array} \right] \Phi + u'
\]</p>
<pre><code>                     Co-persistence prior dummy observations, reflecting the belief that
  when data on all \\(y\\)'s are stable at their initial levels, thy will
  tend to persist at that level:
  \\[
    \left[ \begin{array}{cc} \lambda \bar{y}\_1 &amp; \lambda \bar{y}\_2
           \end{array} \right]
    = \left[ \begin{array}{ccccc} \lambda \bar{y}\_1 &amp; \lambda \bar{y}\_2 &amp; \lambda \bar{y}\_1 &amp; \lambda \bar{y}\_2 &amp; \lambda
             \end{array} \right] \Phi + u'
  \\]
</code></pre>
<p>Own-persistence prior dummy observations, reflecting the belief that
when \(y_i\) has been stable at its initial level, it will tend to persist
at that level, regardless of the value of other variables:
\[
\left[ \begin{array}{cc} \mu \bar{y}_1 &amp; 0 \\
0   &amp; \mu \bar{y}_2
\end{array} \right]
= \left[ \begin{array}{ccccc} \mu \bar{y}_1 &amp; 0 &amp; \mu \bar{y}_1 &amp; 0 &amp; 0  \\
0 &amp; \mu \bar{y}_2 &amp; 0  &amp; \mu \bar{y}_2 &amp; 0
\end{array} \right] \Phi + u'
\]</p>
<p>In the same way we constructed a prior from dummy observations, we can
also construct a prior from a training sample. Suppose we split the
actual sample \(Y = [ Y^{-} , Y^{+} ]\), where \(Y^{-}\) is interpreted as
training sample, then</p>
<p>\begin{eqnarray}
\hspace*{-0.3in}
p(\Phi,\Sigma) &amp;=&amp; c_-^{-1} | \Sigma|^{- \frac{ T^-+n+1}{2}}\nonumber\\
&amp;~&amp;\left\{ - \frac{1}{2} tr[ \Sigma^{-1}(Y^{-&rsquo;}Y^- - \Phi&rsquo;X^{-&rsquo;}Y^-
- Y^{-&rsquo;}X^{-} \Phi + \Phi&rsquo; X^{-&rsquo;}X^- \Phi)] \right\},\nonumber
\end{eqnarray}</p>
<p>Of course one can also combine the dummy observations and training sample
to construct a prior distribution.</p>
<p>Notice that</p>
<p>\begin{eqnarray}
p(\Phi,\Sigma|Y) \propto p(Y|\Phi,\Sigma) p(Y^*|\Phi,\Sigma)
\end{eqnarray}</p>
<p>Now define:</p>
<p>\begin{eqnarray}
\tilde{\Phi}
&amp;=&amp; (X^{*&rsquo;}X^* + X&rsquo;X)^{-1}(X^{*&rsquo;}Y^* + X&rsquo;Y) \label{eq_phitilde}\\
\tilde{\Sigma}_u
&amp;=&amp; \frac{1}{T^*+ T}
\bigg[ (Y^{*&rsquo;}Y^* + Y&rsquo;Y) \nonumber \\
&amp;&amp;  - (X^{*&rsquo;}Y^* + X&rsquo;Y)&rsquo;(X^{*&rsquo;}X^* + X&rsquo;X)^{-1}(X^{*&rsquo;}Y^* + X&rsquo;Y) \bigg]. \label{eqa_sigtilde}
\end{eqnarray}</p>
<p>Since  prior and likelihood function are conjugate, it is straightforward to show,
e.g., Zellner (1971),
that the posterior distribution of \(\Phi\) and \(\Sigma\) is also of
the Inverted Wishart &ndash; Normal form:</p>
<p>\begin{eqnarray}
\Sigma | Y
&amp; \sim &amp; {\cal IW} \bigg( (T^*+T) \tilde{\Sigma}_u,T^*+T-k\bigg) \label{eq_sigpost} \\
\Phi | \Sigma, Y
&amp; \sim &amp; {\cal N}  \bigg( \tilde{\Phi}, \Sigma \otimes (X^{*&rsquo;}X^* + X&rsquo;X)^{-1} \bigg). \label{eq_phipost}
\end{eqnarray}</p>
<p>Suppose that we are using a prior constructed from a training sample
and dummy observations. Then the marginal data density is given by</p>
<p>\begin{eqnarray}
p(Y^+ | Y^-, Y^*, {\cal M}_0) = \frac{ \int p( Y^+, Y^-, Y^* | \Phi,\Sigma) d \Phi d\Sigma }{
\int p(  Y^-, Y^* | \Phi,\Sigma) d \Phi d\Sigma }
\end{eqnarray}</p>
<p>where the integrals in the numerator and denominator are given by the appropriate
modification of \(c_*\) defined above.  More specifically:</p>
<p>\begin{eqnarray}
\hspace{-0.5in}
\int p(Y | \Phi, \Sigma) d \Phi d\Sigma
= \pi^{- \frac{T-k}{2}} | X&rsquo;X |^{-\frac{n}{2}}
| S|^{- \frac{ T -k}{2} }
\pi^{ \frac{n(n-1)}{4} }
\prod_{i=1}^n \Gamma[(T - k+1 - i)/2],
\end{eqnarray}</p>
<p>where</p>
<p>\begin{eqnarray*}
\hat{\Phi} &amp;=&amp; (X&rsquo;X)^{-1}X&rsquo;Y \\
S          &amp;=&amp; (Y - X \hat{\Phi})&rsquo;(Y - X \hat{\Phi}).
\end{eqnarray*}</p>
<h2 id="the-structural-var">The Structural VAR</h2>
<p>Let&rsquo;s restrict to an \(n\) variable VAR(1) with mean zero for simplicity.  The VAR model is given by</p>
<p>\begin{align}
y_t = \Phi_1 y_{t-1} + u_t, \quad u_t \stackrel{i.i.d.}{\sim} {\cal N}(0,\Sigma)
\end{align}</p>
<p>The vector \(u_t\) corresponds to the one-step-ahead forecast errors.  Macroeconomic theory suggests that these forecast errors are driven by structural shocks, \(\epsilon_t\).  We can write the VAR in structural form as</p>
<p>\begin{align}
A_0 y_t = A_1 y_{t-1} + \epsilon_t, \quad \epsilon_t \stackrel{i.i.d.}{\sim} {\cal N}(0,I)
\end{align}</p>
<p>One can think of the rows of \(A_0\) and \(A_1\) as corresponding the coefficients associated with a given equation of the equilibrium system implied by the VAR.  Letting \(A_i = [a_{i,jk}]\) and setting \(A_1 = 0\)</p>
<p>\begin{align}
a_{0,11} y_{t,1} + a_{0,12} y_{t,2} + \cdots + a_{0,1n} y_{t,n} &amp;= \epsilon_{t,1} \\
a_{0,21} y_{t,1} + a_{0,22} y_{t,2} + \cdots + a_{0,2n} y_{t,n} &amp;= \epsilon_{t,2} \\
&amp; \vdots \\
a_{0,n1} y_{t,1} + a_{0,n2} y_{t,2} + \cdots + a_{0,nn} y_{t,n} &amp;= \epsilon_{t,n}
\end{align}</p>
<p>These equations describe the relationships among the variables in \(y_t\) in terms of the structural shocks \(\epsilon_t\).  How does one estimate the relationships? Consider the relationship between the two VAR representations.  Assuming \(A_0\) is invertible&mdash;invertibility here simply means&hellip;&mdash;it&rsquo;s easy to go from \((A_0, A_1)\) to \((\Phi, \Sigma)\):</p>
<p>\begin{align}
y_t = \underbrace{A_0^{-1} A_{1}}_{\Phi_1} y_{t-1} + \underbrace{A_0^{-1} \epsilon_t}_{u_t}, \mbox{ with } \Sigma = A_0^{-1} (A_0^{-1})'
\end{align}</p>
<p>But the reverse is not so easy.  Suppose one knows \(\Sigma\).  We could  consider taking \(A_0^{-1}\) to be the lower Cholesky decomposition of \(\Sigma\).  We write this as \(A_0 = \text{chol}(\Sigma)^{-1}.\) This choice for \(A_0\) with of course satisfy \(\Sigma = A_0^{-1} (A_0^{-1})&rsquo;\) by construction.  But now consider an alternative choice \(\tilde A_0 = \text{chol}(\Sigma)^{-1}\Omega \), where \(\Omega\) is an orthonormal matrix.  An orthonormal matrix \(\Omega\) has the property that \(\Omega&rsquo;\Omega = I\), where \(I\) is the identity matrix. This implies that \(\Omega\) preserves the length of vectors it multiplies and its inverse is simply its transpose, i.e., \(\Omega^{-1} = \Omega&rsquo;\).  We have</p>
<p>\begin{align}
\tilde A_0^{-1} (\tilde A_0^{-1})&rsquo; = A_0^{-1} \Omega^{-1} (\Omega^{-1})&rsquo; (A_0^{-1})&rsquo; = \Sigma.
\end{align}</p>
<p>This is the fundamental identification problem associated with structural VARs.  It is impossible to recover \((A_0, A_1)\) from \((\Phi, \Sigma)\).  An immediate implication of [] is that \(p(Y|A_0, A_1) = p(Y|\tilde A_0, \tilde A_1)\).  The identification can be understood from many different perspectives, but it is perhaps most simple to understand it as one of counting: there are \(n^2\) parameters in \(A_0\) but only \(n(n+1)/2\)&mdash;less than \(n^2\) if \(n&gt;1\)&mdash;free parameters in the covariance matrix \(\Sigma\).</p>
<h3 id="identifying-omega">Identifying \(\Omega\)</h3>
<p>In what follows we&rsquo;ll discuss different identification strategies in the literature in with an emphasis on bivariate VARs.  In general defining the space of orthogonal matrices is tedious, but for \(n = 2\) we can use a Givens matrix, which rotates vectors in a 2-dimensional plane.  It has the form:
\[
\Omega(\theta) = \begin{bmatrix}
\cos(\theta) &amp; -\sin(\theta) \\
\sin(\theta) &amp; \cos(\theta)
\end{bmatrix}
\]
Here, \(\theta\) is the rotation angle. It is trival to check that this matrix satisfies the orthonormality condition \(\Omega&rsquo;\Omega = I\).  As we discussed above, an identification scheme is simply a prior distribution \(p(\theta | \Phi, \Sigma)\).  Note that in general \(\theta \in [-\pi, \pi]\).</p>
<h4 id="short-run-restrictions"><em>Short Run Restrictions</em></h4>
<p>The first kind of restriction is owing to <a href="#citeproc_bib_item_10">Sims (1986)</a>.  After arguing that VARs ought to be the benchmark time series model for macroeconomic analysis, Sims turned to the question of policy analysis.  He argued for timing-based identification of \(A_0\), with an emphasis on lags in information as an identifying assumption.  Here he writes:</p>
<blockquote>
<p>&hellip; the idea that the monetary authority and the banks can see interest rates and indicators of movements in monetary aggregates immediately, but can only react to the remaining variables in the economy after a delay because data on these variables are released later.</p></blockquote>
<p>This logic is extended beyond the policy variables (the interest rate and money supply) to the rest of the variables in his VAR.  Ultimately, these timing assumptions lead to an \(A_0\) that is lower triangular with zeros in the upper right block.  This identification is sometimes called &ldquo;recursive&rdquo; because it implies a sequential response structure where each variable can be contemporaneously influenced only by itself and preceding variables in the ordering, but not by those that follow.  Sims calls this following the &ldquo;Wold causal chain&rdquo; highlighting how the shocks \(\epsilon_{i,t}\) associated with this approach can serve as the innovations in a Wold decomposition of the series.</p>
<p>In this identification scheme, it&rsquo;s easy to recover \(A_0\) from \(\Sigma\).  In the framework above, this amounts to setting \(\Omega = I.\)  In our two variable setup this corresponds to a dogmatic prior on which concentrates \(\Omega\) on this matrix, i.e., \(\theta = 0\).</p>
<p><em>Example</em>. Let&rsquo;s consider the two variable VAR in the interest rate and the output gap, \(y_t = [i_t, x_t]&rsquo;\).  Our recursive identification scheme implies</p>
<p>\begin{align}
a_{0, 11} i_t &amp;= \text{&hellip; lags of \(i_t\) and \(x_t\) &hellip;} + \epsilon_{1,t} \\
a_{0, 21} i_t + a_{0, 22} x_t &amp;= \text{&hellip; lags of \(i_t\) and \(x_t\) &hellip;} + \epsilon_{2,t}
\end{align}</p>
<p>The interest rate \(i_t\) is contemporaneously unaffected by the output gap \(x_t\), but the output gap may respond immediately to changes in the interest rate.  Typically, the first equation is interpeted as the policy equation&mdash;that is, the monetary policy reaction function.  Here, monetary policy is a function of the lagged interest rate, the lagged output gap, and a shock \(\epsilon_{1,t}\).  This shock is given the interpretation of a monetary policy shock and the impulse of the endogenous variables in the system is of great interest to economists.</p>
<p>A potential source of confusion is in myriad sets of notation used in the SVAR literature.  Here we defined \(A_0\) as the matrix of contemporaneous relationships among the variables in the structural VAR.  Some instead define \(A_0\) (labelled however) as the matrix that relates the structural shocks directly to the reduced-form errors, which is usually inverted compared to our definition.  In this &ldquo;impact matrix&rdquo; formulation, \(A_0^{-1}\) captures how structural shocks \(\epsilon_t\) affect the observed data \(y_t\) directly. This can introduce some confusion, so it&rsquo;s important to be clear about how \(A_0\) is being used in any given context.  But it&rsquo;s worth noting that if \(A_0\) is a lower triangular matrix, then \(A_0^{-1}\)  will also be triangular matrix.  In this example:</p>
<p>\begin{align}
A_0^{-1} = \begin{bmatrix}
\frac{1}{a_{0,11}} &amp; 0 \\
-\frac{a_{0,21}}{a_{0,11}a_{0,22}} &amp; \frac{1}{a_{0,22}}
\end{bmatrix}
\end{align}</p>
<p>Note that this inverted structure still maintains the recursive identification. The matrix \(A_0^{-1}\) reflects the assumption that the policy variable (interest rate) does not respond contemporaneously to \(\epsilon_{2,t}\) while
the output gap can be influenced immediately by both shocks \(\epsilon_{1,t}\) and \(\epsilon_{2,t}\).</p>
<p>We estimate a VAR(1) on these two variables from 1959Q1 - 2007Q4.  The output gap is construct as 100 times the log difference of U.S. real GDP and the CBO&rsquo;s estimate of potential GDP.   The interest rate is taken to be the quarterly average of the federal funds rate.  We use a diffuse prior in the Bayesian VAR model.</p>
<figure><img src="https://edherbst.net/ox-hugo/b9d015a84f40fbd7be7771301824e1b76780206c.png">
</figure>

<p>This kind of identification scheme has been extremely popular in economics, with two prominent example being <a href="#citeproc_bib_item_1">Bernanke (1986)</a> and <a href="#citeproc_bib_item_3">Christiano, Eichenbaum, and Evans (2005)</a>. Both approaches use timing assumptions related to monetary policy.</p>
<h4 id="long-run-restrictions">Long Run Restrictions</h4>
<p>In pioneering work, <a href="#citeproc_bib_item_2">Blanchard and Quah (1989)</a> combined the dynamic structure of the VAR with the idea of timing restrictions.  Specifically they propose using economic theory-derived restrictions on the <em>long-run</em> behavior of the model‚Äôs variables. Certain shocks are assumed to have no long-term effect on particular variables.  Specifically,
<a href="#citeproc_bib_item_2">Blanchard and Quah (1989)</a> consider a bivariate VAR in real GNP growth and the unemployment rate.  They identify \(\Omega\) using the long-run behavior of the VAR:</p>
<blockquote>
<p>We assume that there are two kinds of disturbances, each uncorrelated with the other, and that neither has a long-run effect on unemployment. We assume however that the first has a long-run effect on output while
the second does not. These assumptions are sufficient to just identify the two types of disturbances, and their dynamic effects on output and unemployment.</p></blockquote>
<p>Recall that we can write \(A_0 = \text{chol}(\Sigma)^{-1}\Omega\), so that \(\text{chol}{\Sigma} = \Omega&rsquo; A_0^{-1}\).  Using this, write the  MA respresntation of the VAR:</p>
<p>\begin{align}
\begin{bmatrix}
y_{1,t} \\ y_{2,t}
\end{bmatrix} &amp;=
\begin{bmatrix}
C_{11}(L) &amp; C_{12}(L) \\
C_{21}(L) &amp; C_{22}(L)
\end{bmatrix}
\begin{bmatrix}
\cos(\theta) &amp; \sin(\theta) \\
-\sin(\theta) &amp; \cos(\theta)
\end{bmatrix}
\begin{bmatrix}
a_{0,11}^{-1} &amp; 0 \\
-a_{0,21}/(a_{0,11} a_{0,22}) &amp; a_{0,22}^{-1}
\end{bmatrix}
\begin{bmatrix}
\epsilon_{1,t} \\ \epsilon_{2,t}
\end{bmatrix} \nonumber \\
&amp;=
\begin{bmatrix}
C_{11}(L)\cos(\theta)/a_{0,11} + C_{12}(L)(-\sin(\theta)a_{0,21}/(a_{0,11} a_{0,22}) + \cos(\theta)/a_{0,22}) &amp; C_{11}(L)\sin(\theta)/a_{0,11} + C_{12}(L)(\cos(\theta)a_{0,21}/(a_{0,11} a_{0,22}) + \sin(\theta)/a_{0,22}) \\
C_{21}(L)\cos(\theta)/a_{0,11} + C_{22}(L)(-\sin(\theta)a_{0,21}/(a_{0,11} a_{0,22}) + \cos(\theta)/a_{0,22}) &amp; C_{21}(L)\sin(\theta)/a_{0,11} + C_{22}(L)(\cos(\theta)a_{0,21}/(a_{0,11} a_{0,22}) + \sin(\theta)/a_{0,22})
\end{bmatrix}
\begin{bmatrix}
\epsilon_{1,t} \\
\epsilon_{2,t}
\end{bmatrix}
\end{align}</p>
<p>The idea is to apply restrictions on the long-run responses of \(\epsilon_{1,t}\) and \(\epsilon_{2,t}\).
\end{align}</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>[0;31m---------------------------------------------------------------------------[0m
</span></span><span style="display:flex;"><span>[0;31mNameError[0m                                 Traceback (most recent call last)
</span></span><span style="display:flex;"><span>[0;32m&lt;ipython-input-2-eccf72fd6466&gt;[0m in [0;36m&lt;module&gt;[0;34m[0m
</span></span><span style="display:flex;"><span>[1;32m      1[0m [0;31m# 2x2 givens[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
</span></span><span style="display:flex;"><span>[0;32m----&gt; 2[0;31m [0mbk_VAR_struct[0m [0;34m=[0m [0mSimsZhaSVARPrior[0m[0;34m([0m[0mbk_estimation_data[0m[0;34m,[0m [0mnp[0m[0;34m.[0m[0mones[0m[0;34m([0m[0;36m7[0m[0;34m)[0m[0;34m,[0m [0mp[0m[0;34m=[0m[0;36m8[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
</span></span><span style="display:flex;"><span>[0m[1;32m      3[0m [0;34m[0m[0m
</span></span><span style="display:flex;"><span>[1;32m      4[0m [0;32mfrom[0m [0mscipy[0m[0;34m.[0m[0moptimize[0m [0;32mimport[0m [0mroot_scalar[0m[0;34m[0m[0;34m[0m[0m
</span></span><span style="display:flex;"><span>[1;32m      5[0m [0;34m[0m[0m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[0;31mNameError[0m: name &#39;SimsZhaSVARPrior&#39; is not defined
</span></span></code></pre></div><p>In addition to <a href="#citeproc_bib_item_2">Blanchard and Quah (1989)</a>, <a href="#citeproc_bib_item_6">Gal√≠ (1999)</a> is an important addition to the literature using long-run restrictions.  The paper argues that long-run restrictions can be used to distinguish between the effects of technology shocks and other types of shocks on macroeconomic variables, arguing that only technology shocks have long-run effects on labor productivity.  Under this identifying assumption, the Gali&rsquo;s VAR model predicts that hours worked decreases in response to a labor productivity shock, contrary to real business cycle theory.</p>
<h4 id="sign-restrictions">Sign Restrictions</h4>
<p>Sign restrictions are another approach to identifying structural VARs, which use theoretical or empirical assumptions about the signs of the responses of variables to specific shocks over a specified time horizon. Unlike short-run or long-run restrictions, sign restrictions do not impose zero constraints on the contemporaneous relationships among variables.  In an influential work, <a href="#citeproc_bib_item_12">Uhlig (2005)</a> uses sign restrictions to identify the effects of a monetary policy shock.  Rather than pin down a single \(\Omega\), he incorporates a set of potential \(\Omega\) matrices by requiring that, following a monetary policy shock, certain economic variables respond in a way consistent with theory (e.g., the interest rate increases while output and prices are restricted to not rise).  In a Bayesian framework, this is of course possible via the prior distribution.</p>
<p>Suppose we have the bivariate VAR in the interest rate and output gap as before.  Consider the MA representation of the model:</p>
<p>\[
y_t = C(L) \epsilon_t
\]</p>
<p>where \(C(L)\) is the matrix polynomial in the lag operator and \(\epsilon_t\) are the shocks. Sign restrictions would specify expected qualitative responses of the variables in \(y_t\) to those shocks, such as:</p>
<ul>
<li>Interest rate increases following a monetary shock</li>
<li>Output gap may not increase immediately after the shock</li>
</ul>
<p>This framework does not define a unique \(\Omega\) or a full structural representation but restricts the plausible set to matrices consistent with these expected sign patterns.</p>
<h2 id="references">References</h2>
<style>.csl-entry{text-indent: -1.5em; margin-left: 1.5em;}</style><div class="csl-bib-body">
  <div class="csl-entry"><a id="citeproc_bib_item_1"></a><span style="font-variant:small-caps;">Bernanke, B. S.</span> (1986): ‚ÄúAlternative Explanations of the Money-Income Correlation,‚Äù <i>Carnegie-Rochester Conference Series on Public Policy</i>, 25, 49‚Äì99.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_2"></a><span style="font-variant:small-caps;">Blanchard, O. J., and D. Quah</span>. (1989): <a href="http://ideas.repec.org/a/ijc/ijcjou/y2007q4a4.html">‚ÄúThe Dynamic Effects of Aggregate Demand and Supply Disturbances,</a>‚Äù <i>American Economic Review</i>, 79, 655‚Äì73.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_3"></a><span style="font-variant:small-caps;">Christiano, L. J., M. Eichenbaum, and C. L. Evans</span>. (2005): ‚ÄúNominal Rigidities and the Dynamic Effects of a Shock to Monetary Policy,‚Äù <i>Journal of Political Economy</i>, 113, 1‚Äì45.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_4"></a><span style="font-variant:small-caps;">Del Negro, M., and F. Schorfheide</span>. (2011): ‚ÄúBayesian Macroeconometrics,‚Äù in <i>Handbook of Bayesian Econometrics</i>, ed. by Dijk, H. van, G. Koop, and J. Geweke. Oxford University Press, 293-389.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_5"></a><span style="font-variant:small-caps;">Doan, T., R. Litterman, and C. A. Sims</span>. (1984): <a href="http://ideas.repec.org/a/ijc/ijcjou/y2007q4a4.html">‚ÄúForecasting and Conditional Projections Using Realistic Prior Distributions,</a>‚Äù <i>Econometric Reviews</i>, 3, 1‚Äì100.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_6"></a><span style="font-variant:small-caps;">Gal√≠, J.</span> (1999): ‚ÄúTechnology, Employment, and the Business Cycle: Do Technology Shocks Explain Aggregate Fluctuations?,‚Äù <i>American Economic Review</i>, 89, 249‚Äì71.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_7"></a><span style="font-variant:small-caps;">Litterman, R. B.</span> (1979): Techniques of Forecasting Using Vector Autoregressions,Federal Reserve Bank of Minneapolis.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_8"></a><span style="font-variant:small-caps;">---</span>. (1980): A Bayesian Procedure for Forecasting with Vector Autoregression, Working Paper, Massachusetts Institute of Technology, Department of Economics.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_9"></a><span style="font-variant:small-caps;">Sims, C. A.</span> (1980): <a href="http://ideas.repec.org/a/ijc/ijcjou/y2007q4a4.html">‚ÄúMacroeconomics and Reality,</a>‚Äù <i>Econometrica</i>, 48, 1‚Äì48.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_10"></a><span style="font-variant:small-caps;">---</span>. (1986): <a href="https://www.minneapolisfed.org/research/qr/qr1011.pdf">‚ÄúAre Forecasting Models Usable for Policy Analysis?,</a>‚Äù <i>Federal Reserve Bank of Minneapolis Quarterly Review</i>, 10, 2‚Äì16.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_11"></a><span style="font-variant:small-caps;">Theil, H., and A. S. Goldberger</span>. (1961): ‚ÄúOn Pure and Mixed Estimation in Economics,‚Äù <i>International Economic Review</i>, 2, 65‚Äì78.</div>
  <div class="csl-entry"><a id="citeproc_bib_item_12"></a><span style="font-variant:small-caps;">Uhlig, H.</span> (2005): ‚ÄúWhat Are the Effects of Monetary Policy on Output? Results from an Agnostic Identification Procedure,‚Äù <i>Journal of Monetary Economics</i>, 52, 381‚Äì419.</div>
</div>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] },
  tex2jax: {
      inlineMath: [['$','$'],['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script type="text/javascript"
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


        <div class="footer">
    Powered by <a href="https://gohugo.io/">Hugo</a> with
    <a href="https://github.com/mrmierzejewski/hugo-theme-console/">Console Theme</a>. 
</div>

    </div>
  </body>
</html>
