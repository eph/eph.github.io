<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Advanced Monte Carlo for Macroeconometrics  Part 1: Theory on Ed Herbst</title>
    <link>https://edherbst.net/teaching/bank-of-colombia-smc/</link>
    <description>Recent content in Advanced Monte Carlo for Macroeconometrics  Part 1: Theory on Ed Herbst</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Dec 2020 11:38:50 -0500</lastBuildDate><atom:link href="https://edherbst.net/teaching/bank-of-colombia-smc/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Particle Filter</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/07-nonlinear-dsge-models-and-particle-filters/</link>
      <pubDate>Fri, 11 Dec 2020 09:47:16 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/07-nonlinear-dsge-models-and-particle-filters/</guid>
      <description>Nonlinear DSGE Models From Linear to Nonlinear DSGE Models While DSGE models are inherently nonlinear, the nonlinearities are often small and decision rules are approximately linear. One can add certain features that generate more pronounced nonlinearities: stochastic volatility; markov switching coefficients; asymmetric adjustment costs; occasionally binding constraints. From Linear to Nonlinear DSGE Models Linear DSGE model leads to
\begin{eqnarray*} y_t &amp;amp;=&amp;amp; \Psi_0(\theta) + \Psi_1(\theta)t + \Psi_2(\theta) s_t + u_t, \quad u_t \sim N(0,\Sigma_u) ,\\\ s_t &amp;amp;=&amp;amp; \Phi_1(\theta)s_{t-1} + \Phi_\epsilon(\theta) \epsilon_t, \quad \epsilon_t \sim N(0,\Sigma_\epsilon).</description>
    </item>
    
    <item>
      <title>Particle MCMC and SMC^2</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/08-pmcmc-and-smc-squared/</link>
      <pubDate>Fri, 11 Dec 2020 09:47:02 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/08-pmcmc-and-smc-squared/</guid>
      <description>Introduction Embedding PF Likelihoods into Posterior Samplers Likelihood functions for nonlinear DSGE models can be approximated by the PF. We will now embed the likelihood approximation into a posterior sampler: PFMH Algorithm (a special case of PMCMC). Embedding PF Likelihoods into Posterior Samplers Distinguish between: \(\{ p(Y|\theta), p(\theta|Y), p(Y) \}\), which are related according to: \[ p(\theta|Y) = \frac{p(Y|\theta) p(\theta)}{p(Y)} , \quad p(Y) = \int p(Y|\theta) p(\theta) d\theta \] \(\{ \hat{p}(Y|\theta), \hat{p}(\theta|Y), \hat{p}(Y) \}\), which are related according to: \[ \hat{p}(\theta|Y) = \frac{\hat{p}(Y|\theta) p(\theta)}{\hat{p}(Y)} , \quad \hat{p}(Y) = \int \hat{p}(Y|\theta) p(\theta) d\theta.</description>
    </item>
    
    <item>
      <title>Monte Carlo Simulation</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/03-monte-carlo-simulation/</link>
      <pubDate>Fri, 11 Dec 2020 09:46:48 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/03-monte-carlo-simulation/</guid>
      <description>Importance Sampling The main event Inference: Need to characterize posterior \(p(\theta|Y)\). Unfortunately, for many interesting models it is not possible to evaluate the moments and quantiles of the posterior \(p(\theta|Y)\) analytically.
Rules of game: we can only numerically evaluate prior \(p(\theta)\) and likelihood \(p(Y|\theta)\).
To evaluate posterior moments of function \(h(\theta)\), we need numerical techniques.
Estimating Posterior Moments We will often abbreviate posterior distributions \(p(\theta|Y)\) by \(\pi(\theta)\) and posterior expectations of \(h(\theta)\) by \[ \mathbb{E}_\pi[h] = \mathbb{E}_\pi[h(\theta)] = \int h(\theta) \pi(\theta) d\theta = \int h(\theta) p(\theta|Y) d\theta.</description>
    </item>
    
    <item>
      <title>Advanced MCMC: Hamiltonian Monte Carlo</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/10-hamiltonian-monte-carlo/</link>
      <pubDate>Fri, 11 Dec 2020 09:46:32 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/10-hamiltonian-monte-carlo/</guid>
      <description>Introduction Hamiltonian Monte Carlo We previously spoke about how much the efficacy of MCMC algorithms depended on the shape of the posterior. We&amp;rsquo;re going to talk about posterior simulators that make that idea explicit. In particular, the Hamiltonian Monte Carlo sampler described in \cite{Neal_2011}.
Details Hamiltonian Monte Carlo adapts methods from the study of molecular dynamics: simulate the motion of molecules based on Newton&amp;rsquo;s laws. The systems which describe the evolution of molecules over time exhibit so-called Hamiltonian dynamics The state of the system at any point in time is summarized by a pair \((\theta, p)\).</description>
    </item>
    
    <item>
      <title>Introduction</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/00-introduction/</link>
      <pubDate>Fri, 11 Dec 2020 09:46:18 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/00-introduction/</guid>
      <description>Introduction Hello! My name is Ed Herbst. I&amp;rsquo;m currently an economist at the Federal Reserve Board.
I&amp;rsquo;m interested in (Bayesian) macroeconometrics, and I&amp;rsquo;m excited to spend the next two weeks talking about it with you!
The next two weeks The syllabus has a rough plan of where we&amp;rsquo;re going.
But, in my experience, there is usually some re-optimization.
If there&amp;rsquo;s something you&amp;rsquo;d like to talk about, or spend more (or less) time on, just let me know.</description>
    </item>
    
    <item>
      <title>Estimating Three DSGE Models</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/06-estimating-a-linear-dsge-model/</link>
      <pubDate>Fri, 11 Dec 2020 09:46:05 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/06-estimating-a-linear-dsge-model/</guid>
      <description>Three DSGE Models Application 1: A New Keynesian Model with Correlated Shocks The assumption that exogenous shocks evolve according to independent AR(1) is to some extent arbitrary.
Trying to generalize this assumption seems natural.
However, the more elaborate the exogenous propagation mechanism, the more difficult it becomes to disentangle endogenous from exogenous propagation.
This generates identification problems.
Application 1: A New Keynesian Model with Correlated Shocks Technology growth shock \(\hat{z}_t\), government spending shock \index{government!</description>
    </item>
    
    <item>
      <title>Sequential Monte Carlo</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/05-sequential-monte-carlo/</link>
      <pubDate>Fri, 11 Dec 2020 09:45:51 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/05-sequential-monte-carlo/</guid>
      <description>Introduction MCMC: What works and what doesn&amp;rsquo;t, Simple Model State-space representation:
\begin{align} y_t = [\begin{array}{cc} 1 &amp;amp; 1 \end{array} ] s_t, \quad s_t = \left[ \begin{array}{cc} {\color{blue} \phi_1} &amp;amp; 0 \ {\color{blue} \phi_3} &amp;amp; {\color{blue} \phi_2} \end{array} \right] s_{t-1} + \left[ \begin{array}{c} 1 \ 0 \end{array} \right] \epsilon_t. \label{eq_exss} \end{align}
The state-space model can be re-written as ARMA(2,1) process \[ (1- {\color{blue} \phi_1} L)(1-{\color{blue} \phi_2} L) y_t = (1-({\color{blue} \phi_2} - {\color{blue} \phi_3} )L) \epsilon_t.</description>
    </item>
    
    <item>
      <title>Markov Chain Monte Carlo</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/04-metropolis-hastings/</link>
      <pubDate>Fri, 11 Dec 2020 09:45:35 -0500</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/04-metropolis-hastings/</guid>
      <description>Metropolis-Hastings Algorithm The Metropolis-Hastings Algorithm Metropolis-Hastings (MH) algorithm belongs to the class of Markov chain Monte Carlo (MCMC) algorithms. Algorithm constructs a Markov chain such that the stationary distribution associated with this Markov chain is unique and equals the posterior distribution of interest. First version constructed by Metropolis1953. Later generalized by Hastings1970. Tierney1994 proved important convergence results for MCMC algorithms. Introduction: Chib1995a. Textbook Robert2004 or Geweke2005. Markov Chain Monte Carlo Importance sampler generates a sequence of independent draws from the posterior distribution \(\pi(\theta)\), the MH algorithm generates a sequence of serially correlated draws.</description>
    </item>
    
    <item>
      <title>Introduction Bayes 6: (Linear) State Space Models</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/02-linear-dsge-models-and-the-kalman-filter/</link>
      <pubDate>Tue, 27 Oct 2020 21:06:35 -0400</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/02-linear-dsge-models-and-the-kalman-filter/</guid>
      <description>Intro Background Textbook treatments: woodford_2003, Gali2008 Key empirical papers: ireland2004, christiano2005, Smets2007, An2007b, Frequentist estimation: Harvey1991, Hamilton, Bayesian estimation: HerbstSchorfheide2015 A DSGE Model Small-Scale DSGE Model Intermediate and final goods producers Households Monetary and fiscal policy Exogenous processes Equilibrium Relationships Final Goods Producers Perfectly competitive firms combine a continuum of intermediate goods: \[ Y_t = \left( \int_0^1 Y_t(j)^{1-\nu} dj \right)^{\frac{1}{1-\nu}}. \] Firms take input prices \(P_t(j)\) and output prices \(P_t\) as given; maximize profits \[ \Pi_t = P_t \left( \int_0^1 Y_t(j)^{1-\nu} dj \right)^{\frac{1}{1-\nu}} - \int_{0}^1 P_t(j)Y_t(j)dj.</description>
    </item>
    
    <item>
      <title>A Crash Course In Bayesian Inference</title>
      <link>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/01-a-crash-course-in-bayesian-inference/</link>
      <pubDate>Tue, 27 Oct 2020 19:22:48 -0400</pubDate>
      
      <guid>https://edherbst.net/teaching/bank-of-colombia-smc/lectures/01-a-crash-course-in-bayesian-inference/</guid>
      <description>Introduction These notes are available as slides and a Jupyter notebook.
Modes of Inference Previously, we focussed on frequentist inference (repeated sampling prodecures) measures of accuracy and performance that we used to assess the statistical procedures were pre-experimental However, many statisticians and econometricians believed that post-experimental reasoning should be used to assess inference procedures wherein only the actual observation \(Y^T\) is relevant and not the other observations in the sample space that could have been observed Example Suppose \(Y_1\) and \(Y_2\) are independently and identically distributed and \[ P_\theta \{ Y_i = \theta-1 \} = \frac{1}{2}, \quad P_\theta \{ Y_i = \theta+1 \} = \frac{1}{2} \] Consider the following confidence set</description>
    </item>
    
  </channel>
</rss>
